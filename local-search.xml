<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LLM - 提示词优化</title>
    <link href="/2025/05/30/llm/llm-openai-prompt-optimize/"/>
    <url>/2025/05/30/llm/llm-openai-prompt-optimize/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM-提示词优化"><a href="#LLM-提示词优化" class="headerlink" title="LLM - 提示词优化"></a>LLM - 提示词优化</h1><p>前面的文章我们介绍了什么是大语言模型，并且用一个例子快速体验了大语言模型。  </p><p>接下来，我们来了解如何优化自己的提示词，来提升模型的效果。</p><hr><p>我们先来准备一个 <code>chat</code> 方法，以便我们能够与OpenAI进行交互。   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">BASE_URL=<span class="hljs-string">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span> <span class="hljs-comment"># 阿里云的 API 地址</span><br>API_KEY=<span class="hljs-string">&quot;***&quot;</span> <span class="hljs-comment"># 替换为你的 API KEY</span><br>MODEL=<span class="hljs-string">&quot;qwen-turbo-latest&quot;</span><br><br><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br>client = OpenAI(base_url=BASE_URL, api_key=API_KEY)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">chat</span>(<span class="hljs-params">message</span>):<br>    response = client.chat.completions.create(<br>        model=MODEL,<br>        messages=[<br>            &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: message&#125;<br>        ],<br>    )<br>    response_content = response.choices[<span class="hljs-number">0</span>].message.content<br>    <span class="hljs-keyword">return</span> response_content<br><br></code></pre></td></tr></table></figure><h2 id="不好的提示词"><a href="#不好的提示词" class="headerlink" title="不好的提示词"></a>不好的提示词</h2><p>我们先来看一下不好的提示词是什么样子  </p><p>假设我们刚刚学习了孟浩然的五言律诗《春晓》，然后想让AI也帮我们创作一首关于春天的诗：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(chat(<span class="hljs-string">&quot;帮我写一首关于春天的诗&quot;</span>))<br></code></pre></td></tr></table></figure><p>AI 帮我们生成了这个结果：  </p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs text">《春的邮戳》<br><br>风拆开柳枝的信封，<br>抖落一地鹅黄的动词。<br>燕子用尾羽蘸水，<br>在池塘的空白处，<br>盖一枚淡蓝的邮戳。<br></code></pre></td></tr></table></figure><p>看上去像是一首诗，但是是不是不知道具体属于哪一类? 也不知道具体在表达什么？</p><p>这是因为我们只向AI提供了一个模糊的问题，关键信息只有<code>春天</code> 和 <code>诗</code>，除此之外再没有其他的细节和背景信息，因此AI只能基于有限的上下文来生成答案。</p><h2 id="好的提示词"><a href="#好的提示词" class="headerlink" title="好的提示词"></a>好的提示词</h2><p>如果我们想让AI的回答更符合我们的期望，我们需要把 <code>心里话</code> 说出来，提供更多的细节和信息，比如：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(chat(<span class="hljs-string">&quot;帮我写一首关于春天的五言律诗，表达一切刚刚开始，作者发自内心的喜悦和激动，以及对未来美好生活的憧憬。诗的名字用书名号括起来&quot;</span>))<br></code></pre></td></tr></table></figure><p>AI 的回答：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs text">《春望》<br><br>东君初试手，万象始更新。<br>冻解溪声脆，泥融草色匀。<br>开轩纳清气，把酒谢芳邻。<br>莫负扶摇力，青云寄此身。<br></code></pre></td></tr></table></figure><p>这次看起来是不是很像那么回事儿，符合我们想要的五言律诗的格式。 并且 <code>东君处试手，万象始更新</code> 示意了春意初生，天地欣荣，充满希望；<code>莫负扶摇力，青云寄此身</code> 由景入志，抒发诗人不愿辜负大好春光、立志奋发、未来可期的抱负。</p><h2 id="提示词优化建议"><a href="#提示词优化建议" class="headerlink" title="提示词优化建议"></a>提示词优化建议</h2><p>前面的例子展示了如何通过补充细节和背景信息来提升模型的效果，效果还不错吧？ </p><p>总的来说，我们可以对提示词进行以下方面的优化，来提高模型的准确性和可读性：</p><ul><li>限定角色</li><li>限定行业领域</li><li>向AI提供更多的细节和上下文</li><li>限定输出格式</li><li>向AI展示样例</li></ul><p>比如我们想让AI扮演一个古诗词专家的角色，对刚才AI生成的诗来做一个点评  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(chat(<span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">你是一个文学专家</span><br><span class="hljs-string">专注于研究七言律诗</span><br><span class="hljs-string">帮我点评一下这首诗写的如何</span><br><span class="hljs-string">逐字逐词分析一下用了什么意象，表达了什么感情。</span><br><span class="hljs-string"></span><br><span class="hljs-string">输出格式为：</span><br><span class="hljs-string">=========================================</span><br><span class="hljs-string">&lt;编号&gt;. &lt;诗句&gt;。</span><br><span class="hljs-string">  ⭐️ &lt;关键词&gt;: &lt;解释及点评 比如出自哪里，寓意是是什么等&gt;。</span><br><span class="hljs-string">  ✅ 意境: &lt;本句诗的意境&gt;</span><br><span class="hljs-string">  ✅ 技巧: &lt;本句诗的技巧&gt;</span><br><span class="hljs-string">  ✅ 情感: &lt;本句诗的情感&gt;</span><br><span class="hljs-string">=========================================</span><br><span class="hljs-string">总结：</span><br><span class="hljs-string">✅ 题目：&lt;本诗的题目&gt;</span><br><span class="hljs-string">✅ 体裁：&lt;本诗的体裁&gt;</span><br><span class="hljs-string">✅ 主题：&lt;本诗的主题&gt;</span><br><span class="hljs-string">✅ 情感：&lt;本诗表达的情感&gt;</span><br><span class="hljs-string">✅ 点评：&lt;你对本诗的整体评价&gt;</span><br><span class="hljs-string">=========================================</span><br><span class="hljs-string"></span><br><span class="hljs-string">本诗如下：</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">《春望》</span><br><span class="hljs-string"></span><br><span class="hljs-string">东君初试手，万象始更新。</span><br><span class="hljs-string">冻解溪声脆，泥融草色匀。</span><br><span class="hljs-string">开轩纳清气，把酒谢芳邻。</span><br><span class="hljs-string">莫负扶摇力，青云寄此身。</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>))<br><br></code></pre></td></tr></table></figure><p>AI 的回答：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs text">以下是对《春望》的逐句解析与整体点评，严格遵循您要求的格式：<br><br>=========================================<br>1. 东君初试手，万象始更新。<br>  ⭐️ 东君：典出《楚辞·九歌》，司春之神，象征生命萌发。<br>  ⭐️ 万象更新：化用《红楼梦》&quot;万象争辉&quot;句，暗含除旧布新之意。<br>  ✅ 意境：神明轻拂人间，天地苏醒的宏观画卷。<br>  ✅ 技巧：拟人（&quot;试手&quot;）+ 对仗（东君/万象）。<br>  ✅ 情感：对自然伟力的敬畏与欣喜。<br><br>2. 冻解溪声脆，泥融草色匀。<br>  ⭐️ 冻解/泥融：出自杜甫《绝句》&quot;泥融飞燕子&quot;，暗含温度变化。<br>  ⭐️ 脆/匀：通感手法（听觉转视觉），&quot;脆&quot;字尤妙。<br>  ✅ 意境：微观视角的春之交响曲。<br>  ✅ 技巧：工对（冻解/泥融）+ 炼字（&quot;脆&quot;字点睛）。<br>  ✅ 情感：发现细微之美的愉悦。<br><br>3. 开轩纳清气，把酒谢芳邻。<br>  ⭐️ 开轩：陶渊明《归去来兮辞》&quot;倚南窗以寄傲&quot;的化用。<br>  ⭐️ 芳邻：王勃&quot;接孟氏之芳邻&quot;的简写，喻自然万物。<br>  ✅ 意境：文人雅士与春日的互动仪式。<br>  ✅ 技巧：动作叙事（开/把）+ 用典无痕。<br>  ✅ 情感：闲适自得的隐逸情怀。<br><br>4. 莫负扶摇力，青云寄此身。<br>  ⭐️ 扶摇：《庄子·逍遥游》典故，喻机遇。<br>  ⭐️ 青云：双关自然景象与仕途抱负。<br>  ✅ 意境：春日引发的生命哲思。<br>  ✅ 技巧：转折（前实后虚）+ 双关。<br>  ✅ 情感：积极进取的入世志向。<br>=========================================<br><br>总结：<br>✅ 题目：《春望》——&quot;望&quot;字统领全篇视觉层次<br>✅ 体裁：五言律诗（首联不对仗，二三联工对）<br>✅ 主题：通过春日物候变化抒发人生感怀<br>✅ 情感：由自然欣喜转向功业追求，形成张力<br>✅ 点评：<br>1. 优点：典故运用自然如盐入水，炼字精准（脆/匀/纳），空间由远及近再及远<br>2. 建议：尾联&quot;青云&quot;可更含蓄，避免与杜甫《望岳》意象重复<br>3. 文学史定位：得王维山水诗之清丽，兼有杜甫沉郁之思<br><br>（全诗符合&quot;起承转合&quot;结构，80分作品，若颈联能创新意象则更佳）<br><br></code></pre></td></tr></table></figure><p>怎么样，对这个回答还满意吗？ 这就是提示词工程的魅力！ </p><p>希望这个例子能帮助你更好地理解 OpenAI 的提示词工程，以及如何使优化我们的提示词来提升模型的效果。 </p><p>如果有任何问题或需要进一步的帮助，欢迎在评论区留言。</p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>OpenAI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM - 快速上手大语言模型</title>
    <link href="/2025/05/30/llm/llm-quick-start/"/>
    <url>/2025/05/30/llm/llm-quick-start/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM-快速上手大语言模型"><a href="#LLM-快速上手大语言模型" class="headerlink" title="LLM - 快速上手大语言模型"></a>LLM - 快速上手大语言模型</h1><p>前面的文章我们介绍了什么是AI，都是停留在一些概念上面，接下来，我们实际来用几个简单的步骤和代码，来体验一下，快速上手大语言模型。</p><h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1 环境准备"></a>1 环境准备</h2><ul><li>Python  3.10 及以上</li></ul><p>Python 是一个非常流行的编程语言，广泛应用于数据科学、机器学习、人工智能等领域。 本文也将在 Python 环境下来进行实际操作的演示。 </p><p>如果您还没有安装 Python，可以参考 <a href="https://www.runoob.com/python/python-install.html">Python 环境搭建</a> 下载并安装。  </p><h3 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h3><p>OpenAI 是一个提供大语言模型的公司，他们也提供了一套 API 来与大语言模型进行交互。 同时也支持其他厂商的大语言模型，比如 DeepSeek 等。<br>因此我们需要安装 <code>openai</code> 库来与 OpenAI 进行交互。 你可以使用以下命令来安装 <code>openai</code> 库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install openai<br></code></pre></td></tr></table></figure><h3 id="启动-Notebook"><a href="#启动-Notebook" class="headerlink" title="启动 Notebook"></a>启动 Notebook</h3><p>Notebook 是一个非常方便的工具，可以用来交互式地进行编写和运行代码。 </p><p>你可以使用以下命令来启动 Notebook：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">jupyter notebook<br></code></pre></td></tr></table></figure><p>启动Notebook 后，你会跳转到一个界面，类似于以下的截图：  </p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.wispsb20t.webp" alt="Notebook 界面"></p><p>我们点击 <code>New</code> 按钮，然后选择 <code>Python 3</code> 来创建一个新的 Notebook。  </p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.4ub66goojo.webp" alt="新建 Notebook"></p><p>然后在 Notebook 中，你可以开始编写代码了。  </p><p>比如我们来编写一个简单的 Python 代码，输出 “Hello, Python”。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hello, Python&quot;</span>)<br></code></pre></td></tr></table></figure><p>按下 <code>Shift + 回车键</code>，代码就会运行，并且你会看到输出 “Hello, Python”。    </p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.1lc29t2j6o.webp" alt="Notebook 运行结果"></p><p>如果你本地的情况和上面的截图一样，就表示现在 Python 环境准备好了。</p><h2 id="2-申请-API-KEY"><a href="#2-申请-API-KEY" class="headerlink" title="2 申请 API KEY"></a>2 申请 API KEY</h2><p>OpenAI 是与大语言模型交互的 API，但是大语言模型是由各个厂商提供的，比如：</p><ul><li>OpenAI 公司的 <code>GPT-4</code>、<code>GPT-3.5</code>、<code>GPT-3</code> 等</li><li>DeepSeek 公司的 <code>Deepseek-R1</code> </li><li>阿里巴巴公司的 <code>通义千问(Qwen3)</code> </li><li>…</li></ul><p>因此我们需要先去对应的厂商官网申请 API KEY。</p><p>例如 <a href="https://bailian.console.aliyun.com/?tab=model#/model-market/detail/qwen3?modelGroup=qwen3">阿里云百炼平台</a> 为每个用户提供了多个大语言模型的免费使用额度，可以满足我们学习大语言模型的需求。 </p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.lvywp174i.webp" alt="通义千问免费额度"></p><p>因此我们可以访问 <a href="https://bailian.console.aliyun.com/?tab=model#/api-key">阿里云百炼 API-Key</a> 来申请阿里厂商的 API KEY。</p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.8s3jn6tc9x.webp" alt="阿里云百炼 - 创建 API KEY"></p><p>创建好后，点击 <code>查看</code> 按钮，就可以看到 API KEY 的信息了。 然后将其复制下来，后面在代码中使用。  </p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.4g4qfndbk0.png" alt="阿里云百炼 - 查看 API KEY"></p><h2 id="3-与-AI-对话"><a href="#3-与-AI-对话" class="headerlink" title="3 与 AI 对话"></a>3 与 AI 对话</h2><p>Python 环境准备好了，API KEY 也申请好了，接下来就可以与 OpenAI 进行交互了。 我们先回到前面打开的 Notebook 页面中，准备运行我们的AI代码。 </p><ol><li>首先需要导入 <code>openai</code> 库，然后创建一个 <code>OpenAI</code> 的客户端：在 Notebook 中输入以下代码片段，并按 Shift + 回车执行</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">BASE_URL=<span class="hljs-string">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span> <span class="hljs-comment"># 阿里云的 API 地址</span><br>API_KEY=<span class="hljs-string">&quot;***&quot;</span> <span class="hljs-comment"># 替换为你的 API KEY</span><br><br><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br>client = OpenAI(base_url=BASE_URL, api_key=API_KEY)<br><span class="hljs-built_in">print</span>(client)<br></code></pre></td></tr></table></figure><p>将会输出类似以下的输出，即代表成功创建了 OpenAI 的客户端：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">&lt;openai.OpenAI object at 0x10952b790&gt;<br></code></pre></td></tr></table></figure><ol start="2"><li>然后我们使用阿里的 <code>通义千问-Turbo (qwen-turbo-latest)</code> 模型来发起一个对话，向 AI 发送打招呼的消息 “你好”</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">MODEL=<span class="hljs-string">&quot;qwen-turbo-latest&quot;</span><br>response = client.chat.completions.create(<br>    model=MODEL,<br>    messages=[<br>        &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;你好&quot;</span>&#125;<br>    ],<br>)<br>response_content = response.choices[<span class="hljs-number">0</span>].message.content<br><span class="hljs-built_in">print</span>(response_content)<br></code></pre></td></tr></table></figure><p>我们将会看到AI的回答，比如：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">你好！很高兴见到你。有什么我可以帮你的吗？😊<br></code></pre></td></tr></table></figure><ol start="3"><li>为了方便我们向AI发送不同的消息，我们可以上面的代码封装成一个 chat 函数，函数的输入是我们发送给 AI 的消息，函数的返回值是 AI 的回复。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">BASE_URL=<span class="hljs-string">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span> <span class="hljs-comment"># 阿里云的 API 地址</span><br>API_KEY=<span class="hljs-string">&quot;***&quot;</span> <span class="hljs-comment"># 替换为你的 API KEY</span><br>MODEL=<span class="hljs-string">&quot;qwen-turbo-latest&quot;</span><br><br><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br>client = OpenAI(base_url=BASE_URL, api_key=API_KEY)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">chat</span>(<span class="hljs-params">message</span>):<br>    response = client.chat.completions.create(<br>        model=MODEL,<br>        messages=[<br>            &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: message&#125;<br>        ],<br>    )<br>    response_content = response.choices[<span class="hljs-number">0</span>].message.content<br>    <span class="hljs-keyword">return</span> response_content<br></code></pre></td></tr></table></figure><p>然后我们就可以使用这个 <code>chat</code> 函数来与 AI 进行对话了。比如：</p><ul><li><p>发送 “今天天气真不错”  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(chat(<span class="hljs-string">&quot;今天天气真不错&quot;</span>))<br></code></pre></td></tr></table></figure></li><li><p>AI 回复： </p> <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">是啊，今天的天气确实很棒！阳光明媚，微风轻拂，正是出去走走、放松心情的好时候。你有什么计划吗？要不要一起去公园散步或者喝杯咖啡？<br></code></pre></td></tr></table></figure></li><li><p>发送 “什么咖啡最好喝”  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(chat(<span class="hljs-string">&quot;简单说一下喝咖啡的好处&quot;</span>))<br></code></pre></td></tr></table></figure></li><li><p>AI 回复</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">喝咖啡有以下一些好处：<br><br><span class="hljs-number">1.</span> **提神醒脑**：咖啡中的咖啡因能刺激中枢神经系统，帮助提高注意力、集中力和反应速度。<br><br><span class="hljs-number">2.</span> **增强体力与运动表现**：咖啡因可以促进脂肪分解，为身体提供能量，有助于提升运动表现。<br><br><span class="hljs-number">3.</span> **抗氧化作用**：咖啡含有丰富的抗氧化物质，有助于抵抗自由基，减缓细胞老化。<br><br><span class="hljs-number">4.</span> **降低某些疾病风险**：适量饮用咖啡可能降低患<span class="hljs-number">2</span>型糖尿病、帕金森病、肝病和某些癌症的风险。<br><br><span class="hljs-number">5.</span> **改善情绪**：咖啡因可以促进多巴胺分泌，有助于改善心情，减少抑郁风险。<br><br>不过要注意的是，过量饮用咖啡可能导致失眠、心悸、胃部不适等问题，建议每天摄入量不超过<span class="hljs-number">400</span>毫克咖啡因（约<span class="hljs-number">3</span>-<span class="hljs-number">4</span>杯普通咖啡）。<br></code></pre></td></tr></table></figure></li></ul><p>以上就是一个最简单的与 OpenAI 进行交互的示例，希望这个示例能帮助你快速上手大语言模型，非常容易动手实践吧？ </p><p>如果有任何问题或需要进一步的帮助，欢迎在评论区留言。</p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM - 大语言模型简介</title>
    <link href="/2025/05/30/llm/llm-overview/"/>
    <url>/2025/05/30/llm/llm-overview/</url>
    
    <content type="html"><![CDATA[<h1 id="LLM-大语言模型简介"><a href="#LLM-大语言模型简介" class="headerlink" title="LLM - 大语言模型简介"></a>LLM - 大语言模型简介</h1><p>AI 这个词大家应该都不陌生了，最近几年 AI 可谓是火遍全球，AI 的应用领域也越来越广泛，从语音识别到图像处理，从自然语言处理到强化学习，AI 的应用已经渗透到了我们生活的方方面面。</p><p>那么，什么是 AI？什么是 LLM？什么是 AIGC？我们如何体验 LLM？今天，我们就来一起探索这些话题。</p><h2 id="什么是-AI"><a href="#什么是-AI" class="headerlink" title="什么是 AI"></a>什么是 AI</h2><p>AI（Artificial Intelligence）即人工智能，是指由人类创造的系统通过模拟人的智能行为来执行任务，比如：</p><ul><li>感知（如图像识别、人脸识别）</li><li>推理（如下棋、做决策）</li><li>语言理解与生成（如 ChatGPT）</li><li>动作控制（如自动驾驶）</li></ul><h2 id="什么是-AIGC"><a href="#什么是-AIGC" class="headerlink" title="什么是 AIGC"></a>什么是 AIGC</h2><p>AIGC（AI-Generated Content）即人工智能生成内容，是 AI 的一个具体应用领域，即可以像人类一样理解、生成和创作内容。</p><p>然而，目前 AIGC 的应用还处于初级阶段，主要集中在图像生成、文本生成、视频生成等方面，比如：</p><table><thead><tr><th>生成类型</th><th>代表性产品</th></tr></thead><tbody><tr><td>AI 生成文字</td><td>ChatGPT</td></tr><tr><td>AI 生成代码</td><td>Github Copilot</td></tr><tr><td>AI 生成视频</td><td>剪映</td></tr><tr><td>AI 生成图片</td><td>MidJourney</td></tr><tr><td>AI 生成音频</td><td>Stable Audio</td></tr></tbody></table><h2 id="什么是-LLM"><a href="#什么是-LLM" class="headerlink" title="什么是 LLM"></a>什么是 LLM</h2><p>LLM（Large Language Model）即 大语言模型，是基于 NLP（自然语言处理）的深度学习模型。 2022年11月，OpenAI 发布了 ChatGPT，带动了一波大语言模型的热潮。</p><p>LLM 目前完成的任务主要是 <code>生成</code>，<code>分类</code>, <code>翻译</code>，<code>总结</code> 等等。 给模型一些文本内容输入，它能理解输入文本的语义返回相应的输出。</p><p>LLM 首先需要通过大量的数据和大量的算力来进行训练，来增加模型的知识库，从而更好地理解用户输入文本的含义，生成更准确的输出。</p><p>这种训练过程需要大量的计算资源，因此目前 LLM 的训练成本相对较高。</p><p>目前已有很多大型互联网公司发布了自己的大语言模型，包括但不限于以下模型：</p><table><thead><tr><th>公司名称</th><th>模型名称</th><th>应用平台</th></tr></thead><tbody><tr><td>OpenAI</td><td>GPT-4</td><td>ChatGPT</td></tr><tr><td>Anthropic</td><td>Claude-2</td><td>Cursor</td></tr><tr><td>DeepSeek</td><td>DeepSeek-R1</td><td>DeepSeek</td></tr><tr><td>字节跳动</td><td>Doubao</td><td>豆包</td></tr><tr><td>阿里巴巴</td><td>Qwen3</td><td>通义千问</td></tr></tbody></table><hr><p>以上就是关于 AI &#x2F; AIGC &#x2F; LLM 的简单介绍</p><p>如果有任何问题或需要进一步的帮助，欢迎在评论区留言。</p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在 Github 上构建个人博客网站</title>
    <link href="/2025/05/14/build-blog-on-github/"/>
    <url>/2025/05/14/build-blog-on-github/</url>
    
    <content type="html"><![CDATA[<p>本文将安利一个个人博客模板仓库，用于快速搭建个人博客网站。<br>用户只需要编写 Markdown 格式的博客内容，提交后即可自动发布到个人博客网站上。  </p><h1 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h1><ul><li>自动将 Markdown 转换为静态网页代码</li><li>自动发布到 github.io 网站</li><li>支持评论功能</li><li>支持网站统计功能</li></ul><h1 id="准备条件"><a href="#准备条件" class="headerlink" title="准备条件"></a>准备条件</h1><ul><li>github 账号</li></ul><h1 id="1-创建个人博客网站仓库"><a href="#1-创建个人博客网站仓库" class="headerlink" title="1. 创建个人博客网站仓库"></a>1. 创建个人博客网站仓库</h1><p>在 github 上创建一个新的代码仓库, 此仓库用于保存网页代码等，来呈现你的个人博客。<br>创建完成后你可以使用 <a href="https://xxx.github.io/">https://xxx.github.io</a> 来访问个人博客网站。</p><ol><li><p>点击 <code>your repositories</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.361smiekru.webp" alt="image">  </p></li><li><p>点击 <code>New</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.83a9gcu9y8.webp" alt="image">  </p></li><li><p>填写 <code>Repository name</code> 后点击 <code>Create repository</code><br><strong>注意：</strong> 仓库名格式必须是 <code>&lt;your username&gt;.github.io</code>, 例如 <code>tangchenyang.github.io</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.7i0lu24fp5.webp" alt="image">  </p></li><li><p>点击 <code>creating a new file</code> 创建一个初始页面<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.7i0lu28blh.webp" alt="image"><br>初始文件名为 <code>index.html</code>, 并编辑以下内容：   </p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-keyword">html</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">html</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">&quot;en&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>Hello, this is my personal blog<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span><br></code></pre></td></tr></table></figure><p>编辑完成后点击 <code>Commit changes</code> </p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.51edf57flw.webp" alt="image"><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.39lek8pk30.webp" alt="image"></p></li><li><p>等待 Github Action 部署完成<br>页面创建完成后，Github Action 将自动触发部署任务，部署完成后，你的个人网站将可以访问。<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.4g4psuhvjp.webp" alt="image"></p></li><li><p>访问你的个人网站<br>在浏览器输入 <code>&lt;your username&gt;.github.io</code> 即可看到你的个人博客网站初始内容。<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.73u637c7sz.webp" alt="image"></p></li></ol><h1 id="2-创建博客文章仓库"><a href="#2-创建博客文章仓库" class="headerlink" title="2. 创建博客文章仓库"></a>2. 创建博客文章仓库</h1><p>此仓库用于维护 Markdown 格式的博客文章，提交后将自动发布到个人博客网站。</p><h2 id="申请-GitHub-PAT-Personal-Access-Token"><a href="#申请-GitHub-PAT-Personal-Access-Token" class="headerlink" title="申请 GitHub PAT (Personal Access Token)"></a>申请 GitHub PAT (Personal Access Token)</h2><ol><li><p>点击 <code>Settings</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.5q7n0nijk6.webp" alt="image">  </p></li><li><p>点击最底部的 <code>Developers settings</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.5tr8yddq50.webp" alt="image">  </p></li><li><p>依次点击下图按钮，进入 New Personal Access Token 界面<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.4qrjnhk8jj.webp" alt="image"></p></li><li><p>设置有效期和权限，生成 PAT<br><code>Note</code> 见名知意即可；<code>Expiration</code> 根据情况选择，如<code>no expiration</code>；Scope 选择 <code>repo</code> 即可。<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.3k88ew05g4.webp" alt="image"><br>勾选完成后，点击底部的 <code>Generate token</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.6wqy99km34.webp" alt="image"><br>生成 PAT 后，复制并妥善保管，离开此页面后将不再可见。</p></li></ol><h2 id="创建博客文章仓库"><a href="#创建博客文章仓库" class="headerlink" title="创建博客文章仓库"></a>创建博客文章仓库</h2><ol><li><p>Github 创建博客仓库 <code>blog</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.5q7n0n2oqb.webp" alt="image"></p></li><li><p>添加 Repository secrets<br>依次点击 <code>Settings</code> -&gt; <code>Secrets and variables</code> -&gt; <code>Actions</code> -&gt; <code>New repository secret</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.83a9hvmezk.webp" alt="image"><br><code>Name</code> 填写 <code>HEXO_DEPLOY_PAT</code>; <code>Secret</code> 将前面生成的Personal Access Token 填写至此；点击 <code>Add secret</code> 即可。<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.969ysrl8qa.webp" alt="image"></p></li></ol><h2 id="关联模板仓库代码"><a href="#关联模板仓库代码" class="headerlink" title="关联模板仓库代码"></a>关联模板仓库代码</h2><ol><li><p>克隆模板博客仓库到本地，并重命名为 <code>blog</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/tangchenyang/blog_template_repo.git blog<br></code></pre></td></tr></table></figure></li><li><p>修改提交信息<br>进入 <code>blog</code> 目录下，编辑 <code>.github/workflows/deploy.yml</code>文件, 将 <code>GIT_HTTPS_URL</code>，<code>GIT_USER_NAME</code> 和 <code>GIT_USER_EMAIL</code> 按自己的信息替换即可，如：<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.8z6qxdty3c.webp" alt="image"></p></li><li><p>关联 <code>blog</code> 仓库, 并推送到远端  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd blog<br>rm -rf .git<br>echo &quot;我的个人博客&quot; &gt; README.md<br>git init<br>git add * &amp;&amp; git add .*<br>git commit -m &quot;first commit&quot;<br>git branch -M master<br>git remote add origin git@github.com:tangchenyang/blog.git<br>git push -u origin master<br></code></pre></td></tr></table></figure></li><li><p>检查博客文章仓库 <code>blob</code> 的GHA (GitHub Action)<br>代码推送到远端 <code>blog</code> 仓库后，由于包含了 workflow, 因此 GHA 将自动触发 CI&#x2F;CD Pipeline;<br><code>blog/blogs</code> 目录下的所有markdown 文件将自动转换为静态网页代码，并自动提交到你的个人博客网站仓库 <code>&lt;your username&gt;.github.io</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.7w71mhaaey.webp" alt="image"></p></li><li><p>检查个人博客网站仓库 <code>&lt;your username&gt;.github.io</code> 的 GHA<br>由 <code>blog</code> GHA 自动提交的静态网页代码将自动触发 <code>&lt;your username&gt;.github.io</code> 的GHA，自动将静态网页部署到你的个人博客网站上。<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.5tr8yffzsz.webp" alt="image"><br>GHA 执行完成后，在浏览器输入 <code>&lt;your username&gt;.github.io</code> 即可看到你的个人博客网站初始内容。<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.map4yq6d.webp" alt="image"></p></li></ol><h1 id="3-开启评论和网站统计功能"><a href="#3-开启评论和网站统计功能" class="headerlink" title="3. 开启评论和网站统计功能"></a>3. 开启评论和网站统计功能</h1><p>Fluid 主题支持评论和网站统计功能，本文将使用第三方服务 <code>LeanCloud</code> 实现免费功能 。  </p><ol><li><p>申请 LeanCloud 账号<br>进入 <a href="https://console.leancloud.cn/register">LeanCloud 账号注册</a> 注册账号, 登录并完成实名认证和邮箱验证<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.4cl3wq4wkt.webp" alt="image"><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.92qcv53ex1.webp" alt="image"><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.58hlc6pzpw.webp" alt="image">  </p></li><li><p>创建 App<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.70ak73g9fn.webp" alt="image"></p></li><li><p>获取 AppID、AppKey 和 AppServerURL<br>进入 APP<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.2h8j44bgwt.webp" alt="image"><br>依次点击 <code>Settings</code> -&gt; <code>App Keys</code>，即可看到相关信息<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.4qrjnlzcdo.webp" alt="image">  </p></li><li><p>将上述信息添加到 <code>blog</code> 仓库的 Repository secrets  </p><ul><li>将 <code>AppID</code> 添加为 <code>HEXO_LEANCLOUD_APP_ID</code>  </li><li>将 <code>AppKey</code> 添加为 <code>HEXO_LEANCLOUD_APP_KEY</code>    </li><li>将 <code>REST API Server URL</code> 添加为 <code>HEXO_LEANCLOUD_APP_SERVER_URL</code><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.4jobs6lmvr.webp" alt="image"></li></ul></li><li><p>修改启用评论和统计功能<br>修改 <code>blob</code> 仓库中的 <code>.github/workflows/deploy.yml</code> 文件， 将<code>ENABLE_WEB_STATISTICS</code> 和 <code>ENABLE_COMMENTS</code> 设置为 <code>true</code>，提交即可<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.7zqnka9upm.webp" alt="image"><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/image.70ak74bba3.webp" alt="image"></p></li></ol><h1 id="发布任意博客文章"><a href="#发布任意博客文章" class="headerlink" title="发布任意博客文章"></a>发布任意博客文章</h1><p>完成上述所有流程后，当你在 <code>blob</code> 仓库的 blogs 目录下添加任意 Markdown 文件并提交，大约一到两分钟后，你的个人博客网站上就能看到该文章。<br>现在开始享受你的写作旅程吧！ </p>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
      <tag>github</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ELK 集成</title>
    <link href="/2025/05/13/elk/Manual-Install-ELK/"/>
    <url>/2025/05/13/elk/Manual-Install-ELK/</url>
    
    <content type="html"><![CDATA[<h1 id="安装-E-L-K-服务"><a href="#安装-E-L-K-服务" class="headerlink" title="安装 E.L.K. 服务"></a>安装 E.L.K. 服务</h1><h2 id="安装-Elasticsearch"><a href="#安装-Elasticsearch" class="headerlink" title="安装 Elasticsearch"></a>安装 Elasticsearch</h2><p><a href="Manual-Install-Elasticsearch.md">Manual-Install-Elasticsearch.md</a>  </p><h2 id="安装-Logstash"><a href="#安装-Logstash" class="headerlink" title="安装 Logstash"></a>安装 Logstash</h2><p><a href="Manual-Install-Logstash.md">Manual-Install-Logstash.md</a>  </p><h2 id="安装-Kibana"><a href="#安装-Kibana" class="headerlink" title="安装 Kibana"></a>安装 Kibana</h2><p><a href="Manual-Install-Kibana.md">Manual-Install-Kibana.md</a><br>安装完成后，先不要配置，待组网完成后配置 Elasticsearch</p><h1 id="集成-ELK"><a href="#集成-ELK" class="headerlink" title="集成 ELK"></a>集成 ELK</h1><h2 id="组网"><a href="#组网" class="headerlink" title="组网"></a>组网</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker network create net-elk<br>docker network connect net-elk elasticsearch<br>docker network connect net-elk logstash<br>docker network connect net-elk kibana<br></code></pre></td></tr></table></figure><h2 id="Kibana-集成-Elasticsearch"><a href="#Kibana-集成-Elasticsearch" class="headerlink" title="Kibana 集成 Elasticsearch"></a>Kibana 集成 Elasticsearch</h2><p>选择 Manually Configure Kibana 选项，然后输入以下配置即可：<br><a href="http://elasticsearch:9200/">http://elasticsearch:9200</a></p><hr><p>todo 后续深度集成ELK 并提供Example</p>]]></content>
    
    
    <categories>
      
      <category>ELK</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>ELK</tag>
      
      <tag>Elasticsearch</tag>
      
      <tag>Logstash</tag>
      
      <tag>Kibana</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Logstash 安装</title>
    <link href="/2025/05/13/elk/Manual-Install-Logstash/"/>
    <url>/2025/05/13/elk/Manual-Install-Logstash/</url>
    
    <content type="html"><![CDATA[<h1 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h1><h2 id="拉取ubuntu镜像"><a href="#拉取ubuntu镜像" class="headerlink" title="拉取ubuntu镜像"></a>拉取ubuntu镜像</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull ubuntu:22.04<br></code></pre></td></tr></table></figure><h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -itd --privileged -p 5044:5044 --name logstash ubuntu:22.04<br></code></pre></td></tr></table></figure><h2 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec -it logstash bash<br></code></pre></td></tr></table></figure><p>以下操作均在容器内部</p><h2 id="apt-install"><a href="#apt-install" class="headerlink" title="apt install"></a>apt install</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">国内网络切换 阿里云镜像</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">sed -i <span class="hljs-string">&quot;s@ports.ubuntu.com@mirrors.aliyun.com@g&quot;</span> /etc/apt/sources.list</span><br>apt clean &amp;&amp; apt update<br>apt install -y vim wget curl <br></code></pre></td></tr></table></figure><h1 id="安装-Logstash"><a href="#安装-Logstash" class="headerlink" title="安装 Logstash"></a>安装 Logstash</h1><h2 id="创建用于存放安装包和软件的目录"><a href="#创建用于存放安装包和软件的目录" class="headerlink" title="创建用于存放安装包和软件的目录"></a>创建用于存放安装包和软件的目录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /root/software<br>mkdir -p /root/install_packages<br></code></pre></td></tr></table></figure><h2 id="下载安装包"><a href="#下载安装包" class="headerlink" title="下载安装包"></a>下载安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">if [ $(uname -m ) == &quot;aarch64&quot;]; then<br> wget https://artifacts.elastic.co/downloads/logstash/logstash-8.11.1-linux-aarch64.tar.gz -P /root/install_packages/<br>else then<br> wget https://artifacts.elastic.co/downloads/logstash/logstash-8.11.1-linux-x86_64.tar.gz -P /root/install_packages/<br>fi <br></code></pre></td></tr></table></figure><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf /root/install_packages/logstash-*.tar.gz -C /root/software/<br></code></pre></td></tr></table></figure><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# logstash&quot; &gt;&gt; /etc/profile<br>echo &quot;export LOGSTASH_HOME=/root/software/logstash-8.11.1&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$LOGSTASH_HOME/bin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h2 id="加载环境变量"><a href="#加载环境变量" class="headerlink" title="加载环境变量"></a>加载环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><h2 id="测试Logstash"><a href="#测试Logstash" class="headerlink" title="测试Logstash"></a>测试Logstash</h2><h3 id="通过命令行运行"><a href="#通过命令行运行" class="headerlink" title="通过命令行运行"></a>通过命令行运行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">logstash -e &#x27;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&#x27;<br></code></pre></td></tr></table></figure><hr><p>Logstash 安装完成，开始探索吧</p>]]></content>
    
    
    <categories>
      
      <category>Logstash</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>ELK</tag>
      
      <tag>Logstash</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Elasticsearch 安装</title>
    <link href="/2025/05/13/elk/Manual-Install-Elasticsearch/"/>
    <url>/2025/05/13/elk/Manual-Install-Elasticsearch/</url>
    
    <content type="html"><![CDATA[<h1 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h1><h2 id="拉取ubuntu镜像"><a href="#拉取ubuntu镜像" class="headerlink" title="拉取ubuntu镜像"></a>拉取ubuntu镜像</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull ubuntu:22.04<br></code></pre></td></tr></table></figure><h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -itd --privileged -p 9200:9200 --name elasticsearch ubuntu:22.04<br></code></pre></td></tr></table></figure><h2 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec -it elasticsearch bash<br></code></pre></td></tr></table></figure><p>以下操作均在容器内部</p><h2 id="apt-install"><a href="#apt-install" class="headerlink" title="apt install"></a>apt install</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">国内网络切换 阿里云镜像</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">sed -i <span class="hljs-string">&quot;s@ports.ubuntu.com@mirrors.aliyun.com@g&quot;</span> /etc/apt/sources.list</span><br>apt clean &amp;&amp; apt update<br>apt install -y vim wget sudo curl <br></code></pre></td></tr></table></figure><h1 id="安装-Elasticsearch"><a href="#安装-Elasticsearch" class="headerlink" title="安装 Elasticsearch"></a>安装 Elasticsearch</h1><h2 id="创建用于存放安装包和软件的目录"><a href="#创建用于存放安装包和软件的目录" class="headerlink" title="创建用于存放安装包和软件的目录"></a>创建用于存放安装包和软件的目录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /root/software<br>mkdir -p /root/install_packages<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 下载安装包</span></span> <br>```shell<br>if [ $(uname -m ) == &quot;aarch64&quot;]; then<br> wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.11.1-linux-aarch64.tar.gz -P /root/install_packages/<br>else then<br> wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.11.1-linux-x86_64.tar.gz -P /root/install_packages/<br>fi <br></code></pre></td></tr></table></figure><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf /root/install_packages/elasticsearch-*.tar.gz -C /opt/<br>rm -f /root/install_packages/elasticsearch-*.tar.gz<br></code></pre></td></tr></table></figure><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# elasticsearch&quot; &gt;&gt; /etc/profile<br>echo &quot;export ELASTICSEARCH_HOME=/opt/elasticsearch-8.11.1&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$ELASTICSEARCH_HOME/bin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h2 id="加载环境变量"><a href="#加载环境变量" class="headerlink" title="加载环境变量"></a>加载环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><h2 id="创建-logs-和-data-目录"><a href="#创建-logs-和-data-目录" class="headerlink" title="创建 logs 和 data 目录"></a>创建 logs 和 data 目录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p $ELASTICSEARCH_HOME/logs<br>mkdir -p $ELASTICSEARCH_HOME/data<br></code></pre></td></tr></table></figure><h2 id="创建-用户"><a href="#创建-用户" class="headerlink" title="创建 用户"></a>创建 用户</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">useradd -s /bin/bash -m elasticsearch<br>echo &quot;elasticsearch:123456&quot; | sudo chpasswd<br></code></pre></td></tr></table></figure><h2 id="修改权限"><a href="#修改权限" class="headerlink" title="修改权限"></a>修改权限</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">chown -R elasticsearch:elasticsearch $ELASTICSEARCH_HOME<br>chmod -R 777 $ELASTICSEARCH_HOME<br></code></pre></td></tr></table></figure><h2 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">禁用安全认证</span><br>sed -i &quot;s/xpack.security.enabled: true/xpack.security.enabled: false/g&quot;  $ELASTICSEARCH_HOME/config/elasticsearch.yml<br></code></pre></td></tr></table></figure><h2 id="启动-ES"><a href="#启动-ES" class="headerlink" title="启动 ES"></a>启动 ES</h2><p>su - elasticsearch &lt;&lt; EOF<br>elasticsearch -d<br>EOF</p><h2 id="测试-ES"><a href="#测试-ES" class="headerlink" title="测试 ES"></a>测试 ES</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl localhost:9200<br></code></pre></td></tr></table></figure><p>将看到大致如下的响应：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs text">&#123;<br>  &quot;name&quot; : &quot;cb222c82d092&quot;,<br>  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,<br>  &quot;cluster_uuid&quot; : &quot;eZ8SgaMQQTC711gqPCzPAA&quot;,<br>  &quot;version&quot; : &#123;<br>    &quot;number&quot; : &quot;8.11.1&quot;,<br>    &quot;build_flavor&quot; : &quot;default&quot;,<br>    &quot;build_type&quot; : &quot;tar&quot;,<br>    &quot;build_hash&quot; : &quot;6f9ff581fbcde658e6f69d6ce03050f060d1fd0c&quot;,<br>    &quot;build_date&quot; : &quot;2023-11-11T10:05:59.421038163Z&quot;,<br>    &quot;build_snapshot&quot; : false,<br>    &quot;lucene_version&quot; : &quot;9.8.0&quot;,<br>    &quot;minimum_wire_compatibility_version&quot; : &quot;7.17.0&quot;,<br>    &quot;minimum_index_compatibility_version&quot; : &quot;7.0.0&quot;<br>  &#125;,<br>  &quot;tagline&quot; : &quot;You Know, for Search&quot;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="安装-head-插件"><a href="#安装-head-插件" class="headerlink" title="安装 head 插件"></a>安装 head 插件</h3><p>通过 Google Chrome 插件商城安装即可</p><hr><p>Elasticsearch 安装完成，开始探索吧。</p>]]></content>
    
    
    <categories>
      
      <category>Elasticsearch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>ELK</tag>
      
      <tag>Elasticsearch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kibana 安装</title>
    <link href="/2025/05/13/elk/Manual-Install-Kibana/"/>
    <url>/2025/05/13/elk/Manual-Install-Kibana/</url>
    
    <content type="html"><![CDATA[<h1 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h1><h2 id="拉取ubuntu镜像"><a href="#拉取ubuntu镜像" class="headerlink" title="拉取ubuntu镜像"></a>拉取ubuntu镜像</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull ubuntu:22.04<br></code></pre></td></tr></table></figure><h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -itd --privileged -p 5601:5601 --name kibana ubuntu:22.04<br></code></pre></td></tr></table></figure><h2 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec -it kibana bash<br></code></pre></td></tr></table></figure><p>以下操作均在容器内部</p><h2 id="apt-install"><a href="#apt-install" class="headerlink" title="apt install"></a>apt install</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">国内网络切换 阿里云镜像</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">sed -i <span class="hljs-string">&quot;s@ports.ubuntu.com@mirrors.aliyun.com@g&quot;</span> /etc/apt/sources.list</span><br>apt clean &amp;&amp; apt update<br>apt install -y vim wget curl <br></code></pre></td></tr></table></figure><h1 id="安装-Kibana"><a href="#安装-Kibana" class="headerlink" title="安装 Kibana"></a>安装 Kibana</h1><h2 id="创建用于存放安装包和软件的目录"><a href="#创建用于存放安装包和软件的目录" class="headerlink" title="创建用于存放安装包和软件的目录"></a>创建用于存放安装包和软件的目录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /root/software<br>mkdir -p /root/install_packages<br></code></pre></td></tr></table></figure><h2 id="下载安装包"><a href="#下载安装包" class="headerlink" title="下载安装包"></a>下载安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">if [ $(uname -m ) == &quot;aarch64&quot;]; then<br> wget https://artifacts.elastic.co/downloads/kibana/kibana-8.11.1-linux-aarch64.tar.gz -P /root/install_packages/<br>else then<br> wget https://artifacts.elastic.co/downloads/kibana/kibana-8.11.1-linux-x86_64.tar.gz -P /root/install_packages/<br>fi <br></code></pre></td></tr></table></figure><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf /root/install_packages/kibana-*.tar.gz -C /root/software/<br></code></pre></td></tr></table></figure><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# kibana&quot; &gt;&gt; /etc/profile<br>echo &quot;export KIBANA_HOME=/root/software/kibana-8.11.1&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$KIBANA_HOME/bin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h2 id="加载环境变量"><a href="#加载环境变量" class="headerlink" title="加载环境变量"></a>加载环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><h2 id="配置-Kibana"><a href="#配置-Kibana" class="headerlink" title="配置 Kibana"></a>配置 Kibana</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">server.host: 0.0.0.0</span><br>sed -i &quot;s/#server.host: \&quot;localhost\&quot;/server.host: 0.0.0.0/g&quot; $KIBANA_HOME/config/kibana.yml<br></code></pre></td></tr></table></figure><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kibana --allow-root &amp; <br></code></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl localhost:5601<br></code></pre></td></tr></table></figure><hr><p>后续可集成Elasticsearch</p>]]></content>
    
    
    <categories>
      
      <category>Kibana</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>ELK</tag>
      
      <tag>Kibana</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据工程与产品</title>
    <link href="/2025/01/01/Data-Project-And-Product/"/>
    <url>/2025/01/01/Data-Project-And-Product/</url>
    
    <content type="html"><![CDATA[<h1 id="数据工程与产品"><a href="#数据工程与产品" class="headerlink" title="数据工程与产品"></a>数据工程与产品</h1><p>我们想象一下，有一家企业，经历了三年的从0到1野蛮生长期，取得了很大的成就，扩展了很多的客户，沉淀了很多的数据；<br>同时也慢慢进入了发展缓慢期，公司规模越来越大，扩展新的业务越来越难，利润率也开始下降。<br>偶然听到外界嘈杂的声音中传来的一些关键字：大数据时代，数字化转型，便将视线转向了沉淀了许久的数据，想从这一堆堆数据中挖点什么有价值的东西出来。  </p><h2 id="数据工程"><a href="#数据工程" class="headerlink" title="数据工程"></a>数据工程</h2><h3 id="什么是数据工程"><a href="#什么是数据工程" class="headerlink" title="什么是数据工程"></a>什么是数据工程</h3><p>我们先看看什么是软件工程, 根据维基百科的定义，软件工程是：应用计算机科学理论和技术以及工程管理原则和方法，按预算和进度，实现满足用户要求的软件产品的定义、开发、和维护的工程或进行研究的学科。<br>简而言之就是，对某一特定需求进行系统化、模块化、规范化的一个过程。<br>数据工程也是如此，而数据工程的过程通常指的是：从数据的采集、存储、加工到应用的整个流程的工程化。  </p><h3 id="数据工程经历了哪些时代"><a href="#数据工程经历了哪些时代" class="headerlink" title="数据工程经历了哪些时代"></a>数据工程经历了哪些时代</h3><p>其实我们对数据计算或者存储的需求从古代就产生了，古时候钱庄存钱取钱背后的用算盘记账和对账等过程，与现代的数据工程如出一辙。只是相对来说需求少，体量小。<br>而从计算机时代以来，数据就有了翻天覆地的变化。</p><ul><li><p>计算机时代<br>在80、90年代，随着计算机慢慢普及，以及Excel的诞生，而企业财务人员的主要工具，从便携式计算器变成了小型计算机，保存数据和计算数据的主要手段也变成了Excel。<br>这时候企业的用户还相对较少，产生的数据也没有特别的多，基本上一到两个财务人员就能完成公司所需的分析工作。</p></li><li><p>信息化时代<br>在2000年左右，网页数据库等信息技术逐渐在各行各业深入应用，企业的业务流程逐渐自动化，很多企业都构建了自己的EPR系统，并且产生了很多的数据保存在数据库中。<br>这个时候已经有很多BI项目和工具产生，主要是通过关系型数据库提供的计算能力，再加上web 页面的图表来做报表的展示。</p></li><li><p>大数据时代<br>大数据时代，企业的数据从 GB 增长到 TB 甚至 PB，数据爆发式增长，传统关系型数据库如 Mysql&#x2F;Oracle&#x2F;SQL Server 等不能满足海量数据的计算要求<br>这时候出现了很多分布式计算引擎如Hadoop&#x2F;Spark，还有一些MPP架构的数据库如GreenPlum&#x2F;DeepGreen&#x2F;TiDB&#x2F;Redshift等，这些组件的出现解决了海量数据的计算和存储的问题。<br>在吞吐量不是大问题之后，很多公司开始追求时效性，也就是从批处理往流处理发展。从离线数仓到实时数仓，从数据仓库到数据湖，也衍生了很多流批一体的架构如Lambda架构和Kappa架构等等。<br>从战略角度来看，从CRM 发展到 DMP，从DMP 到CDP，再到数据中台，再到 数据网格 DataMesh。在这个时代爆发出了各种让人眼花缭乱的产品或者解决方案。。<br>本篇文章我们就简单介绍一下这些跟数据息息相关的内容。</p></li></ul><h3 id="数据工程包括哪些内容"><a href="#数据工程包括哪些内容" class="headerlink" title="数据工程包括哪些内容"></a>数据工程包括哪些内容</h3><p>有需求就会有市场，那我们先来看看都有哪些数据&#x2F;大数据相关的岗位  </p><ul><li>大数据平台开发工程师</li><li>大数据开发工程师  </li><li>数据开发工程师</li><li>数据仓库工程师</li><li>ETL工程师</li><li>数据平台开发工程师</li><li>数据分析师</li><li>数据挖掘工程师</li><li>数据科学家</li><li>算法&#x2F;机器学习工程师 等</li></ul><h2 id="大数据开发平台"><a href="#大数据开发平台" class="headerlink" title="大数据开发平台"></a>大数据开发平台</h2><p>所有的软件都要运行在硬件之上，而大数据的计算也都要运行在各种大数据组件之上，各种大数据组件集成起来提供数据计算能力的这集合，就是大数据开发平台。<br>大数据平台为数据工程师或者数据分析师等提供数据的开发能力（如计算、存储、分析、安全审计、监控告警等），在此平台之上做各种各样的数据开发</p><ul><li>自建平台&#x2F;私有云<br>企业自己采购硬件、搭建机房、维护设备、维护软件等所有人力和经济成本，均由企业自己承担。<br>这里没有将自建平台和私有云做区分，是因为相对于企业自身来说，都需要投入相同的成本，区别只是这个工作是在不同的部门而已。<ul><li>优点<ul><li>网络只对本地开放，安全性高</li><li>自定义程度高，灵活性高</li><li>可控性更高，服务器类型，软件版本等可以深度自定义</li><li>可以本地开发</li></ul></li><li>缺点<ul><li>运维成本高，需要为更多的基础设施付费，需要独立的运维团队</li></ul></li><li>例子<ul><li>自建 HUAWEI FusionInsight 集群</li><li>自建 CDH 集群等</li></ul></li></ul></li><li>云平台<br>企业租赁云厂商提供的PaaS服务，在此服务之上直接做数据开发<ul><li>优点<ul><li>开箱即用，无需太多额外的运维成本</li><li>按需付费，弹性扩容，无需在前期就投入大量的技术设施成本</li></ul></li><li>缺点<ul><li>安全性不如本地自建平台，服务器在公网上，需要定义防火墙规则来限制访问，极小可能存在被侵入的风险</li><li>灵活性较差，PaaS提供的组件组合以及版本往往是固定的，并且几乎不可能使用到最新版本的开源组件</li></ul></li><li>例子<ul><li>AWS </li><li>Azure </li><li>阿里云 等</li></ul></li></ul></li></ul><h2 id="数据平台"><a href="#数据平台" class="headerlink" title="数据平台"></a>数据平台</h2><p>有了大数据开发平台，但测试这个平台只是具备了数据开发的能力，并没有提供任何数据。<br>用户想要做分析的时候还是需要经过繁琐的数据导入、数据清晰、建模等繁琐的过程，才能够真正地使用数据。<br>而数据平台的出现就是为了解决这些问题，为企业的数据分析挖掘、商业洞察、战略决策提供统一的、标准的、集成的、可靠的、易用的数据支撑服务。<br>常见的数据平台有数据仓库、数据湖、数据中台等，我们来看一看他们的异同。  </p><h3 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h3><p>数据仓库是诞生比较早的数据平台，广义上来说它不是一种具体的技术，而是一种方法、一套架构。它可以构建在如MySql&#x2F;Oracle等传统的RDBMS上，也可以构建在如Hadoop、Hive等分布式架构之上。<br>数据仓库主要是针对特定主题的结构化的数据，SQL是主要语言，一般采用维度建模的方式，有清晰完善的层级结构，主要包括ODS层、DW层、ADS层。  </p><h4 id="数据仓库分层"><a href="#数据仓库分层" class="headerlink" title="数据仓库分层"></a>数据仓库分层</h4><p>我们来看一张数据仓库层级图<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240716/image.1vyjgsxxt0.webp" alt="数据仓库分层">  </p><ul><li><p>ODS 层<br>ODS(Operational Data Store)层又称贴源层, 是从外部系统将操作型数据导入平台之后的第一层，保存了操作型数据最原始的状态。<br>数据从这一层开始将各种外部数据集成在数据平台中，并且无需重复从外部数据中重复采集相同的数据。  </p></li><li><p>DW 层<br>DW(Data Warehouse)层，是数仓中最核心的一个层级，其中包含维度层 DIM, 数据明细层 DWD, 数据服务层 DWS</p><ul><li><p>DIM (Dimension)<br>维度表中主要用来存放一些具有确定含义、不会随着业务发生变化的的维度信息，如时间维度、地区维度、用户维度、商品维度<br>其中如用户维度、商品维度等，虽然不会随着业务发生快速变化，但可能会随着公司发展阶段，用户体量等因素而缓慢变化，因此又被细分为缓慢变化维度 SCD<br>应对 SCD 的手段有很多种，如常用的拉链表等，这里不过多介绍，更多请参考 <a href="https://en.wikipedia.org/wiki/Slowly_changing_dimension">Wiki: Slowly changing dimension</a></p></li><li><p>DWD (Data Warehouse Details)<br>明细表中主要存放业务明细数据，在建模的过程中会做一些标准化处理，和维度退化处理：仅保留维度主键，在分析过程中，一个维度除了主键外，其他维度信息不参与计算，节省资源的同时，也可以让数据分析工作更加聚焦在业务，不受其他维度信息的干扰。   </p></li><li><p>DWS (Data Warehouse Service)<br>数据服务层按照部门的业务需求进行处理，产生的结果是针对特定主体的，如按雪花模型或星型模型做关联形成大宽表，便于进一步的数据分析，或按照特定维度(如以年&#x2F;月的时间维度)做聚合，形成相对公共的汇总数据等。</p></li></ul></li><li><p>ADS 层<br>ADS(Application Data Store)层，是面向应用的最顶层，上层应用直接从ADS层获取特定需求的结果数据，无需再在上层应用做额外的计算。<br>由于对接各种各样的上层应用，ADS层的数据也是多种多样，有按维度深度汇总的报表数据，便于BI团队做可视化；有按照业务属性将DWS表做深度关联，便于OLAP团队做即席查询，便于ML团队做数据挖掘；也有按指标模型构建的标签数据，方便运营团队做精细化运营，进准营销等等。<br>理想的情况，上层应用的每一个报表、或者其他依赖数据的服务，都对应着一张ADS层单独的表。</p></li></ul><h4 id="离线数据仓库架构"><a href="#离线数据仓库架构" class="headerlink" title="离线数据仓库架构"></a>离线数据仓库架构</h4><p>数据仓库早期主要聚焦在离线计算上，数据窗口可能以月甚至年为单位，延迟多为T+1，即当天能看到前一天的数据即可。<br>我们来一张离线数据仓库架构图<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240719/image.86tjiu2bbt.webp" alt="离线数据仓库架构"><br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240719/image.92q0ycab1r.webp" alt="离线数据仓库架构"><br>HDFS 作为分布式文件存储系统，存储能力可以近似认为无限；<br>Spark 作为分布式内存计算框架，计算能力也可以通过扩展节点来提升，从而支持处理任意规模的数据；<br>Hive 作为数据仓库组件，是集中管理数据仓库模型中各种数据库和表的地方，其数据存储在HDFS上，计算引擎默认是MapReduce，也可用Spark进行替换，以提升处理性能。<br>离线数仓使用的技术决定了其能够支持大规模数据集的计算，也导致其具有相对较高的延迟，在准实时计算领域发挥受到限制。  </p><h4 id="实时数据仓库架构"><a href="#实时数据仓库架构" class="headerlink" title="实时数据仓库架构"></a>实时数据仓库架构</h4><p>为了能让数据仓库的延迟更低，让业务洞察的反馈更快，实时数仓应运而生<br>我们再来看一张实时数仓架构图<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240719/image.26lde5yylh.webp" alt="实时数据仓库架构"><br>实数数仓中替换了造成延迟的主要组件，将批计算引擎换成流计算引擎，将离线数据仓库组件换成更为实时的消息中间件<br>Kafka 作为流处理中的消息中间件，不同层级的不同的topic中保存不同属性的数据；<br>SparkStreaming 或 Flink 作为实时计算框架，能够让单个作业的延迟最低控制在秒级；<br>由于流式处理组件的特性，整个链路的延迟能控制在分钟级甚至秒级，但一般只处理增量数据，数据准确性相对不如离线计算。<br>为了同时满足批处理和流处理两个场景，也催化了一些流批一体的架构  </p><h4 id="Lambda-架构"><a href="#Lambda-架构" class="headerlink" title="Lambda 架构"></a>Lambda 架构</h4><p>Lambda 架构是由离线数仓和实时数仓两部分组成的，从需求上同时满足了全量数据和增量数据的计算，并在准确性和效率上取得了平衡<br>先看一张Lambda架构图<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240719/image.361grfu29l.webp" alt="Lambda架构"><br>同一个指标模型同时在Batch Layer和RealTime Layer中进行相同的逻辑，分别产生Batch Table 和 RealTime Table, Server Layer 层将两个用Union或其他方式进行合并，最为最终的ADS层。<br>Batch Layer中的数据为全量的数据，计算出来的结果更加准确，但最多只能显示昨天的数据；RealTime Layer 对今天的增量数据做补充，来提升数据的实时性，<br>但可能会由于数据的变更（比如交易关闭，订单取消等，需要对历史数据进行重新计算等场景）导致数据准确性不高, 需要再第二天的Batch Layer中重新计算，或可称之为数据校准。<br>因此在 Server Layer 将 Batch Table 昨天的数据和 RealTime Table 今天的数据进行合并， 从而兼顾实时性和准确性。<br>显而易见，Lambda架构的缺点也非常明显，就是需要同时维护两套相同业务逻辑的代码，并且技术栈都不太一样，导致开发成本、维护成本以及资源成本都相对较高。<br>为了解决 Lambda 架构下的痛点，LinkedIn 提出了 Kappa 架构  </p><h4 id="Kappa-架构"><a href="#Kappa-架构" class="headerlink" title="Kappa 架构"></a>Kappa 架构</h4><p>Kappa 架构的核心思路是，删除Batch Layer，改进Lambda架构中的Speed Layer，使其能够支持历史数据的计算<br>从架构上采用了纯实时架构，架构图可以参考 <a href="#%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84">实时数据仓库架构</a> 章节<br>Kappa 架构认为在保证处理流程完善的前提下，增量计算的数据就是准确的，通常不需要对历史数据进行计算，只有很少的场景需要重算所有数据。<br>消息队列中的消息通常具有时效性，为了支持重新计算，需要从业务库或者备份库重新拉取全量数据。<br>并且为实时数仓中的Topic创建一个孪生Topic，用来保存计算历史数据时的中间层级的数据，避免污染运行中的作业模型。<br>在需要重新计算时，只需要启动一个额外的任务，将消费的偏移量设置为0，即可从头开始消费，从而重新计算所有的数据。<br>不难看出Kappa架构对消息中间件的要求非常高，需要很高的吞满足计算全量的数据；并且不像离线数据仓库那样，可以查询任意层级，实现灵活的OLAP需求；同时做数据治理的难度很大，比如数据血缘的管理，数据权限的管理等；</p><h4 id="数据仓库痛点"><a href="#数据仓库痛点" class="headerlink" title="数据仓库痛点"></a>数据仓库痛点</h4><p>从上述的分层结构可以看出，不管是离线数仓还是实时数仓，都有着非常规范的结构，和较高的数据质量。<br>但缺点也比较明显，数仓团队需要为每一个特定主题的数据来在不同层级构建不同的表，从需求提出，到数据可用，所经历的周期相对较长。<br>数据基本上都是外部系统中的结构化数据，且都是先有需求再建模，整体提供的数据服务能力呈收敛的状态，相对不太容易能挖掘出更多的价值。<br>为了解决这些痛点，数据湖的架构慢慢发展起来。  </p><h3 id="数据湖"><a href="#数据湖" class="headerlink" title="数据湖"></a>数据湖</h3><p>数据湖(Data Lake)名字也比较形象，顾名思义它可以容纳各种各样的数据，如结构化数据(Table)、半结构化数据(JSON、XML、HTML)以及非结构化数据(文件，音频，视频等)<br>数据湖通常会统一存储企业的所有数据，既包括采集后的原始数据，也包括转换后的数据，转换的层级可以很少（甚至为零），也可以按照严格的层级进行建模，故而数据仓库可以是数据湖的一个真子集，也就是常见的湖仓一体架构。<br>也因为可以不受严格的层级约束，数据入湖提供服务的技术成本和时间成本都大大降低，数据湖可以很轻松地整合各种各样的企业内部数据或者第三方数据，从而打破数据孤岛；同时视数据的情况可以按需进行数据转换，甚至在数据比较规范及安全的情况下，无需转换直接向下游提供数据服务，加速数据产生价值的时间。<br>我们再来看数据湖层级图<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240716/image.6pnecxbeaq.webp" alt="数据湖层级图"><br>所以数据湖相对于数据仓库来说，可以面向任意业务，授权用户可以以任意速度，任意语言，访问数据湖的任意层级的数据。<br>广义上的数据湖是一套架构，一套体系，一套完整的方法论，而狭义上的数据湖则是由这套架构催化的存储数据的具体的技术组件。<br>离线数据仓库中的操作多为文件操作，仅支持读和写，不支持更新；即使可以通过逻辑层面实现更新的功能，本质上也是将整个文件删除和覆写，延迟相对较高，在准实时场景下容易成为性能瓶颈。<br>行业头部公司为了解决这些痛点，各自出现了自己的解决方案，比较知名的 Databrics 的 Dalta Lake，Uber 开源的 Hudi，Netflix 开源的 Iceberg，也就是常说的”数据湖三剑客”。<br>它们的核心都是定义了一种全新的表格式，即在对分布式文件系统之上做了一层封装，对文件进行标记，区分有效数据或已删除数据，通过引入一级索引文件和二级索引文件来对数据进行文件级的过滤和整合，从而实现快速ACID的语义。  </p><h3 id="数据中台"><a href="#数据中台" class="headerlink" title="数据中台"></a>数据中台</h3><p>数据中台是企业级能力复用和赋能平台, 是数据驱动业务场景下孵化的产物, 以一种标准的、安全的、可靠的、统一的、共享的、解耦的、服务化的方式支持前端数据的应用。<br>数据是业务的一种展现形式，业务是产生数据的途径或者手段，当企业的业务足够复杂时，不同的业务部门就无可避免地就产生一些重复的业务模型，并产生重复的数据模型<br>比如阿里巴巴旗下有淘宝、天猫、咸鱼等等不同的产品，每个产品都会涉及到销售、订单、物流、支付等一系列业务。如果对每一个产品的每一个业务模型进行数据平台层面的建模，可想而知重复的工作非常之多，模型建设各自孤立，数据标准难以统一，并且想推出一个新产品时，这些重复的工作也不可避免地需要重做一次。<br>随着用户数量的增长、用户消费习惯的改变、以及市场的多变，传统的数据平台难以及时作出调整，无法快速为业务提供数据支撑，这时就需要一个能够快速响应市场变化的统一的企业级平台架构。<br>而也随着微服务的实践越来越成熟，我们不难想到，可以用微服务的思想来解决这类问题，将可复用的业务能力，按DDD（领域驱动设计）的原则对业务进行拆分和重组，形成一个统一的业务中台；再对产生的领域数据进行统一建模，产生的指标数据反哺到业务中去，即可形成一个统一的数据中台。<br>这也就是 DataMesh 的核心思路 (数据平台 + 领域驱动设计 + 微服务 &#x3D; 数据中台), 数据中台是道，是企业级战略思想；DataMesh 是法，是技术方法论；数据工程、微服务等是术, 是建设中台的手段；各项技术组件是器，是建设中台的工具。<br><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240719/image.4xuftvkwoq.webp" alt="数据中台架构图"><br>可以看出，数据中台的建设成本高，周期长，不是一个或者几个部门能短时间就能完成的事情，它需要企业各个部门之间协作，对齐目标和愿景，从企业顶层视角进行统一设计和建设。<br>因此，在中小型企业，业务没有那么复杂，并且传统数仓或数据湖架构能够满足战略目标时，数据中台的建设反而会引入过高的技术&#x2F;架构复杂度和综合成本。<br>介绍完技术视角的数据平台，我们来看看业务视角下的数据产品。  </p><h2 id="数据产品"><a href="#数据产品" class="headerlink" title="数据产品"></a>数据产品</h2><p>数据产品就是，能够为用户提供数据服务，产生直接或间接价值的应用或者系统。<br>前面所讲到的数据平台都能够为企业提供数据服务，也可以称之为数据产品，但是主要关注在技术视角，即如何利用技术来完成平台的建设，以满足业务需求。<br>我们来看看从业务视角的定位，有哪些数据产品  </p><h3 id="CRM"><a href="#CRM" class="headerlink" title="CRM"></a>CRM</h3><p>CRM(Custom Relationship Management)系统，是管理客户数据的平台，向企业或营销方提供管理或分析上下游，客户渠道，营销情况等能力。  </p><ul><li>特点<ul><li>管理上下游之间的数据</li><li>高时效性：业务数据低延迟</li></ul></li><li>使用场景<ul><li>营销，销售等领域的客户管理、上下游&#x2F;供应链管理等</li></ul></li><li>案例:<ul><li>纷享销客CRM, Salesforce, Zuora</li></ul></li></ul><h3 id="DMP"><a href="#DMP" class="headerlink" title="DMP"></a>DMP</h3><p>DMP(Data Management Platform)，其定位主要是通过标签体系和圈选人群来提供精准投放的服务。   </p><ul><li>特点<ul><li>管理用户标签数据</li><li>匿名数据，只关心用户群体，不关心具体的用户</li></ul></li><li>使用场景<ul><li>电商平台，内容平台等定向推送</li></ul></li><li>案例<ul><li>淘宝-达摩盘, 抖音-巨量千川</li></ul></li></ul><h3 id="CDP"><a href="#CDP" class="headerlink" title="CDP"></a>CDP</h3><p>CDP(Custom Data Platform)系统，是客户数据的管理和分析平台，CDP包含CRM和DMP提供的能力，并通过对数据的分析和挖掘，来向企业提供精细化运营的能力。  </p><ul><li>特点<ul><li>CRM + DMP + 精细化运营</li></ul></li><li>使用场景<ul><li>管理企业客户数据，上下游数据等</li><li>广告投放</li><li>分析和优化现有业务营收及投入成本，降本增效，实现精细化运营。</li><li>商业洞察和战略规划</li></ul></li><li>案例<ul><li>各个企业的数据平台，企业年报背后的技术支撑平台</li><li>GENE DataLake - MCDP</li></ul></li></ul><p>以上几个数据产品都只聚焦在数据能为用户带来的价值，通常产品经理需要考虑自己需要哪一类数据产品；<br>而其底层是使用传统数据库还是需要建设数据平台，则需要数据架构师来根据企业的战略规划来选择最适合的平台架构。 </p><h2 id="数据工程的价值"><a href="#数据工程的价值" class="headerlink" title="数据工程的价值"></a>数据工程的价值</h2><h3 id="一般企业如何产生价值"><a href="#一般企业如何产生价值" class="headerlink" title="一般企业如何产生价值"></a>一般企业如何产生价值</h3><ul><li>商贸企业：<br>经营模式主要为：采购 -&gt; 销售，特点是成本低，利润低，周期短，比如超市，服装经销等。  </li><li>生产企业：<br>经营模式主要为：采购原材料 -&gt; 生产加工 -&gt; -&gt; 销售，特点是成本低，利润高，周期长；比如产品型公司，如护肤品厂商，服装厂商等。  </li><li>互联网企业：<br>经营模式主要为：开发软件 -&gt; 销售软件&#x2F;提供服务； 按使用收费: 前期投入高，边际成本低，用户多，利润高；如电商平台，外卖平台，内容平台等。</li></ul><h3 id="数据产品如何产生价值"><a href="#数据产品如何产生价值" class="headerlink" title="数据产品如何产生价值"></a>数据产品如何产生价值</h3><ul><li>直接价值<br>目前数据交易的法律法规还在逐步完善，能够直接提供价值的数据产品并不多<br>能够提供数据赋能的服务: 如CRM-SaaS，以及用于精准投放的DMP数据包等  </li><li>间接价值<br>大部分数据产品都是间接提供价值，即通过数据分析和挖掘等手段，为上层应用提供分析或决策数据，数据从业务中来，反哺到业务中去，降本增效，优化现有业务，帮助上层应用实现价值增长。  <ul><li>CRM: 精细化运营，提升运营效率，提升用户粘性，淘汰低ROI业务线等</li><li>DMP: 精准营销, 广告投放</li><li>CDP: CRM + DMP + More： 风险预警，数据分析和挖掘，ML + AI等。</li></ul></li><li>创新价值<br>数据中台与其他数据产品相比，有其天然的优势，即通过数据资产或业务能力的复用和重组，来实现组合式创新，扩展新的业务线。<br>如盒马鲜生的诞生，通过复用和组合中台的用户、交易、支付等现有业务能力，形成了线上线下一体的新零售业务线。<br>反观盒马先生的经营模式，也基本上都是已有业务的不同形态的组合。也因为这一点，数据中台往往也只能实现组合式创新，很难做到颠覆式创新。</li><li>颠覆式创新<br>在 ChatGPT 问世之前，ML和AI发展了很长一段时间，但是用户从中获益甚少，基本上处于玩具级定位，人工智能被广大用户戏称为人工智障。<br>随着算力的发展和数据的累积以及对算法的掌握，2022年ChatGPT问世之后，NLP(自然语言处理)领域有了全新的突破，同时也掀起了大模型的浪潮，图像领域、视频领域、音频领域等各种大模型爆发式出现。<br>庞大的数据体量以及对数据的应用，在接下来的人工智能时代的数据训练将会扮演非常重要的角色。<br>我们也来期待一下AI会给我们带来怎样的创新。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Data Engineering</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Data Engineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hadoop 伪分布式安装</title>
    <link href="/2025/01/01/Manual-Install-Hadoop/"/>
    <url>/2025/01/01/Manual-Install-Hadoop/</url>
    
    <content type="html"><![CDATA[<h1 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h1><h2 id="拉取ubuntu镜像，启动容器"><a href="#拉取ubuntu镜像，启动容器" class="headerlink" title="拉取ubuntu镜像，启动容器"></a>拉取ubuntu镜像，启动容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull ubuntu:latest<br></code></pre></td></tr></table></figure><h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -itd --privileged -p 9870:9870 -p 8088:8088 -p 4040:4040 -p 10000:10000 --name hadoop-manual ubuntu:latest<br></code></pre></td></tr></table></figure><h2 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec -it hadoop-manual bash<br></code></pre></td></tr></table></figure><p>以下操作均在容器内部</p><h1 id="配置-SSH-免密"><a href="#配置-SSH-免密" class="headerlink" title="配置 SSH 免密"></a>配置 SSH 免密</h1><h2 id="安装SSH服务"><a href="#安装SSH服务" class="headerlink" title="安装SSH服务"></a>安装SSH服务</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">国内网络切换 阿里云镜像</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">sed -i <span class="hljs-string">&quot;s@ports.ubuntu.com@mirrors.aliyun.com@g&quot;</span> /etc/apt/sources.list</span><br>apt clean &amp;&amp; apt update<br>apt install -y vim openssh-server <br>/etc/init.d/ssh start<br></code></pre></td></tr></table></figure><h2 id="ssh-免密"><a href="#ssh-免密" class="headerlink" title="ssh 免密"></a>ssh 免密</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-keygen -t rsa -N &#x27;&#x27; -f ~/.ssh/id_rsa -q<br>cat ~/.ssh/*.pub &gt; ~/.ssh/authorized_keys &amp;&amp; chmod 600 ~/.ssh/authorized_keys<br>sed -i &quot;s/#   StrictHostKeyChecking ask/   StrictHostKeyChecking no/g&quot; /etc/ssh/ssh_config  <br></code></pre></td></tr></table></figure><h1 id="安装软件"><a href="#安装软件" class="headerlink" title="安装软件"></a>安装软件</h1><h2 id="创建用于存放安装包和软件的目录"><a href="#创建用于存放安装包和软件的目录" class="headerlink" title="创建用于存放安装包和软件的目录"></a>创建用于存放安装包和软件的目录</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /root/software<br>mkdir -p /root/install_packages<br></code></pre></td></tr></table></figure><h2 id="安装-JDK"><a href="#安装-JDK" class="headerlink" title="安装 JDK"></a>安装 JDK</h2><h3 id="apt-安装-jdk8"><a href="#apt-安装-jdk8" class="headerlink" title="apt 安装 jdk8"></a>apt 安装 jdk8</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">apt install -y openjdk-8-jdk<br></code></pre></td></tr></table></figure><h3 id="配置-JAVA-HOME"><a href="#配置-JAVA-HOME" class="headerlink" title="配置 JAVA_HOME"></a>配置 JAVA_HOME</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# java&quot; &gt;&gt; /etc/profile<br>echo &quot;export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64/&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$JAVA_HOME/bin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h2 id="安装-Hadoop"><a href="#安装-Hadoop" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h2><h3 id="下载-Hadoop-安装包"><a href="#下载-Hadoop-安装包" class="headerlink" title="下载 Hadoop 安装包"></a>下载 Hadoop 安装包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">if [ $(uname -m ) == &quot;aarch64&quot;]; then<br>  <br> wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.5/hadoop-3.3.5-aarch64.tar.gz -P /root/install_packages/<br>  <br>else then<br> wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz -P /root/install_packages/<br> <br>  <br>fi <br>wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz -P /root/install_packages/<br><span class="hljs-meta prompt_"># </span><span class="language-bash">下载较慢，可以使用已下载的本地tar包</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">docker <span class="hljs-built_in">cp</span> ./hadoop-3.3.5*.tar.gz hadoop-manual://root/install_packages/</span><br></code></pre></td></tr></table></figure><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf /root/install_packages/hadoop-3.3.5*.tar.gz -C /root/software/<br></code></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# hadoop&quot; &gt;&gt; /etc/profile<br>echo &quot;export HADOOP_HOME=/root/software/hadoop-3.3.5&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h3 id="加载环境变量"><a href="#加载环境变量" class="headerlink" title="加载环境变量"></a>加载环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><h1 id="配置-Hadoop"><a href="#配置-Hadoop" class="headerlink" title="配置 Hadoop"></a>配置 Hadoop</h1><h2 id="创建-dfs-目录"><a href="#创建-dfs-目录" class="headerlink" title="创建 dfs 目录"></a>创建 dfs 目录</h2><ul><li>name: NameNode dir</li><li>data: DataNode dir</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p $&#123;HADOOP_HOME&#125;/dfs/name<br>mkdir -p $&#123;HADOOP_HOME&#125;/dfs/data<br></code></pre></td></tr></table></figure><h2 id="修改-core-site-xml"><a href="#修改-core-site-xml" class="headerlink" title="修改 core-site.xml"></a>修改 core-site.xml</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &quot;/configuration/d&quot; $&#123;HADOOP_HOME&#125;/etc/hadoop/core-site.xml<br>cat &lt;&lt; EOF &gt;&gt; $&#123;HADOOP_HOME&#125;/etc/hadoop/core-site.xml<br>&lt;configuration&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;fs.defaultFS&lt;/name&gt;<br>        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br>EOF<br></code></pre></td></tr></table></figure><h2 id="修改-hdfs-site-xml"><a href="#修改-hdfs-site-xml" class="headerlink" title="修改 hdfs-site.xml"></a>修改 hdfs-site.xml</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &quot;/configuration/d&quot; $&#123;HADOOP_HOME&#125;/etc/hadoop/hdfs-site.xml<br>cat &lt;&lt; EOF &gt;&gt; $&#123;HADOOP_HOME&#125;/etc/hadoop/hdfs-site.xml<br>&lt;configuration&gt;<br>    &lt;property&gt;<br>            &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br>            &lt;value&gt;file:$&#123;HADOOP_HOME&#125;/dfs/name&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>            &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br>            &lt;value&gt;file:$&#123;HADOOP_HOME&#125;/dfs/data&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br>EOF<br></code></pre></td></tr></table></figure><h2 id="修改-yarn-site-xml"><a href="#修改-yarn-site-xml" class="headerlink" title="修改 yarn-site.xml"></a>修改 yarn-site.xml</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &quot;/configuration/d&quot; $&#123;HADOOP_HOME&#125;/etc/hadoop/yarn-site.xml<br>cat &lt;&lt; EOF &gt;&gt; $&#123;HADOOP_HOME&#125;/etc/hadoop/yarn-site.xml<br>&lt;configuration&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;<br>        &lt;value&gt;localhost:8032&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;<br>        &lt;value&gt;localhost:8030&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;<br>        &lt;value&gt;localhost:8031&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;<br>        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br>EOF<br></code></pre></td></tr></table></figure><h2 id="修改-mapred-site-xml"><a href="#修改-mapred-site-xml" class="headerlink" title="修改 mapred-site.xml"></a>修改 mapred-site.xml</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &quot;/configuration/d&quot; $&#123;HADOOP_HOME&#125;/etc/hadoop/mapred-site.xml<br>cat &lt;&lt; EOF &gt;&gt; $&#123;HADOOP_HOME&#125;/etc/hadoop/mapred-site.xml<br>&lt;configuration&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>        &lt;value&gt;yarn&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;<br>        &lt;value&gt;HADOOP_MAPRED_HOME=/root/software/hadoop-3.3.5&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;mapreduce.map.env&lt;/name&gt;<br>        &lt;value&gt;HADOOP_MAPRED_HOME=/root/software/hadoop-3.3.5&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;<br>        &lt;value&gt;HADOOP_MAPRED_HOME=/root/software/hadoop-3.3.5&lt;/value&gt;<br>    &lt;/property&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;<br>        &lt;value&gt;/root/software/hadoop-3.3.5/share/hadoop/mapreduce/*:/root/software/hadoop-3.3.5/share/hadoop/mapreduce/lib/*&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br>EOF<br></code></pre></td></tr></table></figure><h2 id="修改-hadoop-env-sh"><a href="#修改-hadoop-env-sh" class="headerlink" title="修改 hadoop-env.sh"></a>修改 hadoop-env.sh</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &quot;s/# export HDFS_NAMENODE_USER=hdfs/export HDFS_NAMENODE_USER=root\nexport HDFS_DATANODE_USER=root\nexport HDFS_SECONDARYNAMENODE_USER=root\nexport YARN_NODEMANAGER_USER=root\nexport YARN_RESOURCEMANAGER_USER=root\n/g&quot; $&#123;HADOOP_HOME&#125;/etc/hadoop/hadoop-env.sh<br>sed -i &quot;s?# export JAVA_HOME=?export JAVA_HOME=$&#123;JAVA_HOME&#125;?g&quot; $&#123;HADOOP_HOME&#125;/etc/hadoop/hadoop-env.sh<br></code></pre></td></tr></table></figure><h2 id="格式化-hdfs"><a href="#格式化-hdfs" class="headerlink" title="格式化 hdfs"></a>格式化 hdfs</h2><p>有 has been successfully formatted 即为成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs namenode -format<br></code></pre></td></tr></table></figure><h1 id="启动-Hadoop"><a href="#启动-Hadoop" class="headerlink" title="启动 Hadoop"></a>启动 Hadoop</h1><h2 id="启动-HDFS"><a href="#启动-HDFS" class="headerlink" title="启动 HDFS"></a>启动 HDFS</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">start-dfs.sh<br></code></pre></td></tr></table></figure><h2 id="访问-HDFS-WEB-UI"><a href="#访问-HDFS-WEB-UI" class="headerlink" title="访问 HDFS WEB UI"></a>访问 HDFS WEB UI</h2><p><a href="http://localhost:9870/">http://localhost:9870</a></p><h2 id="启动-YARN"><a href="#启动-YARN" class="headerlink" title="启动 YARN"></a>启动 YARN</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">start-yarn.sh<br></code></pre></td></tr></table></figure><h2 id="访问-YARN-WEB-UI"><a href="#访问-YARN-WEB-UI" class="headerlink" title="访问 YARN WEB UI"></a>访问 YARN WEB UI</h2><p><a href="http://localhost:8088/">http://localhost:8088</a></p>]]></content>
    
    
    <categories>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive 安装</title>
    <link href="/2025/01/01/Manual-Install-Hive/"/>
    <url>/2025/01/01/Manual-Install-Hive/</url>
    
    <content type="html"><![CDATA[<h1 id="安装Hive"><a href="#安装Hive" class="headerlink" title="安装Hive"></a>安装Hive</h1><h2 id="创建和启动-Hadoop"><a href="#创建和启动-Hadoop" class="headerlink" title="创建和启动 Hadoop"></a>创建和启动 Hadoop</h2><p>参考 <a href="Manual-Install-Hadoop.md">Manual-Install-Hadoop.md</a></p><p>启动后进入容器, 之后的操作均在容器内进行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec -it hadoop-manual bash<br></code></pre></td></tr></table></figure><h2 id="安装-MySQL"><a href="#安装-MySQL" class="headerlink" title="安装 MySQL"></a>安装 MySQL</h2><h3 id="apt-install"><a href="#apt-install" class="headerlink" title="apt install"></a>apt install</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">apt install -y mysql-server<br></code></pre></td></tr></table></figure><h3 id="启动-MySQL-服务"><a href="#启动-MySQL-服务" class="headerlink" title="启动 MySQL 服务"></a>启动 MySQL 服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">service mysql start<br></code></pre></td></tr></table></figure><h3 id="修改密码和配置"><a href="#修改密码和配置" class="headerlink" title="修改密码和配置"></a>修改密码和配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">mysql -uroot &lt;&lt; EOF<br>ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;<br>EOF<br></code></pre></td></tr></table></figure><h2 id="安装-Hive"><a href="#安装-Hive" class="headerlink" title="安装 Hive"></a>安装 Hive</h2><h3 id="下载安装包"><a href="#下载安装包" class="headerlink" title="下载安装包"></a>下载安装包</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz -P /root/install_packages/<br></code></pre></td></tr></table></figure><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf /root/install_packages/apache-hive-3.1.3-bin.tar.gz -C /root/software/<br></code></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# hive&quot; &gt;&gt; /etc/profile<br>echo &quot;export HIVE_HOME=/root/software/apache-hive-3.1.3-bin&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$HIVE_HOME/bin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h3 id="加载环境变量"><a href="#加载环境变量" class="headerlink" title="加载环境变量"></a>加载环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><h2 id="配置-Hive"><a href="#配置-Hive" class="headerlink" title="配置 Hive"></a>配置 Hive</h2><h3 id="拷贝配置文件"><a href="#拷贝配置文件" class="headerlink" title="拷贝配置文件"></a>拷贝配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">HIVE_CONF_DIR=$&#123;HIVE_HOME&#125;/conf<br>cp $&#123;HIVE_CONF_DIR&#125;/hive-default.xml.template $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br></code></pre></td></tr></table></figure><h3 id="配置-JDBC-信息"><a href="#配置-JDBC-信息" class="headerlink" title="配置 JDBC 信息"></a>配置 JDBC 信息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &quot;s#jdbc:derby:;databaseName=metastore_db;create=true#jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true#g&quot; $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br>sed -i &quot;s/org.apache.derby.jdbc.EmbeddedDriver/com.mysql.jdbc.Driver/g&quot; $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br>sed -i &quot;s/APP/root/g&quot; $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br>sed -i &quot;s#&lt;value&gt;mine&lt;/value&gt;#&lt;value&gt;123456&lt;/value&gt;#g&quot; $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br></code></pre></td></tr></table></figure><h3 id="修改其他配置"><a href="#修改其他配置" class="headerlink" title="修改其他配置"></a>修改其他配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">替换异常字符</span><br>sed -i &quot;s/\&amp;\#8\;/ /g&quot; $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br><span class="hljs-meta prompt_"># </span><span class="language-bash">替换系统文件路径</span><br>sed -i &quot;s#\$&#123;system:java.io.tmpdir&#125;#$&#123;HIVE_HOME&#125;/temp#g&quot; $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br>sed -i &quot;s#\$&#123;system:user.name&#125;#\$&#123;user.name&#125;#g&quot; $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br><span class="hljs-meta prompt_"># </span><span class="language-bash">关闭 doAs</span><br>sed -i &#x27;4821s/true/false/&#x27; $&#123;HIVE_CONF_DIR&#125;/hive-site.xml<br><br></code></pre></td></tr></table></figure><h3 id="添加-JDBC驱动"><a href="#添加-JDBC驱动" class="headerlink" title="添加 JDBC驱动"></a>添加 JDBC驱动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget -P $&#123;HIVE_HOME&#125;/lib https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar<br></code></pre></td></tr></table></figure><h3 id="创建-hdfs-目录"><a href="#创建-hdfs-目录" class="headerlink" title="创建 hdfs 目录"></a>创建 hdfs 目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs dfs -mkdir -p /user/hive/warehouse<br>hdfs dfs -mkdir -p /user/hive/tmp<br>hdfs dfs -mkdir -p /user/hive/log<br>hdfs dfs -chmod -R 777 /user/hive/warehouse<br>hdfs dfs -chmod -R 777 /user/hive/tmp<br>hdfs dfs -chmod -R 777 /user/hive/log<br></code></pre></td></tr></table></figure><h3 id="初始化元数据"><a href="#初始化元数据" class="headerlink" title="初始化元数据"></a>初始化元数据</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">schematool -initSchema -dbType mysql<br></code></pre></td></tr></table></figure><h2 id="启动-Hive"><a href="#启动-Hive" class="headerlink" title="启动 Hive"></a>启动 Hive</h2><h3 id="Embedded-CLI"><a href="#Embedded-CLI" class="headerlink" title="Embedded CLI"></a>Embedded CLI</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hive<br></code></pre></td></tr></table></figure><h3 id="通过-Thrift-协议访问-MetaStore"><a href="#通过-Thrift-协议访问-MetaStore" class="headerlink" title="通过 Thrift 协议访问 MetaStore"></a>通过 Thrift 协议访问 MetaStore</h3><p>启动 MetaStore </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hive --service metastore &amp;<br></code></pre></td></tr></table></figure><p>修改 hive-site.xml 启用metastore连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">&lt;property&gt;<br>  &lt;name&gt;hive.metastore.uris&lt;/name&gt;<br>  &lt;value&gt;thrift://localhost:9083&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><h3 id="Hive-Server-2"><a href="#Hive-Server-2" class="headerlink" title="Hive Server 2"></a>Hive Server 2</h3><p>启动 HiveServer2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hiveserver2 &amp; <br></code></pre></td></tr></table></figure><p>Beeline 连接 或 JDBC工具连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">beeline -u jdbc:hive2://localhost:10000<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark 安装</title>
    <link href="/2025/01/01/Manual-Install-Spark/"/>
    <url>/2025/01/01/Manual-Install-Spark/</url>
    
    <content type="html"><![CDATA[<h1 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h1><ul><li>JDK 1.8</li><li>Hadoop </li><li>Hive</li></ul><h1 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h1><h2 id="拉取ubuntu镜像，启动容器"><a href="#拉取ubuntu镜像，启动容器" class="headerlink" title="拉取ubuntu镜像，启动容器"></a>拉取ubuntu镜像，启动容器</h2><p>备注：此镜像包含Hive + Hadoop 环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull tangchenyang/hive:v0.1<br></code></pre></td></tr></table></figure><h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -itd --privileged -p 9870:9870 -p 8088:8088 -p 4040:4040 -p 10000:10000 --name hive-spark hive:v0.1<br></code></pre></td></tr></table></figure><h2 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec -it hive-spark bash<br></code></pre></td></tr></table></figure><p>以下操作均在容器内部</p><h1 id="安装-Scala"><a href="#安装-Scala" class="headerlink" title="安装 Scala"></a>安装 Scala</h1><h2 id="下载安装包"><a href="#下载安装包" class="headerlink" title="下载安装包"></a>下载安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget https://downloads.lightbend.com/scala/2.12.19/scala-2.12.19.tgz -P /root/install_packages/<br></code></pre></td></tr></table></figure><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf /root/install_packages/scala-2.12.19.tgz -C /root/software/<br></code></pre></td></tr></table></figure><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# scala&quot; &gt;&gt; /etc/profile<br>echo &quot;export SCALA_HOME=/root/software/scala-2.12.19&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$SCALA_HOME/bin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h2 id="加载环境变量"><a href="#加载环境变量" class="headerlink" title="加载环境变量"></a>加载环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><h1 id="安装-Spark"><a href="#安装-Spark" class="headerlink" title="安装 Spark"></a>安装 Spark</h1><h2 id="下载安装包-1"><a href="#下载安装包-1" class="headerlink" title="下载安装包"></a>下载安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz -P /root/install_packages/<br></code></pre></td></tr></table></figure><h2 id="解压-1"><a href="#解压-1" class="headerlink" title="解压"></a>解压</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf /root/install_packages/spark-3.5.1-bin-hadoop3.tgz  -C /root/software/<br></code></pre></td></tr></table></figure><h2 id="配置环境变量-1"><a href="#配置环境变量-1" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# spark&quot; &gt;&gt; /etc/profile<br>echo &quot;export SPARK_HOME=/root/software/spark-3.5.1-bin-hadoop3&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$SPARK_HOME/bin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h2 id="加载环境变量-1"><a href="#加载环境变量-1" class="headerlink" title="加载环境变量"></a>加载环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><h2 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">SPARK_CONF_DIR=$&#123;SPARK_HOME&#125;/conf<br></code></pre></td></tr></table></figure><h3 id="spark-env-sh"><a href="#spark-env-sh" class="headerlink" title="spark-env.sh"></a>spark-env.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">cp $&#123;SPARK_CONF_DIR&#125;/spark-env.sh.template $&#123;SPARK_CONF_DIR&#125;/spark-env.sh<br>echo &quot;export JAVA_HOME=$&#123;JAVA_HOME&#125;&quot; &gt;&gt; $&#123;SPARK_CONF_DIR&#125;/spark-env.sh<br>echo &quot;export SCALA_HOME=$&#123;SCALA_HOME&#125;&quot; &gt;&gt; $&#123;SPARK_CONF_DIR&#125;/spark-env.sh<br>echo &quot;export HADOOP_HOME=$&#123;HADOOP_HOME&#125;&quot; &gt;&gt; $&#123;SPARK_CONF_DIR&#125;/spark-env.sh<br>echo &quot;export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop&quot; &gt;&gt; $&#123;SPARK_CONF_DIR&#125;/spark-env.sh<br>echo &quot;export SPARK_HOME=$&#123;SPARK_HOME&#125;&quot; &gt;&gt; $&#123;SPARK_CONF_DIR&#125;/spark-env.sh<br></code></pre></td></tr></table></figure><h3 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &lt;&lt; EOF &gt;&gt; $&#123;SPARK_CONF_DIR&#125;/hive-site.xml<br>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;<br>&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;<br>&lt;configuration&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;hive.metastore.uris&lt;/name&gt;<br>    &lt;value&gt;thrift://localhost:9083&lt;/value&gt;<br>    &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt;<br>  &lt;/property&gt;<br>  &lt;property&gt;<br>    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;<br>    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;<br>    &lt;description&gt;location of default database for the warehouse&lt;/description&gt;<br>  &lt;/property&gt;<br>&lt;/configuration&gt;<br>EOF<br></code></pre></td></tr></table></figure><h2 id="运行-Example"><a href="#运行-Example" class="headerlink" title="运行 Example"></a>运行 Example</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">spark-submit \<br>  --class org.apache.spark.examples.SparkPi \<br>  --master yarn \<br>  --deploy-mode client \<br><span class="hljs-meta prompt_">  $</span><span class="language-bash">&#123;SPARK_HOME&#125;/examples/jars/spark-examples_2.12-3.5.1.jar \</span><br><span class="language-bash">  1000</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sqoop 安装</title>
    <link href="/2025/01/01/Manual-Install-Sqoop/"/>
    <url>/2025/01/01/Manual-Install-Sqoop/</url>
    
    <content type="html"><![CDATA[<h1 id="安装-Hadoop-和-Hive"><a href="#安装-Hadoop-和-Hive" class="headerlink" title="安装 Hadoop 和 Hive"></a>安装 Hadoop 和 Hive</h1><h2 id="安装-Hadoop"><a href="#安装-Hadoop" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h2><p>参考 <a href="Manual-Install-Hadoop.md">Manual-Install-Hadoop.md</a></p><h2 id="安装-Hive"><a href="#安装-Hive" class="headerlink" title="安装 Hive"></a>安装 Hive</h2><p>参考 <a href="Manual-Install-Hive.md">Manual-Install-Hive.md</a></p><h1 id="安装-Sqoop"><a href="#安装-Sqoop" class="headerlink" title="安装 Sqoop"></a>安装 Sqoop</h1><h2 id="下载安装包"><a href="#下载安装包" class="headerlink" title="下载安装包"></a>下载安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget https://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -P /root/install_packages/<br></code></pre></td></tr></table></figure><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf /root/install_packages/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /root/software/<br><br></code></pre></td></tr></table></figure><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;&quot; &gt;&gt; /etc/profile<br>echo &quot;# sqoop&quot; &gt;&gt; /etc/profile<br>echo &quot;export SQOOP_HOME=/root/software/sqoop-1.4.7.bin__hadoop-2.6.0&quot; &gt;&gt; /etc/profile<br>echo &quot;export PATH=\$PATH:\$SQOOP_HOME/bin&quot; &gt;&gt; /etc/profile<br></code></pre></td></tr></table></figure><h2 id="加载环境变量"><a href="#加载环境变量" class="headerlink" title="加载环境变量"></a>加载环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><h2 id="下载-JDBC-驱动"><a href="#下载-JDBC-驱动" class="headerlink" title="下载 JDBC 驱动"></a>下载 JDBC 驱动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">MySQL</span><br>wget -P $&#123;SQOOP_HOME&#125;/lib https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar<br>wget -P $&#123;SQOOP_HOME&#125;/lib https://jdbc.postgresql.org/download/postgresql-42.5.4.jar<br></code></pre></td></tr></table></figure><h2 id="sqoop-env-sh"><a href="#sqoop-env-sh" class="headerlink" title="sqoop-env.sh"></a>sqoop-env.sh</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">SQOOP_CONF_DIR=$&#123;SQOOP_HOME&#125;/conf<br>cp $&#123;SQOOP_CONF_DIR&#125;/sqoop-env-template.sh $&#123;SQOOP_CONF_DIR&#125;/sqoop-env.sh<br>echo &quot;export HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;&quot; &gt;&gt; $&#123;SQOOP_CONF_DIR&#125;/sqoop-env.sh<br>echo &quot;export HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&quot; &gt;&gt; $&#123;SQOOP_CONF_DIR&#125;/sqoop-env.sh<br>echo &quot;export HIVE_HOME=$&#123;HIVE_HOME&#125;&quot; &gt;&gt; $&#123;SQOOP_CONF_DIR&#125;/sqoop-env.sh<br>echo &quot;export HIVE_CONF_DIR=$&#123;HIVE_HOME&#125;/conf&quot; &gt;&gt; $&#123;SQOOP_CONF_DIR&#125;/sqoop-env.sh<br>echo &quot;export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*&quot; &gt;&gt; $&#123;SQOOP_CONF_DIR&#125;/sqoop-env.sh<br></code></pre></td></tr></table></figure><h2 id="sqoop-version"><a href="#sqoop-version" class="headerlink" title="sqoop version"></a>sqoop version</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sqoop version<br></code></pre></td></tr></table></figure><h2 id="Other-Example-cmd"><a href="#Other-Example-cmd" class="headerlink" title="Other Example cmd"></a>Other Example cmd</h2><h3 id="list-databases"><a href="#list-databases" class="headerlink" title="list databases"></a>list databases</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sqoop list-databases --connect jdbc:mysql://localhost:3306/hive --username root --password 123456<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 爬取数据</span></span><br>mkdir /root/workspace <br><br>apt update &amp;&amp; apt install -y curl <br>curl -XGET &#x27;https://www.gushiwen.cn/gushi/tangshi.aspx&#x27; &gt; /root/workspace/tangpoems.html<br>cat /root/workspace/tangpoems.html | grep &#x27;shiwenv&#x27; | awk -F &#x27;&lt;|&gt;&#x27; &#x27;&#123;print $5&quot;,&quot;$7&#125;&#x27; &gt; /root/workspace/tangpoems.csv<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># set utf-8 以避免中文乱码</span></span><br>cat &lt;&lt; EOF &gt;&gt; /etc/mysql/my.cnf<br>[mysqld]<br>character-set-server=utf8 <br>local-infile <br>[client]<br>default-character-set=utf8 <br>local-infile <br>[mysql]<br>default-character-set=utf8<br>local-infile <br>EOF<br>service mysql restart<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 导入 数据到MySQL</span></span><br>mysql -uroot -p123456 --local-infile=1 &lt;&lt; EOF<br>SET GLOBAL local_infile=1;<br>create database if not exists data;<br>create table if not exists data.tang_poems_300(poem varchar(255), author varchar(255)) default charset=utf8;<br>LOAD DATA LOCAL INFILE &#x27;/root/workspace/tangpoems.csv&#x27; INTO TABLE data.tang_poems_300 <br>FIELDS TERMINATED BY &#x27;,&#x27; ENCLOSED BY &#x27;&quot;&#x27; LINES TERMINATED BY &#x27;\n&#x27;;<br>EOF<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># import from mysql</span></span> <br>hdfs dfs -rm -r /user/hive/warehouse/ODS_TANG_POEMS_300<br>hive -e &quot;drop table ods_tang_poem_300&quot;<br>sqoop import \<br>  --connect jdbc:mysql://localhost/data \<br>  --username root \<br>  --password 123456 \<br>  --table tang_poems_300 \<br>  -m 1 \<br>  --hive-import \<br>  --hive-overwrite \<br>  --create-hive-table \<br>  --hive-table ods_tang_poem_300 \<br>  --target-dir /user/hive/warehouse/ODS_TANG_POEMS_300 \<br>  -m 1 <br><span class="hljs-meta prompt_">  </span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># Mock ETL</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># DWD</span></span><br>hive -e &quot;<br>DROP TABLE IF EXISTS dwd_tang_poem_300; <br>CREATE TABLE dwd_tang_poem_300 AS <br>  SELECT poem, REGEXP_REPLACE(author,&#x27;\\\\(|\\\\)&#x27;, &#x27;&#x27;) AS author<br>    FROM ods_tang_poem_300<br>;<br>&quot;<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># DWS</span></span><br><br>hive -e &quot;<br>DROP TABLE IF EXISTS dws_poem_num_by_author; <br>CREATE TABLE dws_poem_num_by_author AS <br>  SELECT author, count(1) AS poem_num <br>    FROM dwd_tang_poem_300<br>GROUP BY author <br>ORDER by poem_num DESC<br>;<br>&quot;<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># export to mysql</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">create ads db and table</span><br>mysql -uroot -p123456 &lt;&lt; EOF<br>create database ads;<br>create table ads.ads_poem_num_by_author(author varchar(255), poem_num int);<br>EOF<br><br>sqoop export \<br>--connect jdbc:mysql://localhost/ads \<br>--driver com.mysql.jdbc.Driver \<br>--username root \<br>--password 123456 \<br>--table ads_poem_num_by_author \<br>--hcatalog-table dws_poem_num_by_author \<br>--input-fields-terminated-by &#x27;,&#x27; \<br>--num-mappers 1 <br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># 提前在PG中创建表</span></span><br>create table ads_poem_num_by_author(author varchar(255), poem_num bigint);<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># export to postgres</span></span><br>sqoop export \<br>--connect jdbc:postgresql://postgres:5432/postgres \<br>--driver org.postgresql.Driver \<br>--username postgres \<br>--password 123456 \<br>--table ads_poem_num_by_author \<br>--hcatalog-table dws_poem_num_by_author \<br>--input-fields-terminated-by &#x27;,&#x27; \<br>--num-mappers 1 <br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Sqoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>Sqoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark Dataframe</title>
    <link href="/2025/01/01/apache-spark/spark-dataframe/"/>
    <url>/2025/01/01/apache-spark/spark-dataframe/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark-DataFrame"><a href="#Spark-DataFrame" class="headerlink" title="Spark DataFrame"></a>Spark DataFrame</h1><h2 id="DataFrame-简介"><a href="#DataFrame-简介" class="headerlink" title="DataFrame 简介"></a>DataFrame 简介</h2><p>Spark DataFrame 是 Spark 基于 <a href="spark-rdd.md">RDD</a> 实现的面向结构化数据的高阶抽象, 它将分布式的数据集以行列的方式组织起来，并提供了很多关系型操作的 API。<br><em>Spark 在 1.6 版本还提供了 DataSet 的抽象，对 DataFrame 进行了扩展，支持面向对象的处理。<br>而本质上 DataFrame 就是对象类型为 Row 的 DataSet, 因此在 Spark 2.0 之后将 DataFrame API 和 DataSet API 进行了统一， 即 DataFrame &#x3D; DataSet[Row]。</em>  </p><h2 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h2><p>DataFrame 的创建方式有很多种，通过 RDD 创建、通过读取外部系统创建等<br>本篇文章的后续实践可在 spark-shell 中完成, 其中会默认实例化一个 SparkContext 实例 <code>sc</code> 和 SparkSession 实例 <code>spark</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">spark-shell<br></code></pre></td></tr></table></figure><h3 id="通过-Java-集合创建"><a href="#通过-Java-集合创建" class="headerlink" title="通过 Java 集合创建"></a>通过 Java 集合创建</h3><p>通过内存中的集合创建 DataFrame</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val scalaList = List(1 -&gt; &quot;A&quot;, 2 -&gt; &quot;B&quot;, 3 -&gt; &quot;C&quot;) <br>scalaList: List[(Int, String)] = List((1,A), (2,B), (3,C))<br><br>scala&gt; val df = spark.createDataFrame(scalaList)<br>df: org.apache.spark.sql.DataFrame = [_1: int, _2: string]<br><br>scala&gt; val withColNameDF = df.toDF(&quot;id&quot;, &quot;name&quot;)<br>withColNameDF: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; withColNameDF.show</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">| id|name|</span><br><span class="hljs-section">+---+----+</span><br>|  1|   A|<br>|  2|   B|<br><span class="hljs-section">|  3|   C|</span><br><span class="hljs-section">+---+----+</span><br></code></pre></td></tr></table></figure><h3 id="通过-RDD-创建"><a href="#通过-RDD-创建" class="headerlink" title="通过 RDD 创建"></a>通过 RDD 创建</h3><p>前面我们说过 DataFrame 是架构化的高阶抽象，因此为 Row 类型的 RDD 指定结构(schema)即可得到 DataFrame  </p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">scala&gt; <span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-keyword">sql</span>.<span class="hljs-keyword">Row</span><br>scala&gt; val rdd = sc.parallelize(List(<span class="hljs-keyword">Row</span>(<span class="hljs-number">1</span>, &quot;A&quot;), <span class="hljs-keyword">Row</span>(<span class="hljs-number">2</span>, &quot;B&quot;), <span class="hljs-keyword">Row</span>(<span class="hljs-number">3</span>, &quot;C&quot;)))<br><br>scala&gt; <span class="hljs-keyword">import</span> org.apache.spark.<span class="hljs-keyword">sql</span>.<span class="hljs-keyword">types</span>._<br>scala&gt; val schema = <span class="hljs-built_in">new</span> StructType() .<span class="hljs-keyword">add</span>(StructField(&quot;id&quot;, IntegerType)) .<span class="hljs-keyword">add</span>(StructField(&quot;name&quot;, StringType))<br><span class="hljs-keyword">schema</span>: org.apache.spark.<span class="hljs-keyword">sql</span>.<span class="hljs-keyword">types</span>.StructType = StructType(StructField(id,LongType,<span class="hljs-keyword">true</span>),StructField(<span class="hljs-type">name</span>,StringType,<span class="hljs-keyword">true</span>))<br><br>scala&gt; val df = spark.createDataFrame(rdd, <span class="hljs-keyword">schema</span>)<br>df: org.apache.spark.<span class="hljs-keyword">sql</span>.DataFrame = [id: , <span class="hljs-type">name</span>: string]<br><br>scala&gt; df.<span class="hljs-keyword">show</span>()<br>+<span class="hljs-comment">---+----+</span><br>| id|<span class="hljs-type">name</span>|<br>+<span class="hljs-comment">---+----+</span><br>|  <span class="hljs-number">1</span>|   A|<br>|  <span class="hljs-number">2</span>|   B|<br>|  <span class="hljs-number">3</span>|   C|<br>+<span class="hljs-comment">---+----+</span><br></code></pre></td></tr></table></figure><h3 id="读取-Hive-表创建"><a href="#读取-Hive-表创建" class="headerlink" title="读取 Hive 表创建"></a>读取 Hive 表创建</h3><p>Spark 通过与 Hive 集成，即可轻松访问 Hive 中的表<br>假设 Hive 中有张 test 表, 那么 Spark 可以通过 <code>table</code> 或者 <code>sql</code> 方法来将表中的数据读取为 DataFrame  </p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">set</span> hive.cli.print.<span class="hljs-keyword">header</span>=<span class="hljs-keyword">true</span>;<br>hive&gt; <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> test_table;<br>OK<br>test_table.idtest_table.name<br><span class="hljs-number">1</span>A<br><span class="hljs-number">2</span>B<br><span class="hljs-number">3</span>C<br></code></pre></td></tr></table></figure><h4 id="table"><a href="#table" class="headerlink" title="table"></a>table</h4><p>指定表名，将整张表返回为 DataFrame </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.table(&quot;default.test_table&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.show</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">| id|name|</span><br><span class="hljs-section">+---+----+</span><br>|  1|   A|<br>|  2|   B|<br><span class="hljs-section">|  3|   C|</span><br><span class="hljs-section">+---+----+</span><br></code></pre></td></tr></table></figure><h4 id="sql"><a href="#sql" class="headerlink" title="sql"></a>sql</h4><p>执行 SQL 语句，将结果集返回为 DataFrame  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT * FROM default.test_table&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.show</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">| id|name|</span><br><span class="hljs-section">+---+----+</span><br>|  1|   A|<br>|  2|   B|<br><span class="hljs-section">|  3|   C|</span><br><span class="hljs-section">+---+----+</span><br></code></pre></td></tr></table></figure><h3 id="读取外部系统"><a href="#读取外部系统" class="headerlink" title="读取外部系统"></a>读取外部系统</h3><h4 id="通过-jdbc-读取数据库"><a href="#通过-jdbc-读取数据库" class="headerlink" title="通过 jdbc 读取数据库"></a>通过 jdbc 读取数据库</h4><p>通过 jdbc 协议将数据从外部数据库系统如 MySQL&#x2F;Postgres 等读取为 DataFrame<br>本例子中先将 DataFrame 通过 <a href="#jdbc">jdbc</a> 写入 MySQL 后，再从 MySQL 中读取数据，需要使用 MySQL 驱动来连接本地的 MySQL 数据库，因此需要将相应的驱动包加入 classpath，比如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">spark-shell --packages &quot;mysql:mysql-connector-java:8.0.28&quot; <br></code></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val connectionProperties = new java.util.Properties()<br>scala&gt; connectionProperties.put(&quot;user&quot;, &quot;root&quot;)<br>scala&gt; connectionProperties.put(&quot;password&quot;, &quot;123456&quot;)<br>scala&gt; connectionProperties.put(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)<br><br>scala&gt; df.write.jdbc(<br><span class="hljs-code">  url=&quot;jdbc:mysql://localhost:3306/test?createDatabaseIfNotExist=true&quot;, </span><br><span class="hljs-code">  table=&quot;test_table__jdbc&quot;,</span><br><span class="hljs-code">  connectionProperties=connectionProperties</span><br>)<br><br>scala&gt; spark.read.jdbc(<br><span class="hljs-code">  url=&quot;jdbc:mysql://localhost:3306/test?createDatabaseIfNotExist=true&quot;, </span><br><span class="hljs-code">  table=&quot;test_table__jdbc&quot;,</span><br><span class="hljs-code">  properties=connectionProperties</span><br><span class="hljs-section">).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  2|Name2|<br>|  1|Name1|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="读取外部文件系统"><a href="#读取外部文件系统" class="headerlink" title="读取外部文件系统"></a>读取外部文件系统</h4><p>从外部文件系统（如 HDFS，S3等）将数据读取为 DataFrame<br>本例子中先将 DataFrame 通过 <a href="#csv">csv</a> 写入 HDFS 后，再从 HDFS 中读取数据  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; df.write.mode(&quot;overwrite&quot;).option(&quot;header&quot;, &quot;true&quot;).csv(&quot;hdfs:///test_write/test_csv&quot;)<br><br><span class="hljs-section">scala&gt; spark.read.format(&quot;csv&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;hdfs:///test_write/test_csv&quot;).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h2 id="Transformation-算子"><a href="#Transformation-算子" class="headerlink" title="Transformation 算子"></a>Transformation 算子</h2><h3 id="基础转换"><a href="#基础转换" class="headerlink" title="基础转换"></a>基础转换</h3><h4 id="select"><a href="#select" class="headerlink" title="select"></a>select</h4><p>选择一组列，或基于列的函数</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;select 1 as id, <span class="hljs-emphasis">&#x27;A&#x27;</span> as name&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.select(df(&quot;id&quot;), df(&quot;name&quot;), org.apache.spark.sql.functions.lower(df(&quot;name&quot;)) as &quot;lower_name&quot; ).show</span><br><span class="hljs-section">+---+----+----------+</span><br><span class="hljs-section">| id|name|lower_name|</span><br><span class="hljs-section">+---+----+----------+</span><br><span class="hljs-section">|  1|   A|         a|</span><br><span class="hljs-section">+---+----+----------+</span><br></code></pre></td></tr></table></figure><h4 id="selectExpr"><a href="#selectExpr" class="headerlink" title="selectExpr"></a>selectExpr</h4><p>选择一组列，或基于列的 SQL 表达式</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.selectExpr(&quot;id&quot;, &quot;name&quot;, &quot;LOWER(name) as lower_name&quot; ).show</span><br><span class="hljs-section">+---+----+----------+</span><br><span class="hljs-section">| id|name|lower_name|</span><br><span class="hljs-section">+---+----+----------+</span><br><span class="hljs-section">|  1|   A|         a|</span><br><span class="hljs-section">+---+----+----------+</span><br></code></pre></td></tr></table></figure><h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p>根据指定的条件函数过滤出符合条件的数据, 过滤条件也可以是 SQL 表达式</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name UNION SELECT 2 AS id, <span class="hljs-emphasis">&#x27;B&#x27;</span> AS name &quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.filter(df(&quot;id&quot;) &gt; 1).show()</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">| id|name|</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">|  2|   B|</span><br><span class="hljs-section">+---+----+</span><br><br><span class="hljs-section">scala&gt; df.filter(&quot;id &gt; 1&quot;).show()</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">| id|name|</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">|  2|   B|</span><br><span class="hljs-section">+---+----+</span><br></code></pre></td></tr></table></figure><h4 id="where"><a href="#where" class="headerlink" title="where"></a>where</h4><p>与 <a href="#filter">filter</a> 行为一致，提供与 SQL 语义一致的同名算子</p><h4 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h4><p>根据指定的列对数据进行排序  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name UNION SELECT 2 AS id, <span class="hljs-emphasis">&#x27;B&#x27;</span> AS name &quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.sort(df(&quot;id&quot;).desc).show()</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">| id|name|</span><br><span class="hljs-section">+---+----+</span><br>|  2|   B|<br><span class="hljs-section">|  1|   A|</span><br><span class="hljs-section">+---+----+</span><br></code></pre></td></tr></table></figure><h4 id="orderBy"><a href="#orderBy" class="headerlink" title="orderBy"></a>orderBy</h4><p>与 <a href="#sort">sort</a> 行为一致，提供与 SQL 语义一致的同名算子  </p><h4 id="na"><a href="#na" class="headerlink" title="na"></a>na</h4><h4 id="stat"><a href="#stat" class="headerlink" title="stat"></a>stat</h4><h4 id="hint"><a href="#hint" class="headerlink" title="hint"></a>hint</h4><h4 id="as"><a href="#as" class="headerlink" title="as"></a>as</h4><p>为 DataFrame 定义别名  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name UNION SELECT 2 AS id, <span class="hljs-emphasis">&#x27;B&#x27;</span> AS name &quot;).as(&quot;aliasOfDF&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.filter(&quot;aliasOfDF.id &gt; 1&quot;).show()</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">| id|name|</span><br><span class="hljs-section">+---+----+</span><br><span class="hljs-section">|  2|   B|</span><br><span class="hljs-section">+---+----+</span><br></code></pre></td></tr></table></figure><h4 id="alias"><a href="#alias" class="headerlink" title="alias"></a>alias</h4><p>与 <a href="#as-">as</a> 语义一致  </p><h4 id="to"><a href="#to" class="headerlink" title="to"></a>to</h4><p>将 DataFrame 转换为具有给定 schema 的新 DataFrame</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name UNION SELECT 2 AS id, <span class="hljs-emphasis">&#x27;B&#x27;</span> AS name &quot;).as(&quot;aliasOfDF&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br>scala&gt; val schema = new StructType().add(StructField(&quot;name&quot;, StringType)).add(StructField(&quot;id&quot;, IntegerType)) <br>schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true),StructField(id,IntegerType,true))<br><br><span class="hljs-section">scala&gt; df.to(schema).show</span><br><span class="hljs-section">+----+---+</span><br><span class="hljs-section">|name| id|</span><br><span class="hljs-section">+----+---+</span><br>|   A|  1|<br><span class="hljs-section">|   B|  2|</span><br><span class="hljs-section">+----+---+</span><br><br></code></pre></td></tr></table></figure><h4 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h4><p>将 DataFrame 转换为具有指定的列名的新的 DataFrame  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name UNION SELECT 2 AS id, <span class="hljs-emphasis">&#x27;B&#x27;</span> AS name &quot;).as(&quot;aliasOfDF&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.toDF(&quot;name&quot;, &quot;id&quot;).show</span><br><span class="hljs-section">+----+---+</span><br><span class="hljs-section">|name| id|</span><br><span class="hljs-section">+----+---+</span><br>|   1|  A|<br><span class="hljs-section">|   2|  B|</span><br><span class="hljs-section">+----+---+</span><br></code></pre></td></tr></table></figure><h4 id="unpivot"><a href="#unpivot" class="headerlink" title="unpivot"></a>unpivot</h4><p>列转行, 将 DataFrame 的每条记录的每一列转为单独的行  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String, age: Int)<br>scala&gt; val persons = List(Person(1, &quot;Tom&quot;, 30), Person(2, &quot;Jerry&quot;, 28))<br>persons: List[Person] = List(Person(1,Tom,30), Person(2,Jerry,28))<br><br>scala&gt; val df = spark.createDataFrame(persons)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]<br><br><span class="hljs-section">scala&gt; df.show</span><br><span class="hljs-section">+---+-----+---+</span><br><span class="hljs-section">| id| name|age|</span><br><span class="hljs-section">+---+-----+---+</span><br>|  1|  Tom| 30|<br><span class="hljs-section">|  2|Jerry| 28|</span><br><span class="hljs-section">+---+-----+---+</span><br><br>scala&gt; val unpivotedDF = df.unpivot(ids=Array(df(&quot;id&quot;)), values=Array(df(&quot;name&quot;), df(&quot;age&quot;).cast(&quot;string&quot;)), variableColumnName=&quot;k&quot;, valueColumnName=&quot;v&quot;)<br>unpivotedDF: org.apache.spark.sql.DataFrame = [id: int, k: string ... 1 more field]<br><br><span class="hljs-section">scala&gt; unpivotedDF.show</span><br><span class="hljs-section">+---+----+-----+</span><br><span class="hljs-section">| id|   k|    v|</span><br><span class="hljs-section">+---+----+-----+</span><br>|  1|name|  Tom|<br>|  1| age|   30|<br>|  2|name|Jerry|<br><span class="hljs-section">|  2| age|   28|</span><br><span class="hljs-section">+---+----+-----+</span><br></code></pre></td></tr></table></figure><h4 id="melt"><a href="#melt" class="headerlink" title="melt"></a>melt</h4><p>与 <a href="#unpivot-">unpivot</a> 语义一致  </p><h4 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h4><p>根据现有的列或基于现有列的函数，添加或替换一个指定名称的列  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.withColumn(&quot;id_copy&quot;, df(&quot;id&quot;)).withColumn(&quot;lower_name&quot;, org.apache.spark.sql.functions.lower(df(&quot;name&quot;))).show</span><br><span class="hljs-section">+---+----+-------+----------+</span><br><span class="hljs-section">| id|name|id_copy|lower_name|</span><br><span class="hljs-section">+---+----+-------+----------+</span><br><span class="hljs-section">|  1|   A|      1|         a|</span><br><span class="hljs-section">+---+----+-------+----------+</span><br></code></pre></td></tr></table></figure><h4 id="withColumnRenamed"><a href="#withColumnRenamed" class="headerlink" title="withColumnRenamed"></a>withColumnRenamed</h4><p>重命名现有的列  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.withColumnRenamed(&quot;id&quot;, &quot;new_id&quot;).show</span><br><span class="hljs-section">+------+----+</span><br><span class="hljs-section">|new_id|name|</span><br><span class="hljs-section">+------+----+</span><br><span class="hljs-section">|     1|   A|</span><br><span class="hljs-section">+------+----+</span><br></code></pre></td></tr></table></figure><h4 id="withColumns"><a href="#withColumns" class="headerlink" title="withColumns"></a>withColumns</h4><p>对多个列执行 <a href="#withcolumn">withColumn</a> 操作, 入参是 <code>新列名</code> -&gt; <code>现有列或基于现有列的函数</code> 的 Map</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.withColumns(Map(&quot;id_copy&quot; -&gt; df(&quot;id&quot;), &quot;lower_name&quot; -&gt; org.apache.spark.sql.functions.lower(df(&quot;name&quot;)))).show</span><br><span class="hljs-section">+---+----+-------+----------+</span><br><span class="hljs-section">| id|name|id_copy|lower_name|</span><br><span class="hljs-section">+---+----+-------+----------+</span><br><span class="hljs-section">|  1|   A|      1|         a|</span><br><span class="hljs-section">+---+----+-------+----------+</span><br></code></pre></td></tr></table></figure><h4 id="withColumnsRenamed"><a href="#withColumnsRenamed" class="headerlink" title="withColumnsRenamed"></a>withColumnsRenamed</h4><p>重命名多个现有的列, 入参是 <code>现有列的名称</code> -&gt; <code>新列名</code> 的 Map</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.withColumnsRenamed(Map(&quot;id&quot;-&gt; &quot;new_id&quot;, &quot;name&quot; -&gt; &quot;new_name&quot;)).show</span><br><span class="hljs-section">+------+--------+</span><br><span class="hljs-section">|new_id|new_name|</span><br><span class="hljs-section">+------+--------+</span><br><span class="hljs-section">|     1|       A|</span><br><span class="hljs-section">+------+--------+</span><br></code></pre></td></tr></table></figure><h4 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h4><p>移除指定的列  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.drop(&quot;id&quot;).show()</span><br><span class="hljs-section">+----+</span><br><span class="hljs-section">|name|</span><br><span class="hljs-section">+----+</span><br><span class="hljs-section">|   A|</span><br><span class="hljs-section">+----+</span><br></code></pre></td></tr></table></figure><h4 id="transform"><a href="#transform" class="headerlink" title="transform"></a>transform</h4><p>对当前的 DataFrame 作转换，根据传入的 DataFrame &#x3D;&gt; DataFrame 函数，返回新的 DataFrame </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.transform(_df =&gt; df.drop(&quot;id&quot;)).show</span><br><span class="hljs-section">+----+</span><br><span class="hljs-section">|name|</span><br><span class="hljs-section">+----+</span><br><span class="hljs-section">|   A|</span><br><span class="hljs-section">+----+</span><br></code></pre></td></tr></table></figure><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>对 DataFrame 的每一行 row 作转换, 入参是 row &#x3D;&gt; row 的映射函数  </p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">scala&gt; val df = spark.<span class="hljs-keyword">sql</span>(&quot;SELECT 1 AS id, &#x27;A&#x27; AS name&quot;)<br>df: org.apache.spark.<span class="hljs-keyword">sql</span>.DataFrame = [id: <span class="hljs-type">int</span>, <span class="hljs-type">name</span>: string]<br><br>scala&gt; df.map(<span class="hljs-keyword">row</span> =&gt; &quot;prefix_&quot; + <span class="hljs-keyword">row</span>.getAs[String](&quot;name&quot;)).<span class="hljs-keyword">show</span><br>+<span class="hljs-comment">--------+</span><br>|   <span class="hljs-keyword">value</span>|<br>+<span class="hljs-comment">--------+</span><br>|prefix_A|<br>+<span class="hljs-comment">--------+</span><br></code></pre></td></tr></table></figure><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p>对 DataFrame 的 Array 列作转换, 并将每个元素展开成单独的行, 入参是 arrayItem &#x3D;&gt; arrayItem 的映射函数</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT ARRAY(1, 2, 3) as array_column&quot;)<br><br>scala&gt; df.flatMap(row =&gt; row.getAs[Seq[Int]](&quot;array_column&quot;).map(_ * 2)).show<br><span class="hljs-code">+-----+</span><br><span class="hljs-section">|value|</span><br><span class="hljs-section">+-----+</span><br>|    2|<br>|    4|<br><span class="hljs-section">|    6|</span><br><span class="hljs-section">+-----+</span><br></code></pre></td></tr></table></figure><h4 id="toJSON"><a href="#toJSON" class="headerlink" title="toJSON"></a>toJSON</h4><p>对 DataFrame 的每一行 row 转换为 JSON 字符串  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; val df = spark.sql(&quot;SELECT 1 AS id, <span class="hljs-emphasis">&#x27;A&#x27;</span> AS name UNION SELECT 2, <span class="hljs-emphasis">&#x27;B&#x27;</span>&quot;)<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.toJSON.show</span><br><span class="hljs-section">+-------------------+</span><br><span class="hljs-section">|              value|</span><br><span class="hljs-section">+-------------------+</span><br>|&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;A&quot;&#125;|<br><span class="hljs-section">|&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;B&quot;&#125;|</span><br><span class="hljs-section">+-------------------+</span><br></code></pre></td></tr></table></figure><h3 id="分区转换"><a href="#分区转换" class="headerlink" title="分区转换"></a>分区转换</h3><h4 id="sortWithinPartitions"><a href="#sortWithinPartitions" class="headerlink" title="sortWithinPartitions"></a>sortWithinPartitions</h4><p>对每个 Partition 中的数据进行排序，与 SQL 中的 <code>sort by</code> 语义一致 </p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val persons = (1 to 6).toList.map(x =&gt; Person(x, <span class="hljs-string">&quot;Name&quot;</span> + x))<br>persons: List[Person] = List(Person(1,Name1), Person(2,Name2), Person(3,Name3), Person(4,Name4), Person(5,Name5), Person(6,Name6))<br><br>scala&gt; val df = sc.parallelize(persons, 2).toDF<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]<br><br>scala&gt; val sortedDF = df.sortWithinPartitions(df(<span class="hljs-string">&quot;id&quot;</span>).desc)<br>sortedDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, name: string]<br><br>scala&gt; sortedDF.withColumn(<span class="hljs-string">&quot;partition_id&quot;</span>, org.apache.spark.sql.functions.spark_partition_id()).show<br>+---+-----+------------+<br>|<span class="hljs-string"> id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string">partition_id</span>|<br>+---+-----+------------+<br>|<span class="hljs-string">  3</span>|<span class="hljs-string">Name3</span>|<span class="hljs-string">           0</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">           0</span>|<br>|<span class="hljs-string">  1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">           0</span>|<br>|<span class="hljs-string">  6</span>|<span class="hljs-string">Name6</span>|<span class="hljs-string">           1</span>|<br>|<span class="hljs-string">  5</span>|<span class="hljs-string">Name5</span>|<span class="hljs-string">           1</span>|<br>|<span class="hljs-string">  4</span>|<span class="hljs-string">Name4</span>|<span class="hljs-string">           1</span>|<br>+---+-----+------------+<br></code></pre></td></tr></table></figure><h4 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h4><p>对 DataFrame 的每一个分区做转换操作，每个分区中的记录被封装成一个迭代器，因此这个转换函数应是 iterator &#x3D;&gt; iterator 的映射</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String)<br><br>scala&gt; val df =  spark.createDataFrame((1 to 6).toList.map(x =&gt; Person(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]<br><br>scala&gt; val transformed = df.mapPartitions&#123; iter =&gt; val salt = <span class="hljs-string">&quot;abcd_&quot;</span>; iter.map( row =&gt; salt + row.getAs(<span class="hljs-string">&quot;name&quot;</span>)) &#125;<br>transformed: org.apache.spark.sql.Dataset[String] = [value: string]<br><br>scala&gt; transformed.withColumn(<span class="hljs-string">&quot;partition_id&quot;</span>, org.apache.spark.sql.functions.spark_partition_id()).show<br>+----------+------------+<br>|<span class="hljs-string">     value</span>|<span class="hljs-string">partition_id</span>|<br>+----------+------------+<br>|<span class="hljs-string">abcd_Name1</span>|<span class="hljs-string">           0</span>|<br>|<span class="hljs-string">abcd_Name2</span>|<span class="hljs-string">           1</span>|<br>|<span class="hljs-string">abcd_Name3</span>|<span class="hljs-string">           2</span>|<br>|<span class="hljs-string">abcd_Name4</span>|<span class="hljs-string">           3</span>|<br>|<span class="hljs-string">abcd_Name5</span>|<span class="hljs-string">           4</span>|<br>|<span class="hljs-string">abcd_Name6</span>|<span class="hljs-string">           5</span>|<br>+----------+------------+<br></code></pre></td></tr></table></figure><h4 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h4><p>调整 DataFrame 的分区到目标数量，与 <a href="spark-rdd.md#repartition">RDD - repartition</a> 的行为一致  </p><h4 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h4><p>减少 DataFrame 的分区到目标数量，与 <a href="spark-rdd.md#coalesce">RDD - coalesce</a> 的行为一致</p><h4 id="repartitionByRange"><a href="#repartitionByRange" class="headerlink" title="repartitionByRange"></a>repartitionByRange</h4><p>调整 DataFrame 的分区到目标数量，对数据进行随机重新分布，会产生 Shuffle，重分布的过程将按指定列进行区间采样并进行区间排序（默认升序，空值优先），<br>因为采样可能返回不同的结果，故多次 repartitionByRange 的最终结果有可能不一致</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String)<br><br>scala&gt; val df =  spark.createDataFrame((1 to 6).toList.map(x =&gt; Person(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]<br><br>scala&gt; df.repartitionByRange(3, $<span class="hljs-string">&quot;id&quot;</span>).withColumn(<span class="hljs-string">&quot;partition_id&quot;</span>, org.apache.spark.sql.functions.spark_partition_id()).show<br>+---+-----+------------+<br>|<span class="hljs-string"> id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string">partition_id</span>|<br>+---+-----+------------+<br>|<span class="hljs-string">  1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">           0</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">           0</span>|<br>|<span class="hljs-string">  3</span>|<span class="hljs-string">Name3</span>|<span class="hljs-string">           1</span>|<br>|<span class="hljs-string">  4</span>|<span class="hljs-string">Name4</span>|<span class="hljs-string">           1</span>|<br>|<span class="hljs-string">  5</span>|<span class="hljs-string">Name5</span>|<span class="hljs-string">           2</span>|<br>|<span class="hljs-string">  6</span>|<span class="hljs-string">Name6</span>|<span class="hljs-string">           2</span>|<br>+---+-----+------------+<br>scala&gt; df.repartitionByRange(3, $<span class="hljs-string">&quot;id&quot;</span>.desc).withColumn(<span class="hljs-string">&quot;partition_id&quot;</span>, org.apache.spark.sql.functions.spark_partition_id()).show<br>+---+-----+------------+<br>|<span class="hljs-string"> id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string">partition_id</span>|<br>+---+-----+------------+<br>|<span class="hljs-string">  5</span>|<span class="hljs-string">Name5</span>|<span class="hljs-string">           0</span>|<br>|<span class="hljs-string">  6</span>|<span class="hljs-string">Name6</span>|<span class="hljs-string">           0</span>|<br>|<span class="hljs-string">  3</span>|<span class="hljs-string">Name3</span>|<span class="hljs-string">           1</span>|<br>|<span class="hljs-string">  4</span>|<span class="hljs-string">Name4</span>|<span class="hljs-string">           1</span>|<br>|<span class="hljs-string">  1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">           2</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">           2</span>|<br>+---+-----+------------+<br></code></pre></td></tr></table></figure><h3 id="集合运算"><a href="#集合运算" class="headerlink" title="集合运算"></a>集合运算</h3><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>将当前 DataFrame 与另一个 DataFrame 关联，需要自定关联类型，默认为 Inner</p><h5 id="Inner-Join"><a href="#Inner-Join" class="headerlink" title="Inner Join"></a>Inner Join</h5><p>将当前 DataFrame 与另外的 DataFrame 进行内关联, 结果集中仅包含左右 DataFrame 中的能匹配上的记录, 相同关联键存在重复数据时，将返回其笛卡尔积<br>joinType 为 <code>inner</code> 即表示 Inner Join</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br><br>scala&gt; val df1 = spark.createDataFrame(Seq(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(3, &quot;Name3&quot;)))<br>df1: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br>scala&gt; val df2 = spark.createDataFrame(Seq(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(2, &quot;Name22&quot;), Person(4, &quot;Name4&quot;)))<br><br><span class="hljs-section">scala&gt; df1.join(df2, &quot;id&quot;, joinType=&quot;inner&quot;).show</span><br><span class="hljs-section">+---+-----+------+</span><br><span class="hljs-section">| id| name|  name|</span><br><span class="hljs-section">+---+-----+------+</span><br>|  1|Name1| Name1|<br>|  2|Name2| Name2|<br><span class="hljs-section">|  2|Name2|Name22|</span><br><span class="hljs-section">+---+-----+------+</span><br><br><span class="hljs-section">scala&gt; df1.join(df2, df1(&quot;id&quot;) === df2(&quot;id&quot;)).show</span><br><span class="hljs-section">+---+-----+---+------+</span><br><span class="hljs-section">| id| name| id|  name|</span><br><span class="hljs-section">+---+-----+---+------+</span><br>|  1|Name1|  1| Name1|<br>|  2|Name2|  2| Name2|<br><span class="hljs-section">|  2|Name2|  2|Name22|</span><br><span class="hljs-section">+---+-----+---+------+</span><br></code></pre></td></tr></table></figure><h5 id="Full-Join"><a href="#Full-Join" class="headerlink" title="Full Join"></a>Full Join</h5><p>将当前 DataFrame 与另外的 DataFrame 进行全关联, 结果集中将包含左右 DataFrame 中的全部记录，匹配不到的数据置为空<br>joinType 为 <code>outer</code>, <code>full</code>, <code>fullouter</code>, <code>full_outer</code> 均表示 Full Join  </p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String)<br><br>scala&gt; val df1 = spark.createDataFrame(Seq(Person(1, <span class="hljs-string">&quot;Name1&quot;</span>), Person(2, <span class="hljs-string">&quot;Name2&quot;</span>), Person(3, <span class="hljs-string">&quot;Name3&quot;</span>)))<br>df1: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br>scala&gt; val df2 = spark.createDataFrame(Seq(Person(1, <span class="hljs-string">&quot;Name1&quot;</span>), Person(2, <span class="hljs-string">&quot;Name2&quot;</span>), Person(2, <span class="hljs-string">&quot;Name22&quot;</span>), Person(4, <span class="hljs-string">&quot;Name4&quot;</span>)))<br><br>scala&gt; df1.join(df2, <span class="hljs-string">&quot;id&quot;</span>, joinType=<span class="hljs-string">&quot;full&quot;</span>).show<br>+---+-----+------+<br>|<span class="hljs-string"> id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string">  name</span>|<br>+---+-----+------+<br>|<span class="hljs-string">  1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string"> Name1</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string"> Name2</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">Name22</span>|<br>|<span class="hljs-string">  3</span>|<span class="hljs-string">Name3</span>|<span class="hljs-string">  NULL</span>|<br>|<span class="hljs-string">  4</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string"> Name4</span>|<br>+---+-----+------+<br></code></pre></td></tr></table></figure><h5 id="Left-Join"><a href="#Left-Join" class="headerlink" title="Left Join"></a>Left Join</h5><p>将当前 DataFrame 与另外的 DataFrame 进行左关联, 结果集中仅包含左 DataFrame 中的全部记录，右 DataFrame 中匹配不到的数据置为空<br>joinType 为 <code>leftouter</code>, <code>left</code>, <code>left_outer</code> 均表示 Left Join</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br><br>scala&gt; val df1 = spark.createDataFrame(Seq(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(3, &quot;Name3&quot;)))<br>df1: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br>scala&gt; val df2 = spark.createDataFrame(Seq(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(2, &quot;Name22&quot;), Person(4, &quot;Name4&quot;)))<br><br><span class="hljs-section">scala&gt; df1.join(df2, &quot;id&quot;, joinType=&quot;left&quot;).show</span><br><span class="hljs-section">+---+-----+------+</span><br><span class="hljs-section">| id| name|  name|</span><br><span class="hljs-section">+---+-----+------+</span><br>|  1|Name1| Name1|<br>|  2|Name2|Name22|<br>|  2|Name2| Name2|<br><span class="hljs-section">|  3|Name3|  NULL|</span><br><span class="hljs-section">+---+-----+------+</span><br></code></pre></td></tr></table></figure><h5 id="Right-Join"><a href="#Right-Join" class="headerlink" title="Right Join"></a>Right Join</h5><p>将当前 DataFrame 与另外的 DataFrame 进行右关联, 结果集中仅包含右 DataFrame 中的全部记录，左 DataFrame 中匹配不到的数据置为空<br>joinType 为 <code>rightouter</code>, <code>right</code>, <code>right_outer</code> 均表示 Right Join</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br><br>scala&gt; val df1 = spark.createDataFrame(Seq(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(3, &quot;Name3&quot;)))<br>df1: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br>scala&gt; val df2 = spark.createDataFrame(Seq(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(2, &quot;Name22&quot;), Person(4, &quot;Name4&quot;)))<br><br><span class="hljs-section">scala&gt; df1.join(df2, &quot;id&quot;, joinType=&quot;right&quot;).show</span><br><span class="hljs-section">+---+-----+------+</span><br><span class="hljs-section">| id| name|  name|</span><br><span class="hljs-section">+---+-----+------+</span><br>|  1|Name1| Name1|<br>|  2|Name2| Name2|<br>|  2|Name2|Name22|<br><span class="hljs-section">|  4| NULL| Name4|</span><br><span class="hljs-section">+---+-----+------+</span><br></code></pre></td></tr></table></figure><h5 id="Semi-Join"><a href="#Semi-Join" class="headerlink" title="Semi Join"></a>Semi Join</h5><p>将当前 DataFrame 与另外的 DataFrame 进行(左)半关联, 结果集中仅包含左右 DataFrame 中的能匹配上的记录，并且右表中存在重复时，仅返回第一条记录。<br>joinType 为 <code>leftsemi</code>, <code>semi</code>, <code>left_semi</code> 均表示 Semi Join</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br><br>scala&gt; val df1 = spark.createDataFrame(Seq(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(3, &quot;Name3&quot;)))<br>df1: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br>scala&gt; val df2 = spark.createDataFrame(Seq(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(2, &quot;Name22&quot;), Person(4, &quot;Name4&quot;)))<br><br><span class="hljs-section">scala&gt; df1.join(df2, &quot;id&quot;, joinType=&quot;semi&quot;).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br><span class="hljs-section">|  2|Name2|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h5 id="Anti-Join"><a href="#Anti-Join" class="headerlink" title="Anti Join"></a>Anti Join</h5><p>将当前 DataFrame 与另外的 DataFrame 进行(左)反关联, 结果集中仅包含左 DataFrame 中与右 DataFrame 匹配不上的记录。 相当于用右 DF 对左 DF 求差集。<br>joinType 为 <code>leftanti</code>, <code>anti</code>, <code>left_anti</code> 均表示 Anti Join</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br><br>scala&gt; val df1 = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">Seq</span>(<span class="hljs-built_in">Person</span>(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;Name1&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">2</span>, <span class="hljs-string">&quot;Name2&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">3</span>, <span class="hljs-string">&quot;Name3&quot;</span>)))<br>df1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.sql</span><span class="hljs-selector-class">.DataFrame</span> = <span class="hljs-selector-attr">[id: int, name: string]</span><br><br>scala&gt; val df2 = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">Seq</span>(<span class="hljs-built_in">Person</span>(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;Name1&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">2</span>, <span class="hljs-string">&quot;Name2&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">2</span>, <span class="hljs-string">&quot;Name22&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">4</span>, <span class="hljs-string">&quot;Name4&quot;</span>)))<br><br>scala&gt; df1<span class="hljs-selector-class">.join</span>(df2, <span class="hljs-string">&quot;id&quot;</span>, joinType=<span class="hljs-string">&quot;full&quot;</span>).show<br></code></pre></td></tr></table></figure><h4 id="crossJoin"><a href="#crossJoin" class="headerlink" title="crossJoin"></a>crossJoin</h4><p>将当前 DataFrame 与另外的 DataFrame 进行关联, 返回笛卡尔积  </p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String)<br><br>scala&gt; val df1 = spark.createDataFrame(Seq(Person(1, <span class="hljs-string">&quot;Name1&quot;</span>), Person(2, <span class="hljs-string">&quot;Name2&quot;</span>), Person(3, <span class="hljs-string">&quot;Name3&quot;</span>)))<br>df1: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br>scala&gt; val df2 = spark.createDataFrame(Seq(Person(1, <span class="hljs-string">&quot;Name1&quot;</span>), Person(2, <span class="hljs-string">&quot;Name2&quot;</span>), Person(2, <span class="hljs-string">&quot;Name22&quot;</span>), Person(4, <span class="hljs-string">&quot;Name4&quot;</span>)))<br><br>scala&gt; df1.crossJoin(df2).show<br>+---+-----+---+------+<br>|<span class="hljs-string"> id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string"> id</span>|<span class="hljs-string">  name</span>|<br>+---+-----+---+------+<br>|<span class="hljs-string">  1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  1</span>|<span class="hljs-string"> Name1</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  1</span>|<span class="hljs-string"> Name1</span>|<br>|<span class="hljs-string">  3</span>|<span class="hljs-string">Name3</span>|<span class="hljs-string">  1</span>|<span class="hljs-string"> Name1</span>|<br>|<span class="hljs-string">  1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  2</span>|<span class="hljs-string"> Name2</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  2</span>|<span class="hljs-string"> Name2</span>|<br>|<span class="hljs-string">  3</span>|<span class="hljs-string">Name3</span>|<span class="hljs-string">  2</span>|<span class="hljs-string"> Name2</span>|<br>|<span class="hljs-string">  1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name22</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name22</span>|<br>|<span class="hljs-string">  3</span>|<span class="hljs-string">Name3</span>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name22</span>|<br>|<span class="hljs-string">  1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  4</span>|<span class="hljs-string"> Name4</span>|<br>|<span class="hljs-string">  2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  4</span>|<span class="hljs-string"> Name4</span>|<br>|<span class="hljs-string">  3</span>|<span class="hljs-string">Name3</span>|<span class="hljs-string">  4</span>|<span class="hljs-string"> Name4</span>|<br>+---+-----+---+------+<br></code></pre></td></tr></table></figure><h4 id="limit"><a href="#limit" class="headerlink" title="limit"></a>limit</h4><p>取当前 DataFrame 的前 n 条记录，返回一个新的 DataFrame  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df =  spark.createDataFrame((1 to 6).toList.map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df.limit(3).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="offset"><a href="#offset" class="headerlink" title="offset"></a>offset</h4><p>跳过当前 DataFrame 的前 n 条记录，返回一个新的 DataFrame  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df =  spark.createDataFrame((1 to 6).toList.map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df.offset(2).limit(3).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  3|Name3|<br>|  4|Name4|<br><span class="hljs-section">|  5|Name5|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>两个 DataFrame 求并集，按列的位置进行合并，不会对结果去重，返回一个新的 DataFrame  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df1 =  spark.createDataFrame((1 to 3).toList.map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val df2 =  spark.createDataFrame((3 to 5).toList.map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df1.union(df2).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  3|Name3|<br>|  4|Name4|<br><span class="hljs-section">|  5|Name5|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="unionAll"><a href="#unionAll" class="headerlink" title="unionAll"></a>unionAll</h4><p>与 <a href="#union">union</a> 的行为一致，提供与 SQL 语义一致的同名算子</p><h4 id="unionByName"><a href="#unionByName" class="headerlink" title="unionByName"></a>unionByName</h4><p>两个 DataFrame 求并集，按列的名称进行合并，不会对结果去重，返回一个新的 DataFrame  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df1 =  spark.createDataFrame((1 to 3).toList.map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val df2 =  spark.createDataFrame((3 to 5).toList.map(x =&gt; Person(x, &quot;Name&quot; + x))).to<br><br><span class="hljs-section">scala&gt; df1.union(df2).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  3|Name3|<br>|  4|Name4|<br><span class="hljs-section">|  5|Name5|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="intersect"><a href="#intersect" class="headerlink" title="intersect"></a>intersect</h4><p>返回两个 DataFrame 的交集，会对结果去重  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df1 =  spark.createDataFrame(List(1, 2, 3, 3, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val df2 =  spark.createDataFrame(List(3, 3, 4, 5).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df1.intersect(df2).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="intersectAll"><a href="#intersectAll" class="headerlink" title="intersectAll"></a>intersectAll</h4><p>返回两个 DataFrame 的交集，不会对结果去重  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df1 =  spark.createDataFrame(List(1, 2, 3, 3, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val df2 =  spark.createDataFrame(List(3, 3, 4, 5).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df1.intersectAll(df2).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  3|Name3|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br></code></pre></td></tr></table></figure><h4 id="except"><a href="#except" class="headerlink" title="except"></a>except</h4><p>返回两个 DataFrame 的差集，不会对结果去重</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df1 =  spark.createDataFrame(List(1, 2, 2, 3, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val df2 =  spark.createDataFrame(List(3, 4, 5).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df1.except(df2).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br><span class="hljs-section">|  2|Name2|</span><br><span class="hljs-section">+---+-----+</span><br><br></code></pre></td></tr></table></figure><h4 id="exceptAll"><a href="#exceptAll" class="headerlink" title="exceptAll"></a>exceptAll</h4><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df1 =  spark.createDataFrame(List(1, 2, 2, 3, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val df2 =  spark.createDataFrame(List(3, 4, 5).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df1.exceptAll(df2).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h4><p>对 DataFrame 进行采样，返回包含样本记录的新 DataFrame, 参数 fraction 不代表精确的比例，仅代表每条记录被命中的概率  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame((1 to 100).toList.map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df.sample(0.05).show()</span><br><span class="hljs-section">+---+------+</span><br><span class="hljs-section">| id|  name|</span><br><span class="hljs-section">+---+------+</span><br>|  1| Name1|<br>| 17|Name17|<br>| 21|Name21|<br><span class="hljs-section">| 90|Name90|</span><br><span class="hljs-section">+---+------+</span><br></code></pre></td></tr></table></figure><h4 id="randomSplit"><a href="#randomSplit" class="headerlink" title="randomSplit"></a>randomSplit</h4><p>将 DataFrame 切分成一组 DataFrame 的 Array, 切分成多少组由权重 weights 的数组大小决定, 权重不代表精确的比例，仅代表每条记录被命中的概率  </p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">scala&gt; <span class="hljs-keyword">case</span> <span class="hljs-keyword">class</span> Person(id: <span class="hljs-type">Int</span>, <span class="hljs-type">name</span>: String)<br>scala&gt; val df = spark.createDataFrame((<span class="hljs-number">1</span> <span class="hljs-keyword">to</span> <span class="hljs-number">100</span>).toList.map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val splitedDFs = df.randomSplit(<span class="hljs-keyword">Array</span>(<span class="hljs-number">0.2</span>, <span class="hljs-number">0.8</span>))<br>splitedDFs: <span class="hljs-keyword">Array</span>[org.apache.spark.<span class="hljs-keyword">sql</span>.Dataset[org.apache.spark.<span class="hljs-keyword">sql</span>.<span class="hljs-keyword">Row</span>]] = <span class="hljs-keyword">Array</span>([id: <span class="hljs-type">int</span>, <span class="hljs-type">name</span>: string], [id: <span class="hljs-type">int</span>, <span class="hljs-type">name</span>: string])<br><br>scala&gt; splitedDFs.map(_.count)<br>res0: <span class="hljs-keyword">Array</span>[Long] = <span class="hljs-keyword">Array</span>(<span class="hljs-number">16</span>, <span class="hljs-number">84</span>)<br></code></pre></td></tr></table></figure><h4 id="randomSplitAsList"><a href="#randomSplitAsList" class="headerlink" title="randomSplitAsList"></a>randomSplitAsList</h4><p>将 DataFrame 切分成一组 DataFrame 的 java List, 切分成多少组由权重 weights 的数组大小决定, 并要求传入种子值 seed。权重不代表精确的比例，仅代表每条记录被命中的概率</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>((<span class="hljs-number">1</span> to <span class="hljs-number">100</span>)<span class="hljs-selector-class">.toList</span><span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">Person</span>(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; val splitedDFs = df<span class="hljs-selector-class">.randomSplitAsList</span>(weights=<span class="hljs-built_in">Array</span>(<span class="hljs-number">0.2</span>, <span class="hljs-number">0.8</span>), seed=scala<span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.Random</span>.nextLong)<br>splitedDFs: java<span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.List</span><span class="hljs-selector-attr">[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]</span>] = <span class="hljs-selector-attr">[[id: int, name: string]</span>, <span class="hljs-selector-attr">[id: int, name: string]</span>]<br><br>scala&gt; splitedDFs<span class="hljs-selector-class">.asScala</span><span class="hljs-selector-class">.map</span>(_.count)<br>res0: scala<span class="hljs-selector-class">.collection</span><span class="hljs-selector-class">.mutable</span><span class="hljs-selector-class">.Buffer</span><span class="hljs-selector-attr">[Long]</span> = <span class="hljs-built_in">ArrayBuffer</span>(<span class="hljs-number">14</span>, <span class="hljs-number">86</span>)<br></code></pre></td></tr></table></figure><h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p>将 DataFrame 按指定的列进行分组，返回一个 RelationalGroupedDataset，以便进行后续的聚合操作  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 1, 2, 2, 2).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val groupedDF = df.groupBy(df(&quot;id&quot;))<br>groupedDF: org.apache.spark.sql.RelationalGroupedDataset = RelationalGroupedDataset: [grouping expressions: [id: int], value: [id: int, name: string], type: GroupBy]<br><br><span class="hljs-section">scala&gt; groupedDF.count.show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id|count|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|    2|<br><span class="hljs-section">|  2|    3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="agg"><a href="#agg" class="headerlink" title="agg"></a>agg</h4><p>将 DataFrame 视为一个整体进行聚合操作  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 1, 2, 2, 2).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br><span class="hljs-section">scala&gt; df.agg(max(&quot;id&quot;)).show</span><br><span class="hljs-section">+-------+</span><br><span class="hljs-section">|max(id)|</span><br><span class="hljs-section">+-------+</span><br><span class="hljs-section">|      2|</span><br><span class="hljs-section">+-------+</span><br><br><span class="hljs-section">scala&gt; df.agg(Map(&quot;id&quot; -&gt; &quot;count&quot;, &quot;name&quot; -&gt; &quot;max&quot;)).show</span><br><span class="hljs-section">+---------+---------+</span><br><span class="hljs-section">|count(id)|max(name)|</span><br><span class="hljs-section">+---------+---------+</span><br><span class="hljs-section">|        5|    Name2|</span><br><span class="hljs-section">+---------+---------+</span><br></code></pre></td></tr></table></figure><h4 id="rollup"><a href="#rollup" class="headerlink" title="rollup"></a>rollup</h4><p>将 DataFrame 按指定的列进行多维逐级聚合操作，类似与 <a href="#groupby">groupBy</a>，同样返回一个 RelationalGroupedDataset，以便进行后续的聚合操作。<br>但是会以指定的列逐级分组，即： 假设给定维度为 (colA, colB, colC)，则会分别按照如下组合对数据进行汇总：  </p><ul><li>colA, colB, colC</li><li>colA, colB</li><li>colA</li><li>None</li></ul><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String, age: Int)<br>scala&gt; val df = spark.createDataFrame(List(1, 1, 2, 2, 2).map(x =&gt; Person(x, <span class="hljs-string">&quot;Name&quot;</span> + x, 50)))<br><br>scala&gt; df.rollup(<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;name&quot;</span>, <span class="hljs-string">&quot;age&quot;</span>).count().show<br>+----+-----+----+-----+<br>|<span class="hljs-string">  id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string"> age</span>|<span class="hljs-string">count</span>|<br>+----+-----+----+-----+<br>|<span class="hljs-string">   1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">   2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">   1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">   2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">   1</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">   2</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">NULL</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    5</span>|<br>+----+-----+----+-----+<br><br></code></pre></td></tr></table></figure><h4 id="cube"><a href="#cube" class="headerlink" title="cube"></a>cube</h4><p>将 DataFrame 按指定的列创建一个多维立方体，类似与 <a href="#groupby">groupBy</a>，同样返回一个 RelationalGroupedDataset，以便进行后续的聚合操作。<br>但是会以指定的列按所有的维度组合进行分组，多维立方体(cube)即： 假设给定维度为 (colA, colB, colC)，则会分别按照如下组合对数据进行汇总：</p><ul><li>colA, colB, colC</li><li>colA, colB</li><li>colA, colC</li><li>colB, colC</li><li>colA</li><li>colB</li><li>colC</li><li>None</li></ul><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String, age: Int)<br>scala&gt; val df = spark.createDataFrame(List(1, 1, 2, 2, 2).map(x =&gt; Person(x, <span class="hljs-string">&quot;Name&quot;</span> + x, 50)))<br><br>scala&gt; df.cube(<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;name&quot;</span>, <span class="hljs-string">&quot;age&quot;</span>).count().show<br>+----+-----+----+-----+<br>|<span class="hljs-string">  id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string"> age</span>|<span class="hljs-string">count</span>|<br>+----+-----+----+-----+<br>|<span class="hljs-string">   1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">   2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">   1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">   2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">   1</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">   2</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">NULL</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">NULL</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">   1</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">   2</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">NULL</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    2</span>|<br>|<span class="hljs-string">NULL</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    3</span>|<br>|<span class="hljs-string">NULL</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">  50</span>|<span class="hljs-string">    5</span>|<br>|<span class="hljs-string">NULL</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">NULL</span>|<span class="hljs-string">    5</span>|<br>+----+-----+----+-----+<br><br></code></pre></td></tr></table></figure><h4 id="groupingSets-Spark-4-0"><a href="#groupingSets-Spark-4-0" class="headerlink" title="groupingSets  Spark 4.0 +"></a>groupingSets  Spark 4.0 +</h4><p>todo 待补充</p><h4 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h4><p>对 DataFrame 进行去重，完全重复的数据将仅保留一条</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(2, &quot;Name2&quot;), Person(3, &quot;Name3&quot;), Person(3, &quot;Name333&quot;)))<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.distinct.show</span><br><span class="hljs-section">+---+-------+</span><br><span class="hljs-section">| id|   name|</span><br><span class="hljs-section">+---+-------+</span><br>|  1|  Name1|<br>|  2|  Name2|<br>|  3|  Name3|<br><span class="hljs-section">|  3|Name333|</span><br><span class="hljs-section">+---+-------+</span><br></code></pre></td></tr></table></figure><h4 id="dropDuplicates"><a href="#dropDuplicates" class="headerlink" title="dropDuplicates"></a>dropDuplicates</h4><p>对 DataFrame 按指定的列进行去重，相同列的重复记录将仅保留第一条记录</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(Person(1, &quot;Name1&quot;), Person(2, &quot;Name2&quot;), Person(2, &quot;Name2&quot;), Person(3, &quot;Name3&quot;), Person(3, &quot;Name333&quot;)))<br>df: org.apache.spark.sql.DataFrame = [id: int, name: string]<br><br><span class="hljs-section">scala&gt; df.dropDuplicates(&quot;id&quot;).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h4 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h4><p>对 DataFrame 的数据进行描述，返回一些常用的统计指标  </p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String, age: Int)<br>scala&gt; val df = spark.createDataFrame(List(1, 1, 2, 2, 2).map(x =&gt; Person(x, <span class="hljs-string">&quot;Name&quot;</span> + x, 50)))<br><br>scala&gt; df.describe().show<br>+-------+------------------+-----+----+<br>|<span class="hljs-string">summary</span>|<span class="hljs-string">                id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string"> age</span>|<br>+-------+------------------+-----+----+<br>|<span class="hljs-string">  count</span>|<span class="hljs-string">                 5</span>|<span class="hljs-string">    5</span>|<span class="hljs-string">   5</span>|<br>|<span class="hljs-string">   mean</span>|<span class="hljs-string">               1.6</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">50.0</span>|<br>|<span class="hljs-string"> stddev</span>|<span class="hljs-string">0.5477225575051661</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string"> 0.0</span>|<br>|<span class="hljs-string">    min</span>|<span class="hljs-string">                 1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  50</span>|<br>|<span class="hljs-string">    max</span>|<span class="hljs-string">                 2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  50</span>|<br>+-------+------------------+-----+----+<br></code></pre></td></tr></table></figure><h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p>与 <a href="#describe">describe</a> 类似，在其基础上增加了 <code>p25</code>, <code>p50</code>, <code>p75</code> 等指标  </p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">scala&gt; case class Person(id: Int, name: String, age: Int)<br>scala&gt; val df = spark.createDataFrame(List(1, 1, 2, 2, 2).map(x =&gt; Person(x, <span class="hljs-string">&quot;Name&quot;</span> + x, 50)))<br><br>scala&gt; df.summary().show<br>+-------+------------------+-----+----+<br>|<span class="hljs-string">summary</span>|<span class="hljs-string">                id</span>|<span class="hljs-string"> name</span>|<span class="hljs-string"> age</span>|<br>+-------+------------------+-----+----+<br>|<span class="hljs-string">  count</span>|<span class="hljs-string">                 5</span>|<span class="hljs-string">    5</span>|<span class="hljs-string">   5</span>|<br>|<span class="hljs-string">   mean</span>|<span class="hljs-string">               1.6</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">50.0</span>|<br>|<span class="hljs-string"> stddev</span>|<span class="hljs-string">0.5477225575051661</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string"> 0.0</span>|<br>|<span class="hljs-string">    min</span>|<span class="hljs-string">                 1</span>|<span class="hljs-string">Name1</span>|<span class="hljs-string">  50</span>|<br>|<span class="hljs-string">    25%</span>|<span class="hljs-string">                 1</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">  50</span>|<br>|<span class="hljs-string">    50%</span>|<span class="hljs-string">                 2</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">  50</span>|<br>|<span class="hljs-string">    75%</span>|<span class="hljs-string">                 2</span>|<span class="hljs-string"> NULL</span>|<span class="hljs-string">  50</span>|<br>|<span class="hljs-string">    max</span>|<span class="hljs-string">                 2</span>|<span class="hljs-string">Name2</span>|<span class="hljs-string">  50</span>|<br>+-------+------------------+-----+----+<br></code></pre></td></tr></table></figure><h2 id="Action-算子"><a href="#Action-算子" class="headerlink" title="Action 算子"></a>Action 算子</h2><h3 id="转换为内存集合"><a href="#转换为内存集合" class="headerlink" title="转换为内存集合"></a>转换为内存集合</h3><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p>对 DataFrame 进行合并操作, 所有记录按照用户指定的 (left, right) &#x3D;&gt; result 函数从左到右进行合并, 返回一个 Java 集合<br>如下面的例子中，仅保留最大的 id 的第一条记录</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">List</span>(<span class="hljs-built_in">Person</span>(<span class="hljs-number">1</span>, <span class="hljs-string">&quot;Name1&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">2</span>, <span class="hljs-string">&quot;Name2&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">2</span>, <span class="hljs-string">&quot;Name2&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">3</span>, <span class="hljs-string">&quot;Name3&quot;</span>), <span class="hljs-built_in">Person</span>(<span class="hljs-number">3</span>, <span class="hljs-string">&quot;Name333&quot;</span>)))<br><br>scala&gt; df<span class="hljs-selector-class">.reduce</span>((row_left, row_right) =&gt; <span class="hljs-keyword">if</span> (row_right<span class="hljs-selector-class">.getAs</span><span class="hljs-selector-attr">[Int]</span>(<span class="hljs-string">&quot;id&quot;</span>) &gt; row_left<span class="hljs-selector-class">.getAs</span><span class="hljs-selector-attr">[Int]</span>(<span class="hljs-string">&quot;id&quot;</span>)) row_right <span class="hljs-keyword">else</span> row_left)<br>res0: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.sql</span><span class="hljs-selector-class">.Row</span> = <span class="hljs-selector-attr">[3,Name3]</span><br></code></pre></td></tr></table></figure><h4 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h4><p>遍历 DataFrame 中的每一条记录，根据提供的 row &#x3D;&gt; Unit 函数，将记录写入外部系统，或打印到控制台，或添加到其他 Java 集合中等</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs nix">scala<span class="hljs-operator">&gt;</span> case class Person(<span class="hljs-params">id:</span> Int, <span class="hljs-params">name:</span> String)<br>scala<span class="hljs-operator">&gt;</span> val df <span class="hljs-operator">=</span> spark.createDataFrame(List(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).<span class="hljs-built_in">map</span>(x <span class="hljs-operator">=</span><span class="hljs-operator">&gt;</span> Person(x, <span class="hljs-string">&quot;Name&quot;</span> <span class="hljs-operator">+</span> x)))<br><br>scala<span class="hljs-operator">&gt;</span> df.foreach(row <span class="hljs-operator">=</span><span class="hljs-operator">&gt;</span> println(s<span class="hljs-string">&quot;id: <span class="hljs-subst">$&#123;row.get(<span class="hljs-number">0</span>)&#125;</span>, name: <span class="hljs-subst">$&#123;row.get(<span class="hljs-number">1</span>)&#125;</span>&quot;</span>))<br><span class="hljs-params">id:</span> <span class="hljs-number">1</span>, <span class="hljs-params">name:</span> Name1<br><span class="hljs-params">id:</span> <span class="hljs-number">2</span>, <span class="hljs-params">name:</span> Name2<br></code></pre></td></tr></table></figure><h4 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h4><p>遍历 DataFrame 中的每一个 partition，每个 partition 中的 row 被封装在一个 Iterator 中， 根据提供的 iterator[Row] &#x3D;&gt; Unit 函数，将记录写入外部系统，或打印到控制台，或添加到其他 Java 集合中等</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs nix">scala<span class="hljs-operator">&gt;</span> case class Person(<span class="hljs-params">id:</span> Int, <span class="hljs-params">name:</span> String)<br>scala<span class="hljs-operator">&gt;</span> val df <span class="hljs-operator">=</span> spark.createDataFrame(List(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).<span class="hljs-built_in">map</span>(x <span class="hljs-operator">=</span><span class="hljs-operator">&gt;</span> Person(x, <span class="hljs-string">&quot;Name&quot;</span> <span class="hljs-operator">+</span> x))).repartition(<span class="hljs-number">2</span>)<br><br>scala<span class="hljs-operator">&gt;</span> df.foreachPartition&#123; (<span class="hljs-params">iter:</span> Iterator[org.apache.spark.sql.Row]) <span class="hljs-operator">=</span><span class="hljs-operator">&gt;</span> <br>  val partitionId <span class="hljs-operator">=</span> org.apache.spark.TaskContext.getPartitionId <br>  iter.foreach(row <span class="hljs-operator">=</span><span class="hljs-operator">&gt;</span> println(s<span class="hljs-string">&quot;partition: <span class="hljs-subst">$&#123;partitionId&#125;</span>, id: <span class="hljs-subst">$&#123;row.get(<span class="hljs-number">0</span>)&#125;</span>, name: <span class="hljs-subst">$&#123;row.get(<span class="hljs-number">1</span>)&#125;</span>&quot;</span>))<br>&#125;<br><span class="hljs-params">partition:</span> <span class="hljs-number">0</span>, <span class="hljs-params">id:</span> <span class="hljs-number">1</span>, <span class="hljs-params">name:</span> Name1<br><span class="hljs-params">partition:</span> <span class="hljs-number">1</span>, <span class="hljs-params">id:</span> <span class="hljs-number">2</span>, <span class="hljs-params">name:</span> Name2<br></code></pre></td></tr></table></figure><h4 id="isEmpty"><a href="#isEmpty" class="headerlink" title="isEmpty"></a>isEmpty</h4><p>判断 DataFrame 是否为空, 返回 true 或 false  </p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs livescript">scala&gt; <span class="hljs-keyword">case</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Person</span>(id: Int, name: <span class="hljs-built_in">String</span>)<br>scala&gt; val df = spark.createDataFrame(List(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).<span class="hljs-keyword">map</span>(x =&gt; Person(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; df.isEmpty<br>res0: <span class="hljs-built_in">Boolean</span> = <span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure><h4 id="head"><a href="#head" class="headerlink" title="head"></a>head</h4><p>返回 DataFrame 的前 n 条记录，默认为 n &#x3D; 1  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">Person</span>(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; df<span class="hljs-selector-class">.head</span><br>res0: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.sql</span><span class="hljs-selector-class">.Row</span> = <span class="hljs-selector-attr">[1,Name1]</span><br><br>scala&gt; df<span class="hljs-selector-class">.head</span>(<span class="hljs-number">2</span>)<br>res1: Array<span class="hljs-selector-attr">[org.apache.spark.sql.Row]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-selector-attr">[1,Name1]</span>, <span class="hljs-selector-attr">[2,Name2]</span>)<br></code></pre></td></tr></table></figure><h4 id="first"><a href="#first" class="headerlink" title="first"></a>first</h4><p>与 <a href="#head">head(1)</a> 语义一致  </p><h4 id="take"><a href="#take" class="headerlink" title="take"></a>take</h4><p>与 <a href="#head">head(n)</a> 语义一致</p><h4 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h4><p>返回 DataFrame 的后 n 条记录</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">Person</span>(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; df<span class="hljs-selector-class">.tail</span>(<span class="hljs-number">2</span>)<br>res0: Array<span class="hljs-selector-attr">[org.apache.spark.sql.Row]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-selector-attr">[2,Name2]</span>, <span class="hljs-selector-attr">[3,Name3]</span>)<br></code></pre></td></tr></table></figure><h4 id="takeAsList"><a href="#takeAsList" class="headerlink" title="takeAsList"></a>takeAsList</h4><p>与 <a href="#take">take</a> 相似，只是返回返回一个 Java 的 List  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">Person</span>(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; df<span class="hljs-selector-class">.takeAsList</span>(<span class="hljs-number">2</span>)<br>res1: java<span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.List</span><span class="hljs-selector-attr">[org.apache.spark.sql.Row]</span> = <span class="hljs-selector-attr">[[1,Name1]</span>, <span class="hljs-selector-attr">[2,Name2]</span>]<br></code></pre></td></tr></table></figure><h4 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h4><p>将 DataFrame 的所有记录收集起来，返回一个 Java 的 Array 集合到 Driver 端，避免对大数据集使用此操作，以防止 Driver 端 OOM   </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">Person</span>(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; df<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[org.apache.spark.sql.Row]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-selector-attr">[1,Name1]</span>, <span class="hljs-selector-attr">[2,Name2]</span>, <span class="hljs-selector-attr">[3,Name3]</span>)<br></code></pre></td></tr></table></figure><h4 id="collectAsList"><a href="#collectAsList" class="headerlink" title="collectAsList"></a>collectAsList</h4><p>与 <a href="#collect">collect</a> 相似，只是返回一个 Java 的 List，避免对大数据集使用此操作，以防止 Driver 端 OOM  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">Person</span>(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; df<span class="hljs-selector-class">.collect</span><br>res0: java<span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.List</span><span class="hljs-selector-attr">[org.apache.spark.sql.Row]</span> = <span class="hljs-selector-attr">[[1,Name1]</span>, <span class="hljs-selector-attr">[2,Name2]</span>, <span class="hljs-selector-attr">[3,Name3]</span>]<br></code></pre></td></tr></table></figure><h4 id="toLocalIterator"><a href="#toLocalIterator" class="headerlink" title="toLocalIterator"></a>toLocalIterator</h4><p>将 DataFrame 的所有记录封装成 Java 的迭代器， Driver 端消耗的内存将与最大的 partition 消耗的内存一致  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">Person</span>(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; val rowIterator = df<span class="hljs-selector-class">.toLocalIterator</span><br>rowIterator: java<span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.Iterator</span><span class="hljs-selector-attr">[org.apache.spark.sql.Row]</span> = <span class="hljs-built_in">IteratorWrapper</span>(&lt;iterator&gt;)<br><br>scala&gt; rowIterator<span class="hljs-selector-class">.next</span><br>res0: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.sql</span><span class="hljs-selector-class">.Row</span> = <span class="hljs-selector-attr">[1,Name1]</span><br></code></pre></td></tr></table></figure><h4 id="count"><a href="#count" class="headerlink" title="count"></a>count</h4><p>返回 DataFrame 的总记录数 </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; case class <span class="hljs-built_in">Person</span>(id: Int, name: String)<br>scala&gt; val df = spark<span class="hljs-selector-class">.createDataFrame</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">Person</span>(x, <span class="hljs-string">&quot;Name&quot;</span> + x)))<br><br>scala&gt; df<span class="hljs-selector-class">.count</span><br>res0: Long = <span class="hljs-number">3</span><br></code></pre></td></tr></table></figure><h3 id="createOrReplaceGlobalTempView"><a href="#createOrReplaceGlobalTempView" class="headerlink" title="createOrReplaceGlobalTempView"></a>createOrReplaceGlobalTempView</h3><p>与 <a href="#createGlobalTempView">createGlobalTempView</a> 类似，只是视图存在时将覆盖原有视图</p><h3 id="写入外部算子"><a href="#写入外部算子" class="headerlink" title="写入外部算子"></a>写入外部算子</h3><h4 id="write-v1"><a href="#write-v1" class="headerlink" title="write v1"></a>write v1</h4><p>DataFrame.write 方法将返回一个 DataFrameWriter 实例，包含以下方法</p><h5 id="insertInto"><a href="#insertInto" class="headerlink" title="insertInto"></a>insertInto</h5><p>将 DataFrame 按写入指定的表中，写入操作是基于列的顺序的，因此要求目标表结构必须与 DataFrame 的 Schema 一致<br>不会自动创建表，可根据 <a href="#mode-">mode</a> 参数来选择写入模式，默认为追加记录  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; spark.sql(&quot;create table test_table__insert_into(id int, name string)&quot;)<br>res0: org.apache.spark.sql.DataFrame = []<br><br>scala&gt; df.write.mode(&quot;append&quot;).insertInto(&quot;test<span class="hljs-emphasis">_table__insert_into&quot;)</span><br><span class="hljs-emphasis">scala&gt; df.write.mode(&quot;append&quot;).insertInto(&quot;test_table__insert_</span>into&quot;)<br><br><span class="hljs-section">scala&gt; spark.table(&quot;test_table__insert_into&quot;).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  1|Name1|<br>|  2|Name2|<br>|  2|Name2|<br>|  3|Name3|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt; df.write.mode(&quot;overwrite&quot;).insertInto(&quot;test_table__insert_into&quot;)<br><br><span class="hljs-section">scala&gt; spark.table(&quot;test_table__insert_into&quot;).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt; df.select(&quot;name&quot;, &quot;id&quot;).write.insertInto(&quot;test<span class="hljs-emphasis">_table__insert_into&quot;)</span><br><span class="hljs-emphasis">org.apache.spark.sql.AnalysisException: [INCOMPATIBLE_DATA_FOR_TABLE.CANNOT_SAFELY_CAST] Cannot write incompatible data for the table `spark_catalog`.`default`.`test_</span>person<span class="hljs-code">`: Cannot safely cast `</span>id` &quot;STRING&quot; to &quot;INT&quot;.<br><span class="hljs-code">  at org.apache.spark.sql.errors.QueryCompilationErrors$.incompatibleDataToTableCannotSafelyCastError(QueryCompilationErrors.scala:2216)</span><br></code></pre></td></tr></table></figure><h5 id="saveAsTable"><a href="#saveAsTable" class="headerlink" title="saveAsTable"></a>saveAsTable</h5><p>将 DataFrame 按写入指定的表中，写入操作是基于列名的，因此不会对 DataFrame 的列顺序有严格要求，只要列存在即可<br>表不存在时将自动创建，可根据 <a href="#mode-">mode</a> 参数来选择写入模式，默认为表存在时报错  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; df.write.mode(&quot;overwrite&quot;).saveAsTable(&quot;test_person_save_as_table&quot;)<br><br><span class="hljs-section">scala&gt; spark.table(&quot;test_person_save_as_table&quot;).show()</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br><span class="hljs-section">scala&gt; df.select(&quot;name&quot;, &quot;id&quot;).write.mode(&quot;append&quot;).saveAsTable(&quot;test_person_save_as_table&quot;)</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  1|Name1|<br>|  2|Name2|<br>|  2|Name2|<br>|  3|Name3|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h5 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h5><p>将 DataFrame 按 jdbc 协议写入外部数据库，如 MySQL&#x2F;Postgres 等, 等效于 <a href="#format">format</a> 为 <code>jdbc</code> 时的 <a href="#save">save</a> 操作<br>表不存在时将自动创建，可根据 <a href="#mode-">mode</a> 参数来选择写入模式，默认为表存在时报错<br>本例子中需要使用 MySQL 驱动来连接本地的 MySQL 数据库，因此需要将相应的驱动包加入 classpath，比如：  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">spark-shell --packages &quot;mysql:mysql-connector-java:8.0.28&quot; <br></code></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; val connectionProperties = new java.util.Properties()<br>scala&gt; connectionProperties.put(&quot;user&quot;, &quot;root&quot;)<br>scala&gt; connectionProperties.put(&quot;password&quot;, &quot;123456&quot;)<br>scala&gt; connectionProperties.put(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)<br><br>scala&gt; df.write.jdbc(<br><span class="hljs-code">  url=&quot;jdbc:mysql://localhost:3306/test?createDatabaseIfNotExist=true&quot;, </span><br><span class="hljs-code">  table=&quot;test_table__jdbc&quot;,</span><br><span class="hljs-code">  connectionProperties=connectionProperties</span><br>)<br><br>scala&gt; spark.read.jdbc(<br><span class="hljs-code">  url=&quot;jdbc:mysql://localhost:3306/test?createDatabaseIfNotExist=true&quot;, </span><br><span class="hljs-code">  table=&quot;test_table__jdbc&quot;,</span><br><span class="hljs-code">  properties=connectionProperties</span><br><span class="hljs-section">).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  2|Name2|<br>|  1|Name1|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure><h5 id="save"><a href="#save" class="headerlink" title="save"></a>save</h5><p>将 DataFrame 保存到外部存储系统中, 是 <a href="#jdbc">jdbc</a> 以及各个写文件操作（如 <a href="#json">json</a>, <a href="#csv">csv</a> 等）的统一抽象 </p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">scala&gt; <span class="hljs-keyword">case</span> <span class="hljs-keyword">class</span> Person(id: <span class="hljs-type">Int</span>, <span class="hljs-type">name</span>: String)<br>scala&gt; val df = spark.createDataFrame(List(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; &#123; <br>df.<span class="hljs-keyword">write</span>.format(&quot;jdbc&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test?createDatabaseIfNotExist=true&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;user&quot;, &quot;root&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;password&quot;, &quot;123456&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;dbtable&quot;, &quot;test_table__jdbc_save&quot;)<br>  .save<br>&#125;<br><br>scala&gt; &#123; <br>spark.<span class="hljs-keyword">read</span>.format(&quot;jdbc&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test?createDatabaseIfNotExist=true&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;user&quot;, &quot;root&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;password&quot;, &quot;123456&quot;)<br>  .<span class="hljs-keyword">option</span>(&quot;dbtable&quot;, &quot;test_table__jdbc_save&quot;)<br>  .<span class="hljs-keyword">load</span><br>&#125;.<span class="hljs-keyword">show</span><br>+<span class="hljs-comment">---+-----+</span><br>| id| <span class="hljs-type">name</span>|<br>+<span class="hljs-comment">---+-----+</span><br>|  <span class="hljs-number">2</span>|Name2|<br>|  <span class="hljs-number">3</span>|Name3|<br>|  <span class="hljs-number">1</span>|Name1|<br>+<span class="hljs-comment">---+-----+</span><br><br>scala&gt; df.<span class="hljs-keyword">write</span>.mode(&quot;overwrite&quot;).format(&quot;csv&quot;).<span class="hljs-keyword">option</span>(&quot;header&quot;, &quot;true&quot;).save(&quot;hdfs:///test_write/test_csv&quot;)<br><br>scala&gt; spark.<span class="hljs-keyword">read</span>.format(&quot;csv&quot;).<span class="hljs-keyword">option</span>(&quot;header&quot;, &quot;true&quot;).<span class="hljs-keyword">load</span>(&quot;hdfs:///test_write/test_csv&quot;).<span class="hljs-keyword">show</span><br>+<span class="hljs-comment">---+-----+</span><br>| id| <span class="hljs-type">name</span>|<br>+<span class="hljs-comment">---+-----+</span><br>|  <span class="hljs-number">1</span>|Name1|<br>|  <span class="hljs-number">2</span>|Name2|<br>|  <span class="hljs-number">3</span>|Name3|<br>+<span class="hljs-comment">---+-----+</span><br></code></pre></td></tr></table></figure><h5 id="json"><a href="#json" class="headerlink" title="json"></a>json</h5><p>等同于 <a href="#format">format</a> 为 <code>json</code> 时的 <a href="#save">save</a> 操作  </p><h5 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h5><p>等同于 <a href="#format">format</a> 为 <code>parquet</code> 时的 <a href="#save">save</a> 操作</p><h5 id="orc"><a href="#orc" class="headerlink" title="orc"></a>orc</h5><p>等同于 <a href="#format">format</a> 为 <code>orc</code> 时的 <a href="#save">save</a> 操作</p><h5 id="text"><a href="#text" class="headerlink" title="text"></a>text</h5><p>等同于 <a href="#format">format</a> 为 <code>text</code> 时的 <a href="#save">save</a> 操作</p><h5 id="csv"><a href="#csv" class="headerlink" title="csv"></a>csv</h5><p>等同于 <a href="#format">format</a> 为 <code>csv</code> 时的 <a href="#save">save</a> 操作</p><h5 id="xml"><a href="#xml" class="headerlink" title="xml"></a>xml</h5><p>等同于 <a href="#format">format</a> 为 <code>xml</code> 时的 <a href="#save">save</a> 操作</p><h5 id="mode"><a href="#mode" class="headerlink" title="mode"></a>mode</h5><p>指定写入目标资源（表或文件目录等）时的写入模式  </p><ul><li>Overwrite: 将目标资源的原有记录覆盖，saveMode 为 <code>overwrite</code> 即表示 Overwrite 语义</li><li>Append: 向目标资源中追加记录，saveMode 为 <code>append</code> 即表示 Append 语义</li><li>Ignore: 当目标资源已存在时，将不做任何操作，saveMode 为 <code>ingore</code> 即表示 Ignore 语义</li><li>ErrorIfExists: 当目标资源已存在时，将抛出异常，saveMode 为 <code>error</code>, <code>errorifexists</code>, <code>default</code> 均表示 ErrorIfExists 语义</li></ul><h4 id="writeTo-v2"><a href="#writeTo-v2" class="headerlink" title="writeTo v2"></a>writeTo v2</h4><p>DataFrame.writeTo(tableName) 方法将返回一个 DataFrameWriterV2 实例，用于操作 V2 的表（如 DataLake：Iceberg 等）<br>本例子中将采用 Spark Tests 中常用的 InMemoryTable，需先下载 <a href="https://repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.12/3.5.1/spark-catalyst_2.12-3.5.1-tests.jar">spark-catalyst_2.12-3.5.1-tests.jar</a>  </p><p>启动 spark-shell  </p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">spark</span>-shell --jars spark-catalyst_2.<span class="hljs-number">12</span>-<span class="hljs-number">3</span>.<span class="hljs-number">5</span>.<span class="hljs-number">1</span>-tests.jar<br></code></pre></td></tr></table></figure><p>启动后注册 InMemoryTableCatalog 后，即可正常操作 InMemoryTable  </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">spark.conf().<span class="hljs-built_in">set</span>(<span class="hljs-string">&quot;spark.sql.catalog.in_mem_catalog&quot;</span>, <span class="hljs-string">&quot;org.apache.spark.sql.connector.catalog.InMemoryTableCatalog&quot;</span>)<br></code></pre></td></tr></table></figure><h5 id="create"><a href="#create" class="headerlink" title="create"></a>create</h5><p>将当前 DataFrame 创建为指定名称的表，表存在时将报错 </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; df.writeTo(&quot;in_mem_catalog.test_write_to__create&quot;).create()<br><br><span class="hljs-section">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__create&quot;).show </span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt;  df.writeTo(&quot;in<span class="hljs-emphasis">_mem_catalog.test_write_to__create&quot;).create()</span><br><span class="hljs-emphasis">org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `test_write_to__</span>create` because it already exists.<br></code></pre></td></tr></table></figure><h5 id="replace"><a href="#replace" class="headerlink" title="replace"></a>replace</h5><p>用当前 DataFrame 将指定名称的替换，表不存在时将报错  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; df.writeTo(&quot;in<span class="hljs-emphasis">_mem_catalog.test_write_to__replace&quot;).replace()</span><br><span class="hljs-emphasis">org.apache.spark.sql.catalyst.analysis.CannotReplaceMissingTableException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `test_write_to__</span>replace` cannot be found.<br><br>scala&gt; df.writeTo(&quot;in<span class="hljs-emphasis">_mem_catalog.test_write_to__replace&quot;).create()</span><br><span class="hljs-emphasis">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__</span>replace&quot;).show <br><span class="hljs-code">+---+</span>-----+<br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt; df.select(&quot;name&quot;, &quot;id&quot;).writeTo(&quot;in<span class="hljs-emphasis">_mem_catalog.test_write_to__replace&quot;).replace()</span><br><span class="hljs-emphasis">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__</span>replace&quot;).show<br><span class="hljs-code">+-----+</span>---+<br><span class="hljs-section">| name| id|</span><br><span class="hljs-section">+-----+---+</span><br>|Name1|  1|<br>|Name2|  2|<br><span class="hljs-section">|Name3|  3|</span><br><span class="hljs-section">+-----+---+</span><br></code></pre></td></tr></table></figure><h5 id="createOrReplace"><a href="#createOrReplace" class="headerlink" title="createOrReplace"></a>createOrReplace</h5><p><a href="#create">create</a> 和 <a href="#replace">replace</a> 的幂等操作，表不存在时创建，表存在时则替换   </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; df.writeTo(&quot;in_mem_catalog.test_write_to__create_or_replace&quot;).createOrReplace()<br><br><span class="hljs-section">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__create_or_replace&quot;).show </span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt; df.select(&quot;name&quot;, &quot;id&quot;).writeTo(&quot;in<span class="hljs-emphasis">_mem_catalog.test_write_to__create_or_replace&quot;).createOrReplace()</span><br><span class="hljs-emphasis">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__create_or_</span>replace&quot;).show<br><span class="hljs-code">+-----+</span>---+<br><span class="hljs-section">| name| id|</span><br><span class="hljs-section">+-----+---+</span><br>|Name1|  1|<br>|Name2|  2|<br><span class="hljs-section">|Name3|  3|</span><br><span class="hljs-section">+-----+---+</span><br></code></pre></td></tr></table></figure><h5 id="append"><a href="#append" class="headerlink" title="append"></a>append</h5><p>将 DataFrame 中的记录追加写入到目标表中，写入操作是基于列名的，因此不会对 DataFrame 的列顺序有严格要求，只要列存在即可  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; df.writeTo(&quot;in_mem_catalog.test_write_to__append&quot;).createOrReplace()<br><br><span class="hljs-section">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__append&quot;).show </span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt; df.select(&quot;name&quot;, &quot;id&quot;).writeTo(&quot;in<span class="hljs-emphasis">_mem_catalog.test_write_to__append&quot;).append()</span><br><span class="hljs-emphasis">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__</span>append&quot;).show<br><span class="hljs-code">+---+</span>-----+<br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br>|  3|Name3|<br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br></code></pre></td></tr></table></figure><h5 id="overwrite"><a href="#overwrite" class="headerlink" title="overwrite"></a>overwrite</h5><p>用当前 DataFrame 中的记录将目标表按条件覆盖，匹配的行将被覆盖，其余行将会追加到目标表中<br>写入操作是基于列名的，因此不会对 DataFrame 的列顺序有严格要求，只要列存在即可  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; spark.sql(&quot;CREATE TABLE in<span class="hljs-emphasis">_mem_catalog.test_write_to__overwrite (id int, name string) USING foo PARTITIONED BY (id)&quot;)</span><br><span class="hljs-emphasis">scala&gt; df.writeTo(&quot;in_mem_catalog.test_write_to__</span>overwrite&quot;).append<br><br><span class="hljs-section">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__overwrite&quot;).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt; val df2 = spark.createDataFrame(List(Person(3, &quot;Name333&quot;), Person(4, &quot;Name444&quot;)))<br>scala&gt; df2.writeTo(&quot;in_mem_catalog.test_write_to__overwrite&quot;).overwrite($&quot;id&quot; === 3)<br><br><span class="hljs-section">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__overwrite&quot;).show</span><br><span class="hljs-section">+---+-------+</span><br><span class="hljs-section">| id|   name|</span><br><span class="hljs-section">+---+-------+</span><br>|  4|Name444|<br>|  1|  Name1|<br>|  3|Name333|<br><span class="hljs-section">|  2|  Name2|</span><br><span class="hljs-section">+---+-------+</span><br><br>scala&gt; df2.writeTo(&quot;in_mem_catalog.test_write_to__overwrite&quot;).overwrite(org.apache.spark.sql.functions.lit(true))<br><br><span class="hljs-section">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__overwrite&quot;).show</span><br><span class="hljs-section">+---+-------+</span><br><span class="hljs-section">| id|   name|</span><br><span class="hljs-section">+---+-------+</span><br>|  4|Name444|<br><span class="hljs-section">|  3|Name333|</span><br><span class="hljs-section">+---+-------+</span><br></code></pre></td></tr></table></figure><h5 id="overwritePartitions"><a href="#overwritePartitions" class="headerlink" title="overwritePartitions"></a>overwritePartitions</h5><p>用当前 DataFrame 对目标表进行分区覆盖操作，匹配的分区将被覆盖  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 1, 1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; spark.sql(&quot;CREATE TABLE in<span class="hljs-emphasis">_mem_catalog.test_write_to__overwrite_partitions (id int, name string) USING foo PARTITIONED BY (id)&quot;)</span><br><span class="hljs-emphasis">scala&gt; df.writeTo(&quot;in_mem_catalog.test_write_to__overwrite_</span>partitions&quot;).append<br><br><span class="hljs-section">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__overwrite_partitions&quot;).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt; val df2 = spark.createDataFrame(List(Person(1, &quot;Name111&quot;), Person(4, &quot;Name444&quot;)))<br>scala&gt; df2.writeTo(&quot;in_mem_catalog.test_write_to__overwrite_partitions&quot;).overwritePartitions<br><br><span class="hljs-section">scala&gt; spark.table(&quot;in_mem_catalog.test_write_to__overwrite_partitions&quot;).show</span><br><span class="hljs-section">+---+-------+</span><br><span class="hljs-section">| id|   name|</span><br><span class="hljs-section">+---+-------+</span><br>|  4|Name444|<br>|  1|Name111|<br>|  3|  Name3|<br><span class="hljs-section">|  2|  Name2|</span><br><span class="hljs-section">+---+-------+</span><br></code></pre></td></tr></table></figure><h4 id="mergeInto-spark-4-0"><a href="#mergeInto-spark-4-0" class="headerlink" title="mergeInto spark 4.0 +"></a>mergeInto spark 4.0 +</h4><p>DataFrame.mergeInto 方法将按照指定的条件与指定的目标进行关联，将返回一个 MergeIntoWriter 实例，包含以下方法<br>todo 为本章节添加 Example  </p><h5 id="whenMatched"><a href="#whenMatched" class="headerlink" title="whenMatched"></a>whenMatched</h5><p>返回一个 WhenMatched 实例，可对目标表中与源 DataFrame 匹配的数据设置下列操作  </p><h6 id="updateAll"><a href="#updateAll" class="headerlink" title="updateAll"></a>updateAll</h6><h6 id="update"><a href="#update" class="headerlink" title="update"></a>update</h6><h6 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h6><h5 id="whenNotMatched"><a href="#whenNotMatched" class="headerlink" title="whenNotMatched"></a>whenNotMatched</h5><p>返回一个 whenNotMatched 实例，可对源 DataFrame 中与目标表不匹配的数据设置下列操作  </p><h6 id="insertAll"><a href="#insertAll" class="headerlink" title="insertAll"></a>insertAll</h6><h6 id="insert"><a href="#insert" class="headerlink" title="insert"></a>insert</h6><h5 id="whenNotMatchedBySource"><a href="#whenNotMatchedBySource" class="headerlink" title="whenNotMatchedBySource"></a>whenNotMatchedBySource</h5><p>返回一个 WhenNotMatchedBySource 实例，可对目标表中与源 DataFrame 不匹配的数据设置下列操作  </p><h6 id="updateAll-1"><a href="#updateAll-1" class="headerlink" title="updateAll"></a>updateAll</h6><h6 id="update-1"><a href="#update-1" class="headerlink" title="update"></a>update</h6><h6 id="delete-1"><a href="#delete-1" class="headerlink" title="delete"></a>delete</h6><h5 id="withSchemaEvolution"><a href="#withSchemaEvolution" class="headerlink" title="withSchemaEvolution"></a>withSchemaEvolution</h5><p>启用自动 Schema 演进  </p><h5 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h5><p>真正的 action 算子，根据上面的设置，执行最终的 merge 操作  </p><h2 id="控制算子"><a href="#控制算子" class="headerlink" title="控制算子"></a>控制算子</h2><p>DataFrame 提供与 RDD 行为一致的控制算子如:</p><ul><li><a href="spark-rdd.md#persist">persist</a>  </li><li><a href="spark-rdd.md#cache">cache</a>  </li><li><a href="spark-rdd.md#unpersist">unpersist</a>  </li><li><a href="spark-rdd.md#checkpoint">checkpoint</a></li></ul><p>除了这些以外，还提供了一些创建 View 的控制类算子  </p><h3 id="createTempView"><a href="#createTempView" class="headerlink" title="createTempView"></a>createTempView</h3><p>为当前 DataFrame 创建临时视图，可用 SQL 语句对其进行访问，在当前 SparkSession 内有效，视图存在时将报错</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; df.createTempView(&quot;temp_view_person&quot;)<br><br><span class="hljs-section">scala&gt; spark.sql(&quot;SELECT * FROM temp_view_person&quot;).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br>scala&gt; spark<br>res4: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5f59d707<br><br>scala&gt; val newSpark = spark.newSession<br>newSpark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1916f999<br><br>scala&gt; newSpark.sql(&quot;SELECT * FROM temp<span class="hljs-emphasis">_view_person&quot;).show</span><br><span class="hljs-emphasis">org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `temp_view_</span>person` cannot be found.<br></code></pre></td></tr></table></figure><h3 id="createOrReplaceTempView"><a href="#createOrReplaceTempView" class="headerlink" title="createOrReplaceTempView"></a>createOrReplaceTempView</h3><p>与 <a href="#createTempView-">createTempView</a> 类似，只是视图存在时将覆盖原有视图</p><h3 id="createGlobalTempView"><a href="#createGlobalTempView" class="headerlink" title="createGlobalTempView"></a>createGlobalTempView</h3><p>为当前 DataFrame 创建临时视图，可用 SQL 语句对其进行访问，在当前 Spark Application 内有效，视图存在时将报错</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc">scala&gt; case class Person(id: Int, name: String)<br>scala&gt; val df = spark.createDataFrame(List(1, 2, 3).map(x =&gt; Person(x, &quot;Name&quot; + x)))<br><br>scala&gt; df.createGlobalTempView(&quot;temp_view_person&quot;)<br><br><span class="hljs-section">scala&gt; spark.sql(&quot;SELECT * FROM global_temp.temp_view_person&quot;).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br><br>scala&gt; spark<br>res4: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5f59d707<br><br>scala&gt; val newSpark = spark.newSession<br>newSpark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1916f999<br><br><span class="hljs-section">scala&gt; newSpark.sql(&quot;SELECT * FROM global_temp.temp_view_person&quot;).show</span><br><span class="hljs-section">+---+-----+</span><br><span class="hljs-section">| id| name|</span><br><span class="hljs-section">+---+-----+</span><br>|  1|Name1|<br>|  2|Name2|<br><span class="hljs-section">|  3|Name3|</span><br><span class="hljs-section">+---+-----+</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark RDD - Action 算子</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-action/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-action/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark-RDD-Action-算子"><a href="#Spark-RDD-Action-算子" class="headerlink" title="Spark RDD - Action 算子"></a>Spark RDD - Action 算子</h1><h2 id="转换为内存集合"><a href="#转换为内存集合" class="headerlink" title="转换为内存集合"></a>转换为内存集合</h2><h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p>对 RDD 进行 reduce 操作, 所有记录按照用户指定的 (left, right) &#x3D;&gt; result 函数从左到右进行合并, 返回一个 Java 集合</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.reduce</span>(_ + _)<br>res0: Int = <span class="hljs-number">21</span><br></code></pre></td></tr></table></figure><h3 id="treeReduce"><a href="#treeReduce" class="headerlink" title="treeReduce"></a>treeReduce</h3><p>语义上与 <a href="#reduce">reduce</a> 等价 ，只是在合并结果时，treeAggregate 将以多级树的形式逐级聚合，降低 Driver 端的压力  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.treeReduce</span>(_ + _, depth=<span class="hljs-number">2</span>)<br>res0: Int = <span class="hljs-number">21</span><br></code></pre></td></tr></table></figure><h3 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h3><p>对 RDD 进行合并操作, 所有记录按照用户指定的 (left, right) &#x3D;&gt; result 函数从左到右进行合并, 返回一个 Java 集合<br>与 <a href="#reduce">reduce</a> 的功能很相似，不同的是 fold 允许用户提供一个作用于每个分区的初始值</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.fold</span>(<span class="hljs-number">0</span>)(_ + _)<br>res0: Int = <span class="hljs-number">21</span><br></code></pre></td></tr></table></figure><h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p>对 RDD 进行聚合操作，与 <a href="#aggregateByKey">aggregateByKey</a> 算子类似，只是面向 RDD 的所有记录，而非以 Key 为单位。<br>操作分两个阶段，先对单个分区内的数据聚合，再对所有分区聚合结果进行聚合，从而得到最终的聚合结果,<br>它允许返回一个与 RDD 记录类型 V 不同的类型 U, 比如将元素(Int) group 成一个 List<br>因此需要指定一个初始值，和两个聚合函数  </p><ul><li>zeroValue: U, 作用在每个分区的初始值  </li><li>seqOp: (U, V) &#x3D;&gt; U, 作用在每个分区内数据的聚合函数  </li><li>combOp: (U, U) &#x3D;&gt; U, 作用在每个分区聚合结果上的聚合函数</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.aggregate</span>(<span class="hljs-number">0</span>)(_+_, _+_)<br>res0: Int = <span class="hljs-number">21</span><br></code></pre></td></tr></table></figure><h3 id="treeAggregate"><a href="#treeAggregate" class="headerlink" title="treeAggregate"></a>treeAggregate</h3><p>语义上与 <a href="#aggregate">aggregate</a> 等价，只是在合并分区结果时，treeAggregate 将以多级树的形式逐级聚合，降低 Driver 端的压力   </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.treeAggregate</span>(<span class="hljs-number">0</span>)(_+_, _+_, depth=<span class="hljs-number">2</span>)<br>res0: Int = <span class="hljs-number">21</span><br></code></pre></td></tr></table></figure><h3 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h3><p>将 RDD 的所有记录收集起来，返回一个 Java 的 Array 集合到 Driver 端  </p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; rdd.treeAggregate(<span class="hljs-number">0</span>)(_+_, _+_, depth=<span class="hljs-number">2</span>)<br><span class="hljs-attribute">res0</span>: Array[Int] = Array(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>)<br></code></pre></td></tr></table></figure><h3 id="collectAsMap"><a href="#collectAsMap" class="headerlink" title="collectAsMap"></a>collectAsMap</h3><p>将 K-V Pair 型 RDD 的所有记录起来，返回一个 Java 的 Map 集合到 Driver 端<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-string">&quot;A&quot;</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-string">&quot;B&quot;</span> -&gt; <span class="hljs-number">2</span>))<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.collectAsMap</span><br>res0: scala<span class="hljs-selector-class">.collection</span><span class="hljs-selector-class">.Map</span><span class="hljs-selector-attr">[String,Int]</span> = <span class="hljs-built_in">Map</span>(A -&gt; <span class="hljs-number">1</span>, B -&gt; <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>返回 RDD 的总记录数</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.count</span><br>res0: Long = <span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>返回 K-V Pair 型 RDD 的每个 Key 的记录数<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-string">&quot;A&quot;</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-string">&quot;A&quot;</span> -&gt; <span class="hljs-number">11</span>, <span class="hljs-string">&quot;B&quot;</span> -&gt; <span class="hljs-number">2</span>))<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.countByKey</span><br>res0: scala<span class="hljs-selector-class">.collection</span><span class="hljs-selector-class">.Map</span><span class="hljs-selector-attr">[String,Long]</span> = <span class="hljs-built_in">Map</span>(B -&gt; <span class="hljs-number">1</span>, A -&gt; <span class="hljs-number">2</span>)<br><br></code></pre></td></tr></table></figure><h3 id="countApprox"><a href="#countApprox" class="headerlink" title="countApprox"></a>countApprox</h3><p>返回 RDD 总记录数的近似值, 在给定的 timeout 时间内返回有 confidence 概率准确的记录数的区间  </p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs vbnet">scala&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> <span class="hljs-keyword">to</span> <span class="hljs-number">10000</span> * <span class="hljs-number">10000</span>, <span class="hljs-number">1000</span>)<br><span class="hljs-symbol">rdd:</span> org.apache.spark.rdd.RDD[(<span class="hljs-type">String</span>, Int)] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd.countApprox(timeout=<span class="hljs-number">500</span>, confidence=<span class="hljs-number">0.95</span>)<br><span class="hljs-symbol">res0:</span> org.apache.spark.<span class="hljs-keyword">partial</span>.PartialResult[org.apache.spark.<span class="hljs-keyword">partial</span>.BoundedDouble] = (<span class="hljs-keyword">partial</span>: [<span class="hljs-number">99984958.000</span>, <span class="hljs-number">100015042.000</span>])<br><br>scala&gt; rdd.countApprox(timeout=<span class="hljs-number">500</span>, confidence=<span class="hljs-number">0.0</span>)<br><span class="hljs-symbol">res126:</span> org.apache.spark.<span class="hljs-keyword">partial</span>.PartialResult[org.apache.spark.<span class="hljs-keyword">partial</span>.BoundedDouble] = (<span class="hljs-keyword">partial</span>: [<span class="hljs-number">100000000.000</span>, <span class="hljs-number">100000000.000</span>]<br></code></pre></td></tr></table></figure><h3 id="countByKeyApprox"><a href="#countByKeyApprox" class="headerlink" title="countByKeyApprox"></a>countByKeyApprox</h3><p>返回 K-V Pair 型 RDD 的每个 Key 的记录数的近似值, 在给定的 timeout 时间内返回各个 Key 有 confidence 概率准确的记录数的区间<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs livescript">scala&gt; val rdd = sc.parallelize(List(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>), <span class="hljs-number">1</span>) cartesian sc.parallelize(<span class="hljs-number">1</span> <span class="hljs-keyword">to</span> <span class="hljs-number">10000</span> * <span class="hljs-number">10000</span>, <span class="hljs-number">100</span>)<br>rdd: org.apache.spark.rdd.RDD[(<span class="hljs-built_in">String</span>, Int)] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd.countByKeyApprox(timeout=<span class="hljs-number">5000</span>, confidence=<span class="hljs-number">0.95</span>)<br>res0: org.apache.spark.partial.PartialResult[scala.collection.<span class="hljs-built_in">Map</span>[<span class="hljs-built_in">String</span>,org.apache.spark.partial.BoundedDouble]] = (partial: <span class="hljs-built_in">Map</span>(B -&gt; [<span class="hljs-number">99990807.000</span>, <span class="hljs-number">100009194.000</span>], A -&gt; [<span class="hljs-number">99990807.000</span>, <span class="hljs-number">100009194.000</span>], C -&gt; [<span class="hljs-number">99990807.000</span>, <span class="hljs-number">100009194.000</span>]))<br><br>scala&gt; rdd.countByKeyApprox(timeout=<span class="hljs-number">5000</span>, confidence=<span class="hljs-number">0.0</span>)<br>res1: org.apache.spark.partial.PartialResult[scala.collection.<span class="hljs-built_in">Map</span>[<span class="hljs-built_in">String</span>,org.apache.spark.partial.BoundedDouble]] = (partial: <span class="hljs-built_in">Map</span>(B -&gt; [<span class="hljs-number">100000000.000</span>, <span class="hljs-number">100000000.000</span>], A -&gt; [<span class="hljs-number">100000000.000</span>, <span class="hljs-number">100000000.000</span>], C -&gt; [<span class="hljs-number">100000000.000</span>, <span class="hljs-number">100000000.000</span>]))<br></code></pre></td></tr></table></figure><h3 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue"></a>countByValue</h3><p>返回 RDD 中每条记录出现的次数  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>))<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">25</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.countByValue</span><br>res0: scala<span class="hljs-selector-class">.collection</span><span class="hljs-selector-class">.Map</span><span class="hljs-selector-attr">[String,Long]</span> = <span class="hljs-built_in">Map</span>(B -&gt; <span class="hljs-number">2</span>, C -&gt; <span class="hljs-number">1</span>, A -&gt; <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h3 id="countByValueApprox"><a href="#countByValueApprox" class="headerlink" title="countByValueApprox"></a>countByValueApprox</h3><p>返回 RDD 中每条记录出现的次数的近似值, 在给定的 timeout 时间内返回各个 Key 有 confidence 概率准确的记录数的区间</p><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs livescript">scala&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> <span class="hljs-keyword">to</span> <span class="hljs-number">3000</span> * <span class="hljs-number">10000</span>, <span class="hljs-number">100</span>).<span class="hljs-keyword">map</span>(_ % <span class="hljs-number">3</span>)<br>rdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[<span class="hljs-number">281</span>] at <span class="hljs-keyword">map</span> at &lt;console&gt;:<span class="hljs-number">25</span> <br><br>scala&gt; rdd.countByValueApprox(<span class="hljs-number">500</span>, <span class="hljs-number">0.95</span>)<br>res0: org.apache.spark.partial.PartialResult[scala.collection.<span class="hljs-built_in">Map</span>[Int,org.apache.spark.partial.BoundedDouble]] = (partial: <span class="hljs-built_in">Map</span>(<span class="hljs-number">0</span> -&gt; [<span class="hljs-number">9996494.000</span>, <span class="hljs-number">10003507.000</span>], <span class="hljs-number">1</span> -&gt; [<span class="hljs-number">9996494.000</span>, <span class="hljs-number">10003507.000</span>], <span class="hljs-number">2</span> -&gt; [<span class="hljs-number">9996494.000</span>, <span class="hljs-number">10003507.000</span>]))<br><br>scala&gt; rdd.countByValueApprox(<span class="hljs-number">500</span>, <span class="hljs-number">0.0</span>)<br>res1: org.apache.spark.partial.PartialResult[scala.collection.<span class="hljs-built_in">Map</span>[Int,org.apache.spark.partial.BoundedDouble]] = (partial: <span class="hljs-built_in">Map</span>(<span class="hljs-number">0</span> -&gt; [<span class="hljs-number">10000000.000</span>, <span class="hljs-number">10000000.000</span>], <span class="hljs-number">1</span> -&gt; [<span class="hljs-number">10000000.000</span>, <span class="hljs-number">10000000.000</span>], <span class="hljs-number">2</span> -&gt; [<span class="hljs-number">10000000.000</span>, <span class="hljs-number">10000000.000</span>]))<br></code></pre></td></tr></table></figure><h3 id="countApproxDistinct"><a href="#countApproxDistinct" class="headerlink" title="countApproxDistinct"></a>countApproxDistinct</h3><p>返回 RDD 去重后的记录数的近似值, 使用的算法基于 streamlib 实现的<a href="https://dl.acm.org/doi/10.1145/2452376.2452456">《HyperLogLog in practice: algorithmic engineering of a state of the art cardinality estimation algorithm》</a><br>p：正常集合的精度值<br>sp：稀疏集合的精度值  </p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">10000</span> * <span class="hljs-number">10000</span>, <span class="hljs-number">100</span>).map(_ % <span class="hljs-number">1000</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; rdd.distinct.count<br><span class="hljs-attribute">res0</span>: Long = <span class="hljs-number">3</span><br><br><span class="hljs-attribute">scala</span>&gt; rdd.countApproxDistinct(<span class="hljs-number">4</span>, <span class="hljs-number">0</span>)<br><span class="hljs-attribute">res1</span>: Long = <span class="hljs-number">1213</span><br><br></code></pre></td></tr></table></figure><h3 id="max"><a href="#max" class="headerlink" title="max"></a>max</h3><p>返回 RDD 中记录的最大值  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.max</span><br>res0: Int = <span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><h3 id="min"><a href="#min" class="headerlink" title="min"></a>min</h3><p>返回 RDD 中记录的最大值</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.min</span><br>res0: Int = <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h3 id="isEmpty"><a href="#isEmpty" class="headerlink" title="isEmpty"></a>isEmpty</h3><p>判断 RDD 是否为空, 返回 true 或 false  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.isEmpty</span><br>res0: Boolean = false<br></code></pre></td></tr></table></figure><h3 id="first"><a href="#first" class="headerlink" title="first"></a>first</h3><p>返回 RDD 的第一条记录  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.first</span><br>res0: Int = <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><p>返回 RDD 中的 n 条记录, 默认先从一个 partition 中获取，不足 n 时再从其他 partition 中获取, 直到满足 n 为止</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.take</span>(<span class="hljs-number">3</span>)<br>res0: Array<span class="hljs-selector-attr">[Int]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><h3 id="top"><a href="#top" class="headerlink" title="top"></a>top</h3><p>返回 RDD 降序排序之后的前 n 条记录，与 <a href="#takeordered">takeOrdered</a> 相反</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.top</span>(<span class="hljs-number">3</span>)<br>res0: Array<span class="hljs-selector-attr">[Int]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure><h3 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h3><p>返回 RDD 升序排序之后的前 n 条记录, 与 <a href="#top">top</a> 相反</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.takeOrdered</span>(<span class="hljs-number">3</span>)<br>res0: Array<span class="hljs-selector-attr">[Int]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><h3 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h3><p>返回 RDD 中随机的 n 条记录</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt;  rdd<span class="hljs-selector-class">.takeSample</span>(withReplacement=false, <span class="hljs-number">3</span>)<br>res0: Array<span class="hljs-selector-attr">[Int]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">6</span>)<br></code></pre></td></tr></table></figure><h3 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h3><p>遍历 RDD 中的每一条记录，根据提供的 record &#x3D;&gt; Unit 函数，将记录写入外部系统，或打印到控制台，或添加到其他 Java 集合中等</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.foreach</span>(<span class="hljs-attribute">x</span> =&gt; <span class="hljs-built_in">print</span>(x + <span class="hljs-string">&quot;,&quot;</span>))<br><span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">6</span>,<span class="hljs-number">5</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<br></code></pre></td></tr></table></figure><h2 id="写入外部系统"><a href="#写入外部系统" class="headerlink" title="写入外部系统"></a>写入外部系统</h2><h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><p>将 RDD 以 TEXT 文件格式写入 Hadoop 支持的外部文件系统</p><h3 id="saveAsSequenceFile"><a href="#saveAsSequenceFile" class="headerlink" title="saveAsSequenceFile"></a>saveAsSequenceFile</h3><p>将 RDD 以 Hadoop SequenceFile 文件格式写入 Hadoop 支持的外部文件系统</p><h3 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h3><p>将 RDD 序列化之后， 以 Hadoop SequenceFile 文件格式写入 Hadoop 支持的外部文件系统</p><h3 id="saveAsHadoopFile"><a href="#saveAsHadoopFile" class="headerlink" title="saveAsHadoopFile"></a>saveAsHadoopFile</h3><p>等效于 <a href="#saveAsTextFile">saveAsTextFile</a></p><h3 id="saveAsNewAPIHadoopFile"><a href="#saveAsNewAPIHadoopFile" class="headerlink" title="saveAsNewAPIHadoopFile"></a>saveAsNewAPIHadoopFile</h3><p>将 K-V Pair 型 RDD，以指定的文件格式写入 Hadoop 支持的外部文件系统</p><h3 id="saveAsHadoopDataset"><a href="#saveAsHadoopDataset" class="headerlink" title="saveAsHadoopDataset"></a>saveAsHadoopDataset</h3><p>将 K-V Pair 型 RDD，以指定的文件格式写入 Hadoop 支持的外部存储系统中(如 HDFS、HBase等)</p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark RDD - 控制算子</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-control/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-control/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark-RDD-Control-算子"><a href="#Spark-RDD-Control-算子" class="headerlink" title="Spark RDD - Control 算子"></a>Spark RDD - Control 算子</h1><h2 id="persist"><a href="#persist" class="headerlink" title="persist"></a>persist</h2><p>将 RDD 缓存到内存或者磁盘中，通过读取缓存中的数据避免重复读取外部系统，从而提升作业性能</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rddFromLocalFS = sc<span class="hljs-selector-class">.textFile</span>(<span class="hljs-string">&quot;file:///root/software/spark-3.5.1-bin-hadoop3/README.md&quot;</span>)<br>rddFromLocalFS: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = file:<span class="hljs-comment">///root/software/spark-3.5.1-bin-hadoop3/README.md MapPartitionsRDD[0] at textFile at &lt;console&gt;:25</span><br><br>scala&gt; rddFromLocalFS<span class="hljs-selector-class">.persist</span>(org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.storage</span><span class="hljs-selector-class">.StorageLevel</span>.MEMORY_AND_DISK)<br>res0: rddFromLocalFS<span class="hljs-selector-class">.type</span> = file:<span class="hljs-comment">///root/software/spark-3.5.1-bin-hadoop3/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:25</span><br><br>scala&gt; rddFromLocalFS<span class="hljs-selector-class">.count</span><br>res1: Long = <span class="hljs-number">125</span><br></code></pre></td></tr></table></figure><h2 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h2><p>等同于缓存级别为 <code>MEMORY_ONLY</code> 的 <a href="#persist">persist</a>  </p><h2 id="unpersist"><a href="#unpersist" class="headerlink" title="unpersist"></a>unpersist</h2><p>将已经缓存的 RDD 解除缓存，以释放当前作业的内存或磁盘空间  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rddFromLocalFS = sc<span class="hljs-selector-class">.textFile</span>(<span class="hljs-string">&quot;file:///root/software/spark-3.5.1-bin-hadoop3/README.md&quot;</span>)<br>rddFromLocalFS: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = file:<span class="hljs-comment">///root/software/spark-3.5.1-bin-hadoop3/README.md MapPartitionsRDD[0] at textFile at &lt;console&gt;:25</span><br><br>scala&gt; rddFromLocalFS<span class="hljs-selector-class">.persist</span>(org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.storage</span><span class="hljs-selector-class">.StorageLevel</span>.MEMORY_AND_DISK)<br>res0: rddFromLocalFS<span class="hljs-selector-class">.type</span> = file:<span class="hljs-comment">///root/software/spark-3.5.1-bin-hadoop3/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:25</span><br><br>scala&gt; rddFromLocalFS<span class="hljs-selector-class">.unpersist</span>()<br></code></pre></td></tr></table></figure><h2 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h2><p>将当前 RDD 保存在设置的 checkpointDir 中，并移除对父 RDD 的血缘关系，当发生故障时，无需再从父 RDD 重新计算。 </p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">scala</span>&gt; val rddFromLocalFS = <span class="hljs-keyword">sc</span>.textFile(<span class="hljs-string">&quot;file:///root/software/spark-3.5.1-bin-hadoop3/README.md&quot;</span>)<br>rddFromLocalFS: org.apache.spark.rdd.RDD[String] = <span class="hljs-keyword">file</span>:<span class="hljs-comment">///root/software/spark-3.5.1-bin-hadoop3/README.md MapPartitionsRDD[0] at textFile at &lt;console&gt;:25</span><br><br><span class="hljs-keyword">scala</span>&gt; rddFromLocalFS.cache()<br>res0: rddFromLocalFS.<span class="hljs-keyword">type</span> = <span class="hljs-keyword">file</span>:<span class="hljs-comment">///root/software/spark-3.5.1-bin-hadoop3/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:25</span><br><br><span class="hljs-keyword">scala</span>&gt; <span class="hljs-keyword">sc</span>.setCheckpointDir(<span class="hljs-string">&quot;/tmp/spark/app-checkpoint/&quot;</span>)<br><br><span class="hljs-keyword">scala</span>&gt; rddFromLocalFS.checkpoint<br><br><span class="hljs-keyword">scala</span>&gt; rddFromLocalFS.<span class="hljs-keyword">count</span><br>res1: Long = 125<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark 创建 RDD</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-creation/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-creation/</url>
    
    <content type="html"><![CDATA[<h1 id="创建-RDD"><a href="#创建-RDD" class="headerlink" title="创建 RDD"></a>创建 RDD</h1><p>创建RDD的方式主要有两种：通过并行化现有的集合创建 RDD ；或者通过读取外部系统如 HDFS 等创建 RDD。<br>本篇文章的后续实践可在 spark-shell 中完成, 其中会默认实例化一个 SparkContext 实例 <code>sc</code> 和 SparkSession 实例 <code>spark</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">spark-shell<br></code></pre></td></tr></table></figure><h2 id="并行化现有集合"><a href="#并行化现有集合" class="headerlink" title="并行化现有集合"></a>并行化现有集合</h2><p>根据内存中的集合来生成RDD</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val scalaList = <span class="hljs-built_in">List</span>(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>, <span class="hljs-string">&quot;E&quot;</span>, <span class="hljs-string">&quot;F&quot;</span>)<br>scalaList: List<span class="hljs-selector-attr">[String]</span> = <span class="hljs-built_in">List</span>(A, B, C, D, E, F)<br><br>scala&gt; val sparkRDD =  sc<span class="hljs-selector-class">.parallelize</span>(scalaList, <span class="hljs-number">3</span>)<br>sparkRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; sparkRDD<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br>res0: Int = <span class="hljs-number">3</span><br></code></pre></td></tr></table></figure><p>并行化现有集合相关其他算子  </p><ul><li>range</li><li>makeRDD  </li><li>emptyRDD: no partitions or elements.</li></ul><h2 id="读取外部系统数据"><a href="#读取外部系统数据" class="headerlink" title="读取外部系统数据"></a>读取外部系统数据</h2><p>从外部系统重读取数据来生成RDD</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rddFromLocalFS = sc<span class="hljs-selector-class">.textFile</span>(<span class="hljs-string">&quot;file:///root/software/spark-3.5.1-bin-hadoop3/README.md&quot;</span>)<br>rddFromLocalFS: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = file:<span class="hljs-comment">///root/software/spark-3.5.1-bin-hadoop3/README.txt MapPartitionsRDD[0] at textFile at &lt;console&gt;:23</span><br><br>scala&gt; val rddFromHDFS = sc<span class="hljs-selector-class">.textFile</span>(<span class="hljs-string">&quot;hdfs:///README.txt&quot;</span>)<br>rddFromHDFS: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = hdfs:<span class="hljs-comment">///README.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:23</span><br><br>scala&gt; val rddFromHDFS = sc<span class="hljs-selector-class">.textFile</span>(<span class="hljs-string">&quot;/README.txt&quot;</span>)<br>rddFromHDFS: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = /README<span class="hljs-selector-class">.txt</span> MapPartitionsRDD<span class="hljs-selector-attr">[2]</span> at textFile at &lt;console&gt;:<span class="hljs-number">23</span><br></code></pre></td></tr></table></figure><p>读取外部系统相关其他算子</p><ul><li>wholeTextFiles</li><li>binaryFiles</li><li>hadoopRDD</li><li>hadoopFile</li><li>newAPIHadoopFile</li><li>sequenceFile</li><li>objectFile</li></ul>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark RDD Overview</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-overview/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-overview/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark-RDD"><a href="#Spark-RDD" class="headerlink" title="Spark RDD"></a>Spark RDD</h1><h2 id="RDD-简介"><a href="#RDD-简介" class="headerlink" title="RDD 简介"></a>RDD 简介</h2><p>RDD(Resilient Distributed Dataset) - 弹性分布式数据集，是 Spark 用来并行操作跨节点数据的主要抽象。</p><h3 id="RDD的五大特性"><a href="#RDD的五大特性" class="headerlink" title="RDD的五大特性"></a>RDD的五大特性</h3><ol><li>一组分区：每个RDD拥有一组partitions, 每个partition将由一个task来处理；</li><li>数据血缘：每个RDD记录了其依赖关系，可向上追溯父RDD，发生错误时，可从父RDD开始重新计算；</li><li>转换函数：每个RDD拥有其转换函数，记录了其是怎样通过父RDD转换而来的；</li><li>分区器：每个RDD拥有一个Partitioner, 记录了其重新分区的规则；</li><li>数据本地性：移动计算优于移动数据，RDD会尽可能让计算task发生在离数据更近的地方。</li></ol><h3 id="Shuffle-操作"><a href="#Shuffle-操作" class="headerlink" title="Shuffle 操作"></a>Shuffle 操作</h3><p>Shuffle 是 Spark 进行数据交换或者说重新分配数据的一种操作，这通常会产生跨 Executor 、跨节点，甚至跨机房、跨地区的数据拷贝，因此 Shuffle 操作的成本一来说都比较高。<br>只有宽依赖(Wide-Dependency)算子会产生Shuffle, 窄依赖(Narrow-Dependency)算子不会产生Shuffle。    </p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240808/image.5c0w5b3q3u.webp" alt="image">   </p><h4 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h4><p>父RDD的每一个分区，最多只会被子RDD的一个分区所依赖，这意味着在计算过程中，当前partition中的数据，不需要与其他partitions中的数据进行交互，即可完成计算。<br>因为不涉及Shuffle，这类算的的计算速度一般都很快；也以为其一对一的特点，多个相邻的窄依赖算子可以被Chain起来，放在一个Stage中，形成一个优化的流水线。<br>常见的窄依赖算子有 map, filter, union 等。</p><h4 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h4><p>父RDD的每一个分区，会被自RDD的多个分区所依赖, 这意味着当前partition中的数据，会按需(partitioner)被重新分配到子RDD的不同各个partition中，从而产生大规模的数据交换动作。<br>因为产生了数据交换，当前的数据流水线(Stage)也将被截断，在数据重新分配之后，开始一个新的数据流水线(Stage)，故而每遇到一个Shuffle算子，都会产生一个新的Stage。<br>常见的宽依赖算子有 groupByKey, repartition 等。</p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark RDD 共享变量</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-shared-variable/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-shared-variable/</url>
    
    <content type="html"><![CDATA[<h1 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h1><h2 id="Accumulators"><a href="#Accumulators" class="headerlink" title="Accumulators"></a>Accumulators</h2><p>由于 Spark 计算时会将程序及变量序列化之后发送到各个 partition ，因此在各个 task 中执行计算时，只会操作变量的副本，并不会更新 Driver 端的原变量<br>如下面的例子中，看似更新了 sum 的值，实际上 Driver 端的 sum 仍然为 0</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">scala</span>&gt; <span class="hljs-keyword">var</span> <span class="hljs-keyword">sum</span> = 0<br><span class="hljs-keyword">sum</span>: Int = 0<br><br><span class="hljs-keyword">scala</span>&gt; val rdd = <span class="hljs-keyword">sc</span>.parallelize(1 to 6)<br>rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:23<br><br><span class="hljs-keyword">scala</span>&gt; rdd.<span class="hljs-keyword">foreach</span>(x =&gt; <span class="hljs-keyword">sum</span> = <span class="hljs-keyword">sum</span> + x)<br><br><span class="hljs-keyword">scala</span>&gt; <span class="hljs-keyword">sum</span><br>res1: Int = 0<br></code></pre></td></tr></table></figure><p>大多数场景中可以用 Spark 丰富的算子完成类似的需求。<br>但为了应对不同的业务场景，Spark 仍然可更新的共享变量: 累加器(Accumulators)</p><p>累加器是仅支持添加的变量，其引用会被分发到每个 partition 中，因此能够支持在多个 task 中并行计算</p><h3 id="longAccumulator"><a href="#longAccumulator" class="headerlink" title="longAccumulator"></a>longAccumulator</h3><p>long 类型累加器，提供 <code>add</code> 方法用来在 Executor 中并行添加元素，并提供 <code>value</code>, <code>sum</code>, <code>count</code>, <code>avg</code> 方法来在 Driver 获取结果<br>以及提供 <code>merge</code> 方法来合并其他累加器中的值</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val longAccum = sc<span class="hljs-selector-class">.longAccumulator</span>(<span class="hljs-string">&quot;long accumulator&quot;</span>)<br>longAccum: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.LongAccumulator</span> = <span class="hljs-built_in">LongAccumulator</span>(id: <span class="hljs-number">0</span>, name: <span class="hljs-built_in">Some</span>(long accumulator), value: <span class="hljs-number">0</span>)<br><br>scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.foreach</span>(longAccum<span class="hljs-selector-class">.add</span>(_))<br><br>scala&gt; longAccum<span class="hljs-selector-class">.value</span><br>res1: Long = <span class="hljs-number">21</span><br><br>scala&gt; longAccum<span class="hljs-selector-class">.sum</span><br>res2: Long = <span class="hljs-number">21</span><br><br>scala&gt; longAccum<span class="hljs-selector-class">.count</span><br>res3: Long = <span class="hljs-number">6</span><br><br>scala&gt; longAccum<span class="hljs-selector-class">.avg</span><br>res4: Double = <span class="hljs-number">3.5</span><br><br>scala&gt; longAccum<span class="hljs-selector-class">.merge</span>(longAccum)<br><br>scala&gt; longAccum<span class="hljs-selector-class">.value</span><br>res5: Long = <span class="hljs-number">42</span><br><br>scala&gt; longAccum<span class="hljs-selector-class">.sum</span><br>res6: Long = <span class="hljs-number">42</span><br><br>scala&gt; longAccum<span class="hljs-selector-class">.count</span><br>res7: Long = <span class="hljs-number">12</span><br><br>scala&gt; longAccum<span class="hljs-selector-class">.avg</span><br>res8: Double = <span class="hljs-number">3.5</span><br></code></pre></td></tr></table></figure><h3 id="doubleAccumulator"><a href="#doubleAccumulator" class="headerlink" title="doubleAccumulator"></a>doubleAccumulator</h3><p>double 类型累加器， 与 <a href="#longAccumulator">longAccumulator</a> 行为一致，只是类型不同  </p><h3 id="collectionAccumulator"><a href="#collectionAccumulator" class="headerlink" title="collectionAccumulator"></a>collectionAccumulator</h3><p>集合类型累加器，提供 <code>add</code> 方法用来在 Executor 中并行添加元素，并提供 <code>value</code> 方法来在 Driver 获取结果<br>以及提供 <code>merge</code> 方法来合并其他累加器中的值</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs prolog">scala&gt; val collectionAccum = sc.collectionAccumulator[<span class="hljs-symbol">Int</span>](<span class="hljs-string">&quot;collection accumulator&quot;</span>)<br>collectionAccum: org.apache.spark.util.<span class="hljs-symbol">CollectionAccumulator</span>[<span class="hljs-symbol">Int</span>] = <span class="hljs-symbol">CollectionAccumulator</span>(id: <span class="hljs-number">0</span>, name: <span class="hljs-symbol">Some</span>(collection accumulator), value: [])<br><br>scala&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org.apache.spark.rdd.<span class="hljs-symbol">RDD</span>[<span class="hljs-symbol">Int</span>] = <span class="hljs-symbol">ParallelCollectionRDD</span>[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt;  rdd.foreach(collectionAccum.add(<span class="hljs-symbol">_</span>))<br><br>scala&gt; collectionAccum.value<br>res0: java.util.<span class="hljs-symbol">List</span>[<span class="hljs-symbol">Int</span>] = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br><br>scala&gt; collectionAccum.merge(collectionAccum)<br><br>scala&gt; collectionAccum.value<br>res1: java.util.<span class="hljs-symbol">List</span>[<span class="hljs-symbol">Int</span>] = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br><span class="hljs-string">``</span><span class="hljs-string">``</span><span class="hljs-string">``</span><br><br>## <span class="hljs-symbol">Broadcast</span> <span class="hljs-symbol">Variables</span><br>由于 <span class="hljs-symbol">Spark</span> 计算时会将程序及变量序列化之后发送到各个 partition，当变量较大或者 task 数量较多的情况，拷贝和传输变量会占用很多 <span class="hljs-symbol">Driver</span> 端的带宽资源  <br>因此 <span class="hljs-symbol">Spark</span> 提供了广播变量(<span class="hljs-symbol">Broadcast</span> <span class="hljs-symbol">Variables</span>)，来将变量的只读副本分发到每台节点上面，该节点上的所有 task 可以直接从此只读副本中读取变量的值，从而降低计算时发生的网络传输成本  <br></code></pre></td></tr></table></figure><p>scala&gt; val broadcastMapping &#x3D; sc.broadcast(Map(“A” -&gt; 1, “B” -&gt; 2, “C” -&gt; 3 ))<br>broadcastVariable: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,Int]] &#x3D; Broadcast(0)</p><p>scala&gt; broadcastMapping.value<br>res0: scala.collection.immutable.Map[String,Int] &#x3D; Map(A -&gt; 1, B -&gt; 2, C -&gt; 3)</p><p>scala&gt; val rdd &#x3D; sc.parallelize(List(“A”, “B”, “C”, “D”))<br>rdd: org.apache.spark.rdd.RDD[String] &#x3D; ParallelCollectionRDD[0] at parallelize at <console>:23</p><p>scala&gt; val transformedRDD &#x3D; rdd.map(x &#x3D;&gt; (x, broadcastMapping.value.getOrElse(x, 0)))<br>transformedRDD: org.apache.spark.rdd.RDD[(String, Int)] &#x3D; MapPartitionsRDD[1] at map at <console>:24</p><p>scala&gt; transformedRDD.collect<br>res1: Array[(String, Int)] &#x3D; Array((A,1), (B,2), (C,3), (D,0))</p><pre><code class="hljs"></code></pre>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark RDD - Transformation 算子 - 聚合操作</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-transformation-aggregation-transform/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-transformation-aggregation-transform/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformation-算子-聚合操作"><a href="#Transformation-算子-聚合操作" class="headerlink" title="Transformation 算子 - 聚合操作"></a>Transformation 算子 - 聚合操作</h1><h2 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h2><p>对 RDD 按 key 进行分组, 具有相同 key 的记录会被 group 到一起, key 值由用户指定的 record &#x3D;&gt; key 函数决定</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>, <span class="hljs-number">2</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val groupedRDD = rdd<span class="hljs-selector-class">.groupBy</span> &#123; <span class="hljs-attribute">x</span> =&gt; if (x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) <span class="hljs-string">&quot;even&quot;</span> else <span class="hljs-string">&quot;odd&quot;</span> &#125;<br>transformedDF: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, Int)]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at keyBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; groupedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(String, Iterable[Int]</span>)] = <span class="hljs-built_in">Array</span>((even,<span class="hljs-built_in">CompactBuffer</span>(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>)), (odd,<span class="hljs-built_in">CompactBuffer</span>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><h2 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h2><p>对 K-V Pair 型 RDD 按 key 进行分组, 具有相同 key 的记录会被 group 到一起<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>, <span class="hljs-number">2</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val kvRDD = rdd<span class="hljs-selector-class">.keyBy</span> &#123; <span class="hljs-attribute">x</span> =&gt; if (x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) <span class="hljs-string">&quot;even&quot;</span> else <span class="hljs-string">&quot;odd&quot;</span> &#125;<br>kvRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at keyBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val groupedRDD = kvRDD<span class="hljs-selector-class">.groupByKey</span>()<br>groupedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Iterable[Int]</span>)] = ShuffledRDD<span class="hljs-selector-attr">[2]</span> at groupByKey at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; groupedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(String, Iterable[Int]</span>)] = <span class="hljs-built_in">Array</span>((even,<span class="hljs-built_in">CompactBuffer</span>(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>)), (odd,<span class="hljs-built_in">CompactBuffer</span>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><h2 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h2><p>对 K-V Pair 型 RDD 按 key 进行 reduce 操作, 具有相同 key 的记录将按照用户指定的 (left, right) &#x3D;&gt; result 函数从左到右进行合并<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>, <span class="hljs-number">2</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val kvRDD = rdd<span class="hljs-selector-class">.keyBy</span> &#123; <span class="hljs-attribute">x</span> =&gt; if (x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) <span class="hljs-string">&quot;even&quot;</span> else <span class="hljs-string">&quot;odd&quot;</span> &#125;<br>kvRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at keyBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val reducedRDD = kvRDD<span class="hljs-selector-class">.reduceByKey</span>(_ + _)<br>reducedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ShuffledRDD<span class="hljs-selector-attr">[2]</span> at reduceByKey at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; reducedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(String, Int)]</span> = <span class="hljs-built_in">Array</span>((even,<span class="hljs-number">12</span>), (odd,<span class="hljs-number">9</span>))<br></code></pre></td></tr></table></figure><h2 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h2><p>对 K-V Pair 型 RDD 按 key 进行合并, 具有相同 key 的记录将按照用户指定的 (left, right) &#x3D;&gt; result 函数从左到右进行合并<br>与 <a href="#reduceByKey">reduceByKey</a> 的功能很相似，不同的是 foldByKey 允许用户提供一个作用于每个分区的初始值<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>, <span class="hljs-number">2</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val kvRDD = rdd<span class="hljs-selector-class">.keyBy</span> &#123; <span class="hljs-attribute">x</span> =&gt; if (x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) <span class="hljs-string">&quot;even&quot;</span> else <span class="hljs-string">&quot;odd&quot;</span> &#125;<br>kvRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at keyBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val foldedRDD = kvRDD<span class="hljs-selector-class">.foldByKey</span>(<span class="hljs-number">0</span>)(_ + _)<br>reducedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ShuffledRDD<span class="hljs-selector-attr">[2]</span> at reduceByKey at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; foldedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(String, Int)]</span> = <span class="hljs-built_in">Array</span>((even,<span class="hljs-number">12</span>), (odd,<span class="hljs-number">9</span>))<br></code></pre></td></tr></table></figure><h2 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h2><p>对 K-V Pair 型 RDD 进行聚合操作，操作分两个阶段，先对单个分区内的数据按 Key 进行聚合，再对所有分区各个 Key 的聚合结果进行聚合，从而得到最终的聚合结果,<br>它允许返回一个与 RDD 记录类型 V 不同的类型 U, 比如将元素(Int) group 成一个 List<br>因此需要指定一个初始值，和两个聚合函数  </p><ul><li>zeroValue: U, 作用在每个分区的初始值  </li><li>seqOp: (U, V) &#x3D;&gt; U, 作用在每个分区内数据的聚合函数  </li><li>combOp: (U, U) &#x3D;&gt; U, 作用在每个分区聚合结果上的聚合函数<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>, <span class="hljs-number">2</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val kvRDD = rdd<span class="hljs-selector-class">.keyBy</span> &#123; <span class="hljs-attribute">x</span> =&gt; if (x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) <span class="hljs-string">&quot;even&quot;</span> else <span class="hljs-string">&quot;odd&quot;</span> &#125;<br>kvRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at keyBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val aggregatedRDD = kvRDD<span class="hljs-selector-class">.aggregateByKey</span>(List<span class="hljs-selector-attr">[Int]</span>())(_ :+ _, _ ++ _)<br>aggregatedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, List[Int]</span>)] = ShuffledRDD<span class="hljs-selector-attr">[2]</span> at aggregateByKey at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; aggregatedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(String, List[Int]</span>)] = <span class="hljs-built_in">Array</span>((even,<span class="hljs-built_in">List</span>(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>)), (odd,<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><h2 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h2><p>对 RDD 进行合并操作，与 <a href="#aggregateByKey">aggregateByKey</a> 类似，操作分两个阶段，先对单个分区内的数据聚合，再对所有分区聚合结果进行聚合，从而得到最终的聚合结果,<br>它允许返回一个与 RDD 记录类型 V 不同的类型 U, 比如将元素(Int) group 成一个 List<br>同样需要两个聚合函数，与 <a href="#aggregateByKey">aggregateByKey</a> 不同的是，combineByKey 不需要指定初始值，但是需要指定一个用来创建初始值的函数, 这个函数的入参将是每个分区的第一个元素  </p><ul><li>createCombiner: V &#x3D;&gt; U, 用于创建每个分区的初始值  </li><li>mergeValue: (U, V) &#x3D;&gt; U, 作用在每个分区内数据的聚合函数  </li><li>mergeCombiners: (U, U) &#x3D;&gt; U, 作用在每个分区聚合结果上的聚合函数<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>, <span class="hljs-number">2</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val kvRDD = rdd<span class="hljs-selector-class">.keyBy</span> &#123; <span class="hljs-attribute">x</span> =&gt; if (x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) <span class="hljs-string">&quot;even&quot;</span> else <span class="hljs-string">&quot;odd&quot;</span> &#125;<br>kvRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at keyBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val combinedRDD = kvRDD<span class="hljs-selector-class">.combineByKey</span><span class="hljs-selector-attr">[List[Int]</span>]((<span class="hljs-attribute">x</span>: Int) =&gt; List[Int](x), (l: List[Int], i: Int) =&gt; l :+ i,(l1: List[Int], l2: List[Int]) =&gt; l1 ++ l2)<br>combinedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, List[Int]</span>)] = ShuffledRDD<span class="hljs-selector-attr">[2]</span> at combineByKey at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; combinedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(String, List[Int]</span>)] = <span class="hljs-built_in">Array</span>((even,<span class="hljs-built_in">List</span>(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>)), (odd,<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><h2 id="countApproxDistinctByKey"><a href="#countApproxDistinctByKey" class="headerlink" title="countApproxDistinctByKey"></a>countApproxDistinctByKey</h2><p>返回 K-V Pair 型 RDD 的每个 Key 去重后的记录数的近似值, 使用的算法基于 streamlib 实现的<a href="https://dl.acm.org/doi/10.1145/2452376.2452456">《HyperLogLog in practice: algorithmic engineering of a state of the art cardinality estimation algorithm》</a><br>p：正常集合的精度值<br>sp：稀疏集合的精度值<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(List(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>), <span class="hljs-number">1</span>) cartesian sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">10000</span>, <span class="hljs-number">100</span>).map(_ % <span class="hljs-number">1000</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; rdd.distinct.countByKey<br><span class="hljs-attribute">res0</span>: scala.collection.Map[String,Long] = Map(A -&gt; <span class="hljs-number">1000</span>, B -&gt; <span class="hljs-number">1000</span>, C -&gt; <span class="hljs-number">1000</span>)<br><br><span class="hljs-attribute">scala</span>&gt; rdd.countApproxDistinctByKey(<span class="hljs-number">4</span>, <span class="hljs-number">0</span>).collect<br><span class="hljs-attribute">res1</span>: Array[(String, Long)] = Array((A,<span class="hljs-number">1213</span>), (B,<span class="hljs-number">1213</span>), (C,<span class="hljs-number">1213</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark RDD - Transformation 算子 - 基础转换</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-transformation-basic-transform/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-transformation-basic-transform/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformation-算子-基础转换"><a href="#Transformation-算子-基础转换" class="headerlink" title="Transformation 算子 - 基础转换"></a>Transformation 算子 - 基础转换</h1><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><p>对RDD的每一条记录做转换操作</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val intRDD = sc.range(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)<br><span class="hljs-attribute">intRDD</span>: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[<span class="hljs-number">0</span>] at range at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val transformedRDD = intRDD.map(_ * <span class="hljs-number">2</span>)<br><span class="hljs-attribute">transformedRDD</span>: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[<span class="hljs-number">1</span>] at map at &lt;console&gt;:<span class="hljs-number">23</span>cala&gt; val transformedRDD = intRDD<br><br><span class="hljs-attribute">scala</span>&gt; transformedRDD.collect<br><span class="hljs-attribute">res0</span>: Array[Long] = Array(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>)<br></code></pre></td></tr></table></figure><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p>对RDD的每一条记录做过滤操作</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val intRDD = sc<span class="hljs-selector-class">.range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>)<br>intRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Long]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[0]</span> at range at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val filteredRDD = intRDD<span class="hljs-selector-class">.filter</span>(_ &lt;= <span class="hljs-number">2</span>)<br>filteredRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Long]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at <span class="hljs-attribute">filter</span> at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; filteredRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[Long]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h2><p>对RDD的每一条记录中的集合列做转换操作，同时将数组中的元素展开成多行</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val twoDList = List(List(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), List(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br><span class="hljs-attribute">twoDList</span>: List[List[Int]] = List(List(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), List(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br><br><span class="hljs-attribute">scala</span>&gt; val rdd= sc.parallelize(twoDList)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[List[Int]] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">24</span>cala&gt; val rdd= sc.parallelize(twoDList)<br><br><span class="hljs-attribute">scala</span>&gt; val transformedDF = rdd.flatMap(l =&gt; l.map(_ * <span class="hljs-number">2</span>))<br><span class="hljs-attribute">transformedDF</span>: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[<span class="hljs-number">1</span>] at flatMap at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; transformedDF.collect<br><span class="hljs-attribute">res0</span>: Array[Int] = Array(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>)<br></code></pre></td></tr></table></figure><h2 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h2><p>对RDD进行采样，返回样本记录, fraction 不代表精确的比例，仅代表每条记录被命中的概率</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">100</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val sampleRDD = rdd.sample(withReplacement=true, fraction=<span class="hljs-number">0</span>.<span class="hljs-number">1</span>)<br><span class="hljs-attribute">sampleRDD</span>: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[<span class="hljs-number">1</span>] at sample at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; sampleRDD.collect<br><span class="hljs-attribute">res0</span>: Array[Int] = Array(<span class="hljs-number">15</span>, <span class="hljs-number">17</span>, <span class="hljs-number">21</span>, <span class="hljs-number">21</span>, <span class="hljs-number">36</span>, <span class="hljs-number">43</span>, <span class="hljs-number">54</span>, <span class="hljs-number">59</span>, <span class="hljs-number">63</span>, <span class="hljs-number">67</span>, <span class="hljs-number">83</span>, <span class="hljs-number">83</span>, <span class="hljs-number">95</span>)<br></code></pre></td></tr></table></figure><h2 id="sampleByKey"><a href="#sampleByKey" class="headerlink" title="sampleByKey"></a>sampleByKey</h2><p>对 K-V Pair 型 RDD 进行采样，返回每个 Key 的样本记录, fraction 不代表精确的比例，仅代表各个 Key 的每条记录被命中的概率<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(List(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>)).cartesian(sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">100</span>))<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val sampleRDD = rdd.sampleByKey(withReplacement=false, Map(<span class="hljs-string">&quot;A&quot;</span> -&gt; <span class="hljs-number">0</span>.<span class="hljs-number">05</span>, <span class="hljs-string">&quot;B&quot;</span> -&gt; <span class="hljs-number">0</span>.<span class="hljs-number">05</span>, <span class="hljs-string">&quot;C&quot;</span> -&gt; <span class="hljs-number">0</span>.<span class="hljs-number">1</span>))<br><span class="hljs-attribute">kvRDD</span>: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[<span class="hljs-number">1</span>] at keyBy at &lt;console&gt;:<span class="hljs-number">24</span><br><br><span class="hljs-attribute">scala</span>&gt; sampleRDD.collect<br><span class="hljs-attribute">res0</span>: Array[(String, Int)] = Array((A,<span class="hljs-number">63</span>), (A,<span class="hljs-number">65</span>), (A,<span class="hljs-number">68</span>), (A,<span class="hljs-number">82</span>), (B,<span class="hljs-number">25</span>), (B,<span class="hljs-number">85</span>), (B,<span class="hljs-number">100</span>), (C,<span class="hljs-number">5</span>), (C,<span class="hljs-number">8</span>), (C,<span class="hljs-number">10</span>), (C,<span class="hljs-number">18</span>), (C,<span class="hljs-number">27</span>), (C,<span class="hljs-number">38</span>), (C,<span class="hljs-number">82</span>), (C,<span class="hljs-number">96</span>))<br></code></pre></td></tr></table></figure><h2 id="pipe"><a href="#pipe" class="headerlink" title="pipe"></a>pipe</h2><p>对RDD进行操作系统级的管道操作</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">100</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; rdd.pipe(<span class="hljs-string">&quot;grep 0&quot;</span>).collect<br><span class="hljs-attribute">res0</span>: Array[String] = Array(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">50</span>, <span class="hljs-number">60</span>, <span class="hljs-number">70</span>, <span class="hljs-number">80</span>, <span class="hljs-number">90</span>, <span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure><h2 id="zipWithIndex"><a href="#zipWithIndex" class="headerlink" title="zipWithIndex"></a>zipWithIndex</h2><p>为RDD的每一条记录生成Index, 返回 (record, index) 的元祖<br>其 Index 是根据每个 partition 的 Index 和各个 partition 中每个元素的 index 计算而来，因此产生一个额外的 Job</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val zippedRDD = rdd.zipWithIndex<br><span class="hljs-attribute">zippedRDD</span>: org.apache.spark.rdd.RDD[(Int, Long)] = ZippedWithIndexRDD[<span class="hljs-number">1</span>] at zipWithIndex at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; zippedRDD.collect<br><span class="hljs-attribute">res0</span>: Array[(Int, Long)] = Array((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>), (<span class="hljs-number">2</span>,<span class="hljs-number">1</span>), (<span class="hljs-number">3</span>,<span class="hljs-number">2</span>), (<span class="hljs-number">4</span>,<span class="hljs-number">3</span>), (<span class="hljs-number">5</span>,<span class="hljs-number">4</span>), (<span class="hljs-number">6</span>,<span class="hljs-number">5</span>))<br></code></pre></td></tr></table></figure><h2 id="zipWithUniqueId"><a href="#zipWithUniqueId" class="headerlink" title="zipWithUniqueId"></a>zipWithUniqueId</h2><p>为RDD的每一条记录生成唯一ID, 返回 (record, uniqueId) 的元祖<br>其 uniqueId 是根据 UID &#x3D; itemIndex * partitionNum + partitionIndex 生成的, 因此不同于 zipWithIndex, 此算子不会产生额外的 Job</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val zippedRDD = rdd.zipWithUniqueId<br><span class="hljs-attribute">zippedRDD</span>: org.apache.spark.rdd.RDD[(Int, Long)] = MapPartitionsRDD[<span class="hljs-number">1</span>] at zipWithUniqueId at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; zippedRDD.collect<br><span class="hljs-attribute">res0</span>: Array[(Int, Long)] = Array((<span class="hljs-number">1</span>,<span class="hljs-number">0</span>), (<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), (<span class="hljs-number">3</span>,<span class="hljs-number">4</span>), (<span class="hljs-number">4</span>,<span class="hljs-number">1</span>), (<span class="hljs-number">5</span>,<span class="hljs-number">3</span>), (<span class="hljs-number">6</span>,<span class="hljs-number">5</span>))<br></code></pre></td></tr></table></figure><h2 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h2><p>对 RDD 进行排序，将以用户指定的 U &#x3D;&gt; V 函数的返回值作为排序依据  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val descSortedRDD = rdd<span class="hljs-selector-class">.sortBy</span>(<span class="hljs-attribute">x</span> =&gt; x, ascending=false)<br>sortedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at sortBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; descSortedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[Int]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h2 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h2><p>对 K-V Pair 型 RDD 按 Key 进行排序<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val kvRDD = rdd<span class="hljs-selector-class">.keyBy</span>(<span class="hljs-attribute">x</span> =&gt; x)<br>sortedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at sortBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val ascSortedRDD = kvRDD<span class="hljs-selector-class">.sortByKey</span>(ascending=true)<br>ascSortedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, Int)]</span> = ShuffledRDD<span class="hljs-selector-attr">[2]</span> at sortByKey at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; ascSortedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, Int)]</span> = <span class="hljs-built_in">Array</span>((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), (<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), (<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), (<span class="hljs-number">4</span>,<span class="hljs-number">4</span>), (<span class="hljs-number">5</span>,<span class="hljs-number">5</span>), (<span class="hljs-number">6</span>,<span class="hljs-number">6</span>))<br></code></pre></td></tr></table></figure><h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><p>对 RDD 进行去重, 完全相同的记录将被移除  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-string">&quot;A&quot;</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-string">&quot;A&quot;</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-string">&quot;A&quot;</span> -&gt; <span class="hljs-number">11</span>, <span class="hljs-string">&quot;B&quot;</span> -&gt; <span class="hljs-number">2</span>))<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val distinctedRDD = rdd<span class="hljs-selector-class">.distinct</span><br>distinctedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at distinct at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; distinctedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(String, Int)]</span> = <span class="hljs-built_in">Array</span>((A,<span class="hljs-number">1</span>), (B,<span class="hljs-number">2</span>), (A,<span class="hljs-number">11</span>))<br></code></pre></td></tr></table></figure><h2 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h2><p>为RDD的每一条记录生成一个 Key, 返回(key, record) 的元祖，key 由用户指定的 record &#x3D;&gt; key 函数生成</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">6</span>, <span class="hljs-number">2</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val transformedDF = rdd<span class="hljs-selector-class">.keyBy</span> &#123; <span class="hljs-attribute">x</span> =&gt; if (x % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) <span class="hljs-string">&quot;even&quot;</span> else <span class="hljs-string">&quot;odd&quot;</span> &#125;<br>transformedDF: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, Int)]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at keyBy at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; transformedDF<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(String, Int)]</span> = <span class="hljs-built_in">Array</span>((odd,<span class="hljs-number">1</span>), (even,<span class="hljs-number">2</span>), (odd,<span class="hljs-number">3</span>), (even,<span class="hljs-number">4</span>), (odd,<span class="hljs-number">5</span>), (even,<span class="hljs-number">6</span>))<br></code></pre></td></tr></table></figure><h2 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h2><p>对 K-V Pair 型 RDD 的每一条记录的 values 进行转换<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(List(<span class="hljs-string">&quot;A&quot;</span> -&gt; <span class="hljs-number">1</span>, <span class="hljs-string">&quot;B&quot;</span> -&gt; <span class="hljs-number">2</span>))<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val transformedRDD = rdd.mapValues(_ * <span class="hljs-number">2</span>)<br><span class="hljs-attribute">transformedRDD</span>: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[<span class="hljs-number">1</span>] at mapValues at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; transformedRDD.collect<br><span class="hljs-attribute">res0</span>: Array[(String, Int)] = Array((A,<span class="hljs-number">2</span>), (B,<span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure><h2 id="flatMapValues"><a href="#flatMapValues" class="headerlink" title="flatMapValues"></a>flatMapValues</h2><p>对 K-V Pair 型 RDD的每一条记录中的集合列做转换操作，同时将数组中的元素展开成多行<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em>  </p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(List(<span class="hljs-number">1</span> -&gt; List(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>), <span class="hljs-number">2</span> -&gt; List(<span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>)))<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[(Int, List[String])] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val transformedRDD = rdd.flatMapValues(l =&gt; l.map(_ * <span class="hljs-number">2</span>))<br><span class="hljs-attribute">transformedRDD</span>: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[<span class="hljs-number">1</span>] at flatMapValues at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; transformedRDD.collect<br><span class="hljs-attribute">res0</span>: Array[(Int, String)] = Array((<span class="hljs-number">1</span>,AA), (<span class="hljs-number">1</span>,aa), (<span class="hljs-number">2</span>,BB), (<span class="hljs-number">2</span>,bb))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark RDD - Transformation 算子 - 集合运算</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-transformation-collection-transform/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-transformation-collection-transform/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformation-算子-集合运算"><a href="#Transformation-算子-集合运算" class="headerlink" title="Transformation 算子 - 集合运算"></a>Transformation 算子 - 集合运算</h1><h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><p>对两个RDD求并集</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">5</span>, <span class="hljs-number">2</span>)<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">6</span> to <span class="hljs-number">10</span>, <span class="hljs-number">2</span>)<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val unionRDD = rdd1<span class="hljs-selector-class">.union</span>(rdd2)<br>unionRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = UnionRDD<span class="hljs-selector-attr">[2]</span> at union at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; unionRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[Int]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h2 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h2><p>对两个RDD求交集</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">5</span>, <span class="hljs-number">2</span>)<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">3</span> to <span class="hljs-number">7</span>, <span class="hljs-number">2</span>)<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val intersectionRDD = rdd1<span class="hljs-selector-class">.intersection</span>(rdd2)<br>unionRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = UnionRDD<span class="hljs-selector-attr">[2]</span> at union at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; intersectionRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[Int]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><h2 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h2><p>对RDD求差集, 返回在 rdd1 但不在 rdd2 中的记录</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">5</span>)<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">3</span> to <span class="hljs-number">7</span>)<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val subtractRDD = rdd1<span class="hljs-selector-class">.subtract</span>(rdd2)<br>unionRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[129]</span> at subtract at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; subtractRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[Int]</span> = <span class="hljs-built_in">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>将两个RDD 按 Key 关联在一起，返回关联后的RDD, 对于重复 value，会单独出现在不同记录中<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em>  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;b&quot;</span>))<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;Bb&quot;</span>))<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val joinedRDD = rdd1<span class="hljs-selector-class">.join</span>(rdd2)<br>joinedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, (String, String))]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[30]</span> at join at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; joinedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, (String, String))]</span> = <span class="hljs-built_in">Array</span>((<span class="hljs-number">1</span>,(<span class="hljs-selector-tag">a</span>,A)), (<span class="hljs-number">2</span>,(<span class="hljs-selector-tag">b</span>,B)), (<span class="hljs-number">2</span>,(<span class="hljs-selector-tag">b</span>,Bb)))<br></code></pre></td></tr></table></figure><h2 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h2><p>将两个RDD按 Key 关联在一起，返回关联后的RDD，相同 Key 的 value 会被 group 到一个集合中<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">scala&gt; </span>val rdd1 = <span class="hljs-keyword">sc.parallelize(List(1 </span>-&gt; <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;b&quot;</span>))<br><span class="hljs-symbol">rdd1:</span> <span class="hljs-keyword">org.apache.spark.rdd.RDD[Int] </span>= ParallelCollectionRDD[<span class="hljs-number">0</span>] <span class="hljs-built_in">at</span> parallelize <span class="hljs-built_in">at</span> &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-keyword">scala&gt; </span>val rdd2 = <span class="hljs-keyword">sc.parallelize(List(1 </span>-&gt; <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;Bb&quot;</span>))<br><span class="hljs-symbol">rdd2:</span> <span class="hljs-keyword">org.apache.spark.rdd.RDD[Int] </span>= ParallelCollectionRDD[<span class="hljs-number">1</span>] <span class="hljs-built_in">at</span> parallelize <span class="hljs-built_in">at</span> &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-keyword">scala&gt; </span>val cogroupedRDD = rdd1.cogroup(rdd2)<br><span class="hljs-symbol">cogroupedRDD:</span> <span class="hljs-keyword">org.apache.spark.rdd.RDD[(Int, </span>(Iterable[String], Iterable[String]))] = MapPartitionsRDD[<span class="hljs-number">3</span> <span class="hljs-built_in">at</span> cogroup <span class="hljs-built_in">at</span> &lt;console&gt;:<span class="hljs-number">24</span><br><br><span class="hljs-keyword">scala&gt; </span>cogroupedRDD.collect<br><span class="hljs-symbol">res0:</span> Array[(Int, (Iterable[String], Iterable[String]))] = Array((<span class="hljs-number">1</span>,(CompactBuffer(a),CompactBuffer(A))), (<span class="hljs-number">2</span>,(CompactBuffer(<span class="hljs-keyword">b),CompactBuffer(B, </span><span class="hljs-keyword">Bb))))</span><br></code></pre></td></tr></table></figure><h2 id="groupWith"><a href="#groupWith" class="headerlink" title="groupWith"></a>groupWith</h2><p><a href="#cogroup">cogroup</a> 的别名，行为与 cogroup 完全一致</p><h2 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h2><p>将当前 RDD 与另外的 RDD 进行左关联, 结果集中仅包含左 RDD 中的全部记录，右 RDD 中匹配不到的数据置为空<br>即对于每一条记录 (key, value_left), 能匹配到时返回 (key, (value_left, value_right)), 匹配不到时返回(key, (value_left, None))<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em>  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-number">3</span> -&gt; <span class="hljs-string">&quot;c&quot;</span>))<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;Bb&quot;</span>, <span class="hljs-number">4</span> -&gt; <span class="hljs-string">&quot;D&quot;</span>))<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val leftJoinedRDD = rdd1<span class="hljs-selector-class">.leftOuterJoin</span>(rdd2)<br>leftJoinedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, (String, Option[String]</span>))] = MapPartitionsRDD<span class="hljs-selector-attr">[4]</span> at leftOuterJoin at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; leftJoinedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, (String, Option[String]</span>))] = <span class="hljs-built_in">Array</span>((<span class="hljs-number">1</span>,(<span class="hljs-selector-tag">a</span>,<span class="hljs-built_in">Some</span>(A))), (<span class="hljs-number">2</span>,(<span class="hljs-selector-tag">b</span>,<span class="hljs-built_in">Some</span>(B))), (<span class="hljs-number">2</span>,(<span class="hljs-selector-tag">b</span>,<span class="hljs-built_in">Some</span>(Bb))), (<span class="hljs-number">3</span>,(c,None)))<br></code></pre></td></tr></table></figure><h2 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h2><p>将当前 RDD 与另外的 RDD 进行右关联, 结果集中仅包含右 RDD 中的全部记录，左 RDD 中匹配不到的数据置为空<br>即对于每一条记录 (key, value_left), 能匹配到时返回 (key, (value_left, value_right)), 匹配不到时返回(key, (None, value_right))<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em>  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-number">3</span> -&gt; <span class="hljs-string">&quot;c&quot;</span>))<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;Bb&quot;</span>, <span class="hljs-number">4</span> -&gt; <span class="hljs-string">&quot;D&quot;</span>))<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rightJoinedRDD = rdd1<span class="hljs-selector-class">.rightOuterJoin</span>(rdd2)<br>leftJoinedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, (String, Option[String]</span>))] = MapPartitionsRDD<span class="hljs-selector-attr">[4]</span> at leftOuterJoin at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; rightJoinedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, (Option[String]</span>, String))] = <span class="hljs-built_in">Array</span>((<span class="hljs-number">1</span>,(<span class="hljs-built_in">Some</span>(a),A)), (<span class="hljs-number">2</span>,(<span class="hljs-built_in">Some</span>(b),B)), (<span class="hljs-number">2</span>,(<span class="hljs-built_in">Some</span>(b),Bb)), (<span class="hljs-number">4</span>,(None,D)))<br></code></pre></td></tr></table></figure><h2 id="fullOuterJoin"><a href="#fullOuterJoin" class="headerlink" title="fullOuterJoin"></a>fullOuterJoin</h2><p>将当前 RDD 与另外的 RDD 进行全关联, 结果集中将包含左右 RDD 中的全部记录，匹配不到的数据置为空<br>即对于每一条记录 (key, value_left), 能匹配到时返回 (key, (value_left, value_right)), 匹配不到时返回(key, (value_left, value_right)) 或 (key, (None, value_right))<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em>  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-number">3</span> -&gt; <span class="hljs-string">&quot;c&quot;</span>))<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;Bb&quot;</span>, <span class="hljs-number">4</span> -&gt; <span class="hljs-string">&quot;D&quot;</span>))<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val fullJoinedRDD = rdd1<span class="hljs-selector-class">.fullOuterJoin</span>(rdd2)<br>leftJoinedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, (String, Option[String]</span>))] = MapPartitionsRDD<span class="hljs-selector-attr">[4]</span> at leftOuterJoin at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; fullJoinedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, (Option[String]</span>, Option<span class="hljs-selector-attr">[String]</span>))] = <span class="hljs-built_in">Array</span>((<span class="hljs-number">1</span>,(<span class="hljs-built_in">Some</span>(a),<span class="hljs-built_in">Some</span>(A))), (<span class="hljs-number">2</span>,(<span class="hljs-built_in">Some</span>(b),<span class="hljs-built_in">Some</span>(B))), (<span class="hljs-number">2</span>,(<span class="hljs-built_in">Some</span>(b),<span class="hljs-built_in">Some</span>(Bb))), (<span class="hljs-number">3</span>,(<span class="hljs-built_in">Some</span>(c),None)), (<span class="hljs-number">4</span>,(None,<span class="hljs-built_in">Some</span>(D))))<br></code></pre></td></tr></table></figure><h2 id="subtractByKey"><a href="#subtractByKey" class="headerlink" title="subtractByKey"></a>subtractByKey</h2><p>对 K-V Pair 型 RDD 求差集, 返回 key 在 rdd1 但不在 rdd2 中的记录<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-number">3</span> -&gt; <span class="hljs-string">&quot;c&quot;</span>))<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-number">1</span> -&gt; <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-number">2</span> -&gt; <span class="hljs-string">&quot;Bb&quot;</span>, <span class="hljs-number">4</span> -&gt; <span class="hljs-string">&quot;D&quot;</span>))<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val subtractedRDD = rdd1<span class="hljs-selector-class">.subtractByKey</span>(rdd2)<br>subtractedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, String)]</span> = SubtractedRDD<span class="hljs-selector-attr">[2]</span> at subtractByKey at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; subtractedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, String)]</span> = <span class="hljs-built_in">Array</span>((<span class="hljs-number">3</span>,c))<br></code></pre></td></tr></table></figure><h2 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h2><p>对两个RDD进行笛卡尔积运算</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">3</span>)<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>( <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>))<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val cartesianRDD = rdd1<span class="hljs-selector-class">.cartesian</span>(rdd2)<br>cartesianRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, String)]</span> = CartesianRDD<span class="hljs-selector-attr">[2]</span> at cartesian at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; cartesianRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, String)]</span> = <span class="hljs-built_in">Array</span>((<span class="hljs-number">1</span>,A), (<span class="hljs-number">1</span>,B), (<span class="hljs-number">1</span>,C), (<span class="hljs-number">2</span>,A), (<span class="hljs-number">2</span>,B), (<span class="hljs-number">2</span>,C), (<span class="hljs-number">3</span>,A), (<span class="hljs-number">3</span>,B), (<span class="hljs-number">3</span>,C))<br></code></pre></td></tr></table></figure><h2 id="randomSplit"><a href="#randomSplit" class="headerlink" title="randomSplit"></a>randomSplit</h2><p>将RDD切分成一组RDD, 切分成多少组由权重 weights 的数组大小决定, 权重不代表精确的比例，仅代表每条记录被命中的概率</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(<span class="hljs-number">1</span> to <span class="hljs-number">10</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val splitedRDDs = rdd.randomSplit(Array(<span class="hljs-number">0</span>.<span class="hljs-number">2</span>, <span class="hljs-number">0</span>.<span class="hljs-number">2</span>, <span class="hljs-number">0</span>.<span class="hljs-number">6</span>))<br><span class="hljs-attribute">splitedRDDs</span>: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[<span class="hljs-number">1</span>] at randomSplit at &lt;console&gt;:<span class="hljs-number">23</span>, MapPartitionsRDD[<span class="hljs-number">98</span>] at randomSplit at &lt;console&gt;:<span class="hljs-number">23</span>, MapPartitionsRDD[<span class="hljs-number">99</span>] at randomSplit at &lt;console&gt;:<span class="hljs-number">23</span>)<br><br><span class="hljs-attribute">scala</span>&gt; splitedRDDs.map(_.collect)<br><span class="hljs-attribute">res0</span>: Array[Array[Int]] = Array(Array(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>), Array(<span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>), Array(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><h2 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h2><p>将两个RDD按index进行zip, 返回具有相同 index 的 (record1, record2) 的元祖 </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">4</span>, <span class="hljs-number">2</span>)<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>), <span class="hljs-number">2</span>)<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val zippedRDD = rdd1<span class="hljs-selector-class">.zip</span>(rdd2)<br>zippedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, String)]</span> = ZippedPartitionsRDD2<span class="hljs-selector-attr">[2]</span> at zipPartitions at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; zippedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, String)]</span> = <span class="hljs-built_in">Array</span>((<span class="hljs-number">1</span>,A), (<span class="hljs-number">2</span>,B), (<span class="hljs-number">3</span>,C), (<span class="hljs-number">4</span>,D))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark RDD - Transformation 算子 - 分区转换</title>
    <link href="/2025/01/01/apache-spark/spark-rdd-transformation-partition-transform/"/>
    <url>/2025/01/01/apache-spark/spark-rdd-transformation-partition-transform/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformation-算子-分区转换"><a href="#Transformation-算子-分区转换" class="headerlink" title="Transformation 算子 - 分区转换"></a>Transformation 算子 - 分区转换</h1><h2 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h2><p>对 RDD 的每一个分区做转换操作，每个分区中的元素被封装成一个迭代器，因此这个转换函数应是 iterator &#x3D;&gt; iterator 的映射</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">scala&gt; </span>val rdd = <span class="hljs-keyword">sc.parallelize((1 </span>to <span class="hljs-number">6</span>).map(_.toString), <span class="hljs-number">3</span>)<br><span class="hljs-symbol">rdd:</span> <span class="hljs-keyword">org.apache.spark.rdd.RDD[String] </span>= ParallelCollectionRDD[<span class="hljs-number">0</span>] <span class="hljs-built_in">at</span> parallelize <span class="hljs-built_in">at</span> &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-keyword">scala&gt; </span>val transformedRDD = rdd.mapPartitions&#123; iter =&gt; val salt = <span class="hljs-string">&quot;abcd_&quot;</span><span class="hljs-comment">; iter.map(x=&gt;salt + x) &#125;</span><br><span class="hljs-symbol">transformedRDD:</span> <span class="hljs-keyword">org.apache.spark.rdd.RDD[String] </span>= MapPartitionsRDD[<span class="hljs-number">1</span>] <span class="hljs-built_in">at</span> mapPartitions <span class="hljs-built_in">at</span> &lt;console&gt;:<span class="hljs-number">24</span><br><br><span class="hljs-keyword">scala&gt; </span>transformedRDD.collect<br><span class="hljs-symbol">res0:</span> Array[String] = Array(abcd_1, abcd_2, abcd_3, abcd_4, abcd_5, abcd_6)<br></code></pre></td></tr></table></figure><h2 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h2><p>对 RDD 的每一个分区做转换操作，每个分区中的元素被封装成一个迭代器, 并拥有当前 partition 的 Index，因此这个转换函数应是 (partitionIndex, iterator) &#x3D;&gt; iterator 的映射</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>((<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<span class="hljs-selector-class">.map</span>(_.toString), <span class="hljs-number">3</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val transformedRDD = rdd.mapPartitionsWithIndex&#123;(idx, iter) =&gt; iter<span class="hljs-selector-class">.map</span>(<span class="hljs-attribute">x</span>=&gt; s<span class="hljs-string">&quot;p_$&#123;idx&#125;__$&#123;x&#125;&quot;</span>)&#125;<br>transformedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = MapPartitionsRDD<span class="hljs-selector-attr">[1]</span> at mapPartitionsWithIndex at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; transformedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[String]</span> = <span class="hljs-built_in">Array</span>(p_0__1, p_0__2, p_1__3, p_1__4, p_2__5, p_2__6)<br></code></pre></td></tr></table></figure><h2 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h2><p>将 RDD 的每个分区中的所有记录合并成一个 Array</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>((<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<span class="hljs-selector-class">.map</span>(_.toString), <span class="hljs-number">3</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val transformedRDD = rdd<span class="hljs-selector-class">.glom</span><br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; transformedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[Array[String]</span>] = <span class="hljs-built_in">Array</span>(<span class="hljs-built_in">Array</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), <span class="hljs-built_in">Array</span>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>), <span class="hljs-built_in">Array</span>(<span class="hljs-number">5</span>, <span class="hljs-number">6</span>))<br></code></pre></td></tr></table></figure><h2 id="zipPartitions"><a href="#zipPartitions" class="headerlink" title="zipPartitions"></a>zipPartitions</h2><p>将两个RDD按分区进行 zip, 并按用户给定的 (Iter[A], Iter[B]) &#x3D;&gt; Iter[C] 函数，在每个 partition 中返回新的迭代器</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd1 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-number">1</span> to <span class="hljs-number">4</span>, <span class="hljs-number">2</span>)<br>rdd1: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[Int]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val rdd2 = sc<span class="hljs-selector-class">.parallelize</span>(<span class="hljs-built_in">List</span>(<span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;B&quot;</span>, <span class="hljs-string">&quot;C&quot;</span>, <span class="hljs-string">&quot;D&quot;</span>), <span class="hljs-number">2</span>)<br>rdd2: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[1]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val zippedRDD = rdd1<span class="hljs-selector-class">.zipPartitions</span>(rdd2)((iter1, iter2) =&gt; iter1 zip iter2)<br>zippedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(Int, String)]</span> = ZippedPartitionsRDD2<span class="hljs-selector-attr">[2]</span> at zipPartitions at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; zippedRDD<span class="hljs-selector-class">.collect</span><br>res0: Array<span class="hljs-selector-attr">[(Int, String)]</span> = <span class="hljs-built_in">Array</span>((<span class="hljs-number">1</span>,A), (<span class="hljs-number">2</span>,B), (<span class="hljs-number">3</span>,C), (<span class="hljs-number">4</span>,D))<br></code></pre></td></tr></table></figure><h2 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h2><p>减少 RDD 的分区，默认不产生 Shuffle, 当目标分区数大于当前分区数时，将保持当前分区数<br>也可将 shuffle 设置为 true，以得到更多的分区，但是会产生Shuffle, 此场景建议使用 repartition </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>((<span class="hljs-number">1</span> to <span class="hljs-number">6</span>)<span class="hljs-selector-class">.map</span>(_.toString), <span class="hljs-number">3</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[34]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.coalesce</span>(<span class="hljs-number">2</span>)<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br>res0: Int = <span class="hljs-number">2</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.coalesce</span>(<span class="hljs-number">10</span>)<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br>res1: Int = <span class="hljs-number">3</span><br><br>scala&gt; rdd<span class="hljs-selector-class">.coalesce</span>(<span class="hljs-number">10</span>, shuffle=true)<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br>res2: Int = <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><h2 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h2><p>调整 RDD 的分区到目标数量，对数据进行随机重新分布，会产生 Shuffle  </p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">scala</span>&gt; val rdd = sc.parallelize(-<span class="hljs-number">5</span> to <span class="hljs-number">10</span>, <span class="hljs-number">2</span>)<br><span class="hljs-attribute">rdd</span>: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[<span class="hljs-number">0</span>] at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; val repartitionedRDD = rdd.repartition(<span class="hljs-number">3</span>)<br><span class="hljs-attribute">repartitionedRDD</span>: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[<span class="hljs-number">65</span>] at repartition at &lt;console&gt;:<span class="hljs-number">23</span><br><br><span class="hljs-attribute">scala</span>&gt; repartitionedRDD.partitions.size<br><span class="hljs-attribute">res0</span>: Int = <span class="hljs-number">3</span><br><br><span class="hljs-attribute">scala</span>&gt; repartitionedRDD.mapPartitions(iter =&gt; Iterator(iter.toList)).collect<br><span class="hljs-attribute">res1</span>: Array[List[String]] = Array(List(-<span class="hljs-number">5</span>, -<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span>), List(-<span class="hljs-number">4</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>), List(-<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">7</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><h2 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h2><p>调整 K-V Pair 型 RDD 的分区到目标数量，根据 Key 值对数据进行重新分布，会产生 Shuffle<br><em><strong>只能作用于 K-V Pair 型 RDD</strong></em></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs stylus">scala&gt; val rdd = sc<span class="hljs-selector-class">.parallelize</span>(-<span class="hljs-number">5</span> to <span class="hljs-number">10</span>, <span class="hljs-number">2</span>)<br>rdd: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[String]</span> = ParallelCollectionRDD<span class="hljs-selector-attr">[0]</span> at parallelize at &lt;console&gt;:<span class="hljs-number">23</span><br><br>scala&gt; val kvRDD = rdd<span class="hljs-selector-class">.keyBy</span> &#123; <span class="hljs-attribute">x</span> =&gt; if (x &lt; <span class="hljs-number">0</span>) <span class="hljs-string">&quot;negative number&quot;</span> else <span class="hljs-string">&quot;non-negative number&quot;</span> &#125;<br><br>scala&gt; val repartitionedRDD = kvRDD<span class="hljs-selector-class">.partitionBy</span>(new org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.HashPartitioner</span>(<span class="hljs-number">3</span>))<br>repartitionedRDD: org<span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.spark</span><span class="hljs-selector-class">.rdd</span><span class="hljs-selector-class">.RDD</span><span class="hljs-selector-attr">[(String, Int)]</span> = ShuffledRDD<span class="hljs-selector-attr">[]</span> at partitionBy at &lt;console&gt;:<span class="hljs-number">24</span><br><br>scala&gt; repartitionedRDD<span class="hljs-selector-class">.partitions</span><span class="hljs-selector-class">.size</span><br>res0: Int = <span class="hljs-number">3</span><br><br>scala&gt; repartitionedRDD<span class="hljs-selector-class">.values</span><span class="hljs-selector-class">.mapPartitions</span>(iter =&gt; <span class="hljs-built_in">Iterator</span>(iter.toList))<span class="hljs-selector-class">.collect</span><br>res1: Array<span class="hljs-selector-attr">[List[Int]</span>] = <span class="hljs-built_in">Array</span>(<span class="hljs-built_in">List</span>(), <span class="hljs-built_in">List</span>(-<span class="hljs-number">5</span>, -<span class="hljs-number">4</span>, -<span class="hljs-number">3</span>, -<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>), <span class="hljs-built_in">List</span>(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark Streaming Overview</title>
    <link href="/2025/01/01/apache-spark/spark-streaming/"/>
    <url>/2025/01/01/apache-spark/spark-streaming/</url>
    
    <content type="html"><![CDATA[<h1 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h1><h2 id="Spark-Streaming-简介"><a href="#Spark-Streaming-简介" class="headerlink" title="Spark Streaming 简介"></a>Spark Streaming 简介</h2><p><em>Spark Streaming 是 Spark 中的上一代流计算引擎，目前作为一个遗留系统，Spark 3.4.0 之后已经停止更新。如果想从零开始一个 Spark 流处理的应用，请使用 Spark Structured Streaming, todo add link</em>  </p><p>Spark Streaming 是基于 Spark RDD API 抽象出来的流处理计算框架，核心思路是将无界的流数据按时间窗口切分成有界的数据集合，再交给 Spark 引擎对每个有界的数据集进行批处理操作。<br>因此，Spark Streaming 并不是严格意义上的基于数据流的实时计算引擎，而是基于微批的准实时计算引擎，微批之间的间隔最低为1秒左右。但即便如此，也足以应对除了对大多数秒级或分钟级近实时计算的场景。  </p><p><img src="https://github.com/tangchenyang/picx-images-hosting/raw/master/20240816/image.7sn4vpg33o.webp" alt="image"></p><h2 id="创建-DStream"><a href="#创建-DStream" class="headerlink" title="创建 DStream"></a>创建 DStream</h2><p>DStream (Discretized Stream) 是 Spark Streaming 基于 RDD 高度抽象的离散化数据流 API，数据流可以从多种数据源获取，如 Socket、消息队列 Kafka、文件系统 HDFS&#x2F;S3 等<br>Spark Streaming 的操作都是基于 StreamingContext 的，因此需要先创建一个 sparkStreamingContext(ssc) 实例  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.spark._<br><span class="hljs-keyword">import</span> org.apache.spark.streaming._<br><br><span class="hljs-keyword">val</span> sparkConf = <span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>().setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>)<br><span class="hljs-keyword">val</span> ssc = <span class="hljs-keyword">new</span> <span class="hljs-type">StreamingContext</span>(sparkConf, <span class="hljs-type">Seconds</span>(<span class="hljs-number">5</span>))<br>ssc.checkpoint(<span class="hljs-string">&quot;checkpoint&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a>Socket</h3><h4 id="socketTextStream"><a href="#socketTextStream" class="headerlink" title="socketTextStream"></a>socketTextStream</h4><p>根据指定的 hostname 和 port 创建一个基于 TCP Socket 的文本数据流<br>Example: <a href="/spark-example/src/main/scala/org/exmaple/spark/sparkstreaming/SocketTextStreamExample.scala">sparkstream&#x2F;SocketTextStreamExample</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据  </p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-selector-tag">a</span><br><span class="hljs-selector-tag">b</span><br>c<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据 </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724326720000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a<br>b<br>c<br></code></pre></td></tr></table></figure><h4 id="socketStream"><a href="#socketStream" class="headerlink" title="socketStream"></a>socketStream</h4><p>与 <a href="#socketTextStream">socketTextStream</a> 类似，但可以支持自定义的 converter，来将字节流转换为类对象<br>Example: <a href="/spark-example/src/main/scala/org/exmaple/spark/sparkstreaming/SocketTextStreamExample.scala">sparkstream&#x2F;SocketStreamExample</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-comment">// define class WordCount </span><br><span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordCount</span>(<span class="hljs-params">word: <span class="hljs-type">String</span>, count: <span class="hljs-type">Int</span></span>) <span class="hljs-keyword">extends</span> <span class="hljs-title">Serializable</span></span><br><br><span class="hljs-comment">// define custom converter</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convertBytesToWords</span></span>(inputStream: <span class="hljs-type">InputStream</span>): <span class="hljs-type">Iterator</span>[<span class="hljs-type">WordCount</span>] = &#123;<br>  <span class="hljs-keyword">val</span> dataInputStream = <span class="hljs-keyword">new</span> <span class="hljs-type">BufferedReader</span>(<br>    <span class="hljs-keyword">new</span> <span class="hljs-type">InputStreamReader</span>(inputStream, <span class="hljs-string">&quot;UTF-8&quot;</span>)<br>  )<br>  <span class="hljs-keyword">val</span> linesIterator = dataInputStream.lines().iterator().asScala<br>  <span class="hljs-keyword">val</span> wordsIterator = linesIterator.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>))<br>  <span class="hljs-keyword">val</span> wordCountIterator = wordsIterator.map(<span class="hljs-keyword">new</span> <span class="hljs-type">WordCount</span>(_, <span class="hljs-number">1</span>))<br>  wordCountIterator<br>&#125;<br><br><span class="hljs-comment">// create socket stream</span><br><span class="hljs-keyword">val</span> socketWordCountDStream = ssc.socketStream[<span class="hljs-type">WordCount</span>](<br>  <span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>, convertBytesToWords, <span class="hljs-type">StorageLevel</span>.<span class="hljs-type">MEMORY_AND_DISK_SER_2</span><br>)<br>socketWordCountDStream.print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs llvm">$ nc -lk <span class="hljs-number">9999</span><br>a<br>b b<br><span class="hljs-keyword">c</span> <span class="hljs-keyword">c</span> <span class="hljs-keyword">c</span><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs stylus">-------------------------------------------<br>Time: <span class="hljs-number">1724326945000</span> ms<br>-------------------------------------------<br><span class="hljs-function"><span class="hljs-title">WordCount</span><span class="hljs-params">(a,<span class="hljs-number">1</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">WordCount</span><span class="hljs-params">(b,<span class="hljs-number">1</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">WordCount</span><span class="hljs-params">(b,<span class="hljs-number">1</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">WordCount</span><span class="hljs-params">(c,<span class="hljs-number">1</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">WordCount</span><span class="hljs-params">(c,<span class="hljs-number">1</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">WordCount</span><span class="hljs-params">(c,<span class="hljs-number">1</span>)</span></span><br></code></pre></td></tr></table></figure><h3 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h3><p>Spark Streaming 支持与消息队列系统集成，如 Kafka 等  </p><h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><p>根据指定的 Kafka Topic 创建一个持续消费 Kafka Message 的 DStream<br>Spark Streaming 与 Kafka 集成需要引入 <code>org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.1</code> 依赖<br>Example: <a href="/spark-example/src/main/scala/org/exmaple/spark/sparkstreaming/KafkaStreamExample.scala">sparkstream&#x2F;KafkaStreamExample</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">import</span> org.apache.kafka.clients.consumer.<span class="hljs-type">ConsumerRecord</span><br><span class="hljs-keyword">import</span> org.apache.kafka.common.serialization.<span class="hljs-type">StringDeserializer</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.dstream.<span class="hljs-type">InputDStream</span><br><span class="hljs-keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="hljs-type">ConsumerStrategies</span>, <span class="hljs-type">KafkaUtils</span>, <span class="hljs-type">LocationStrategies</span>&#125;<br><br><span class="hljs-keyword">val</span> subscribeTopics = <span class="hljs-type">Array</span>(<span class="hljs-string">&quot;test-topic&quot;</span>)<br><span class="hljs-keyword">val</span> kafkaParams = <span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">Object</span>](<br>  <span class="hljs-string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="hljs-string">&quot;localhost:9092&quot;</span>,<br>  <span class="hljs-string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>],<br>  <span class="hljs-string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="hljs-type">StringDeserializer</span>],<br>  <span class="hljs-string">&quot;group.id&quot;</span> -&gt; <span class="hljs-string">&quot;test_group&quot;</span>,<br>  <span class="hljs-string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="hljs-string">&quot;earliest&quot;</span>,<br>  <span class="hljs-string">&quot;enable.auto.commit&quot;</span> -&gt; <span class="hljs-string">&quot;false&quot;</span><br>)<br><br><span class="hljs-keyword">val</span> kafkaDStream: <span class="hljs-type">InputDStream</span>[<span class="hljs-type">ConsumerRecord</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]] = <span class="hljs-type">KafkaUtils</span>.createDirectStream[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](<br>  ssc,<br>  <span class="hljs-type">LocationStrategies</span>.<span class="hljs-type">PreferConsistent</span>,<br>  <span class="hljs-type">ConsumerStrategies</span>.<span class="hljs-type">Subscribe</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>](subscribeTopics, kafkaParams)<br>)<br><br><span class="hljs-keyword">val</span> kafkaMessageDStream = kafkaDStream.map(_.value)<br><br>kafkaMessageDStream.print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 kafka-producer 命令往本地的 kafka 发送一些消息 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">a</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">bb</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">ccc</span><br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724327280000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a <br>bb<br>ccc<br></code></pre></td></tr></table></figure><h3 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h3><h4 id="textFileStream"><a href="#textFileStream" class="headerlink" title="textFileStream"></a>textFileStream</h4><p>根据指定的文件系统目录创建一个 DStream，用来监控目录中的新添加的文件，并将这些新文件的每一行读取为 DStream 中的每一条记录<br>Example: <a href="/spark-example/src/main/scala/org/exmaple/spark/sparkstreaming/TextFileStreamExample.scala">sparkstream&#x2F;TextFileStreamExample</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> textFileStream = ssc.textFileStream(<span class="hljs-string">&quot;/tmp/spark/logs/&quot;</span>)<br>textFileStream.print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 Linux 命令往该目录下写入一些文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">mkdir</span> -p /tmp/spark/logs/</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;a&quot;</span> &gt; /tmp/spark/logs/a.txt</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;bbb&quot;</span> &gt; /tmp/spark/logs/b.txt</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;ccc&quot;</span> &gt; /tmp/spark/logs/c.txt</span><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724327465000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>ccc<br>bbb<br>a<br></code></pre></td></tr></table></figure><h3 id="自定义-Receiver"><a href="#自定义-Receiver" class="headerlink" title="自定义 Receiver"></a>自定义 Receiver</h3><h4 id="receiverStream"><a href="#receiverStream" class="headerlink" title="receiverStream"></a>receiverStream</h4><p>todo </p><h2 id="Transformation-算子"><a href="#Transformation-算子" class="headerlink" title="Transformation 算子"></a>Transformation 算子</h2><h3 id="基本转换"><a href="#基本转换" class="headerlink" title="基本转换"></a>基本转换</h3><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p>对 DStream 中的每一条记录，根据给定的 U &#x3D;&gt; V 函数做转换操作  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.map(_.toUpperCase).print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>AAA<br>BBB<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及转换后的数据  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724327765000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>AAA<br>BBB<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724327765000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aaa<br>bbb<br></code></pre></td></tr></table></figure><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p>对 DStream 中的每一条记录，根据给定的 U &#x3D;&gt; V 函数做转换操作，同时当 V 为集合时，将集合中的元素展开成多行  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>)).print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-selector-tag">a</span> <span class="hljs-selector-tag">a</span><br><span class="hljs-selector-tag">b</span> <span class="hljs-selector-tag">b</span> b<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及转换后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724327990000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a a<br>b b b<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724327990000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a<br>a<br>b<br>b<br>b<br></code></pre></td></tr></table></figure><h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p>对 DStream 中的每一条记录做过滤操作</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.filter(_.length &gt;= <span class="hljs-number">2</span>).print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>a<br>bb<br>ccc<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及转换后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724328160000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a<br>bb<br>ccc<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724328160000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>bb<br>ccc<br></code></pre></td></tr></table></figure><h4 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h4><p>对  K-V Pair 型 DStream 中的每一条记录的 values 进行转换<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em>  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.map((_, <span class="hljs-number">1</span>)).mapValues(_ * <span class="hljs-number">2</span>).print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>a<br>b<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及转换后的数据  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724328160000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a<br><span class="hljs-section">b</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724328160000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,2)<br>(b,2)<br></code></pre></td></tr></table></figure><h4 id="flatMapValues"><a href="#flatMapValues" class="headerlink" title="flatMapValues"></a>flatMapValues</h4><p>对 K-V Pair 型 DStream 的每一条记录中的列做转换操作，同时结果将数组中的元素展开成多行<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em>  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream<br>  .map(line =&gt; (line, line))<br>  .flatMapValues(_.split(<span class="hljs-string">&quot; &quot;</span>).toList)<br>  .print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-built_in">a1</span> <span class="hljs-built_in">a2</span><br><span class="hljs-keyword">b1 </span><span class="hljs-keyword">b2</span><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及转换后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724400550000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a1 a2<br><span class="hljs-section">b1 b2</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724400550000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a1 a2,a1)<br>(a1 a2,a2)<br>(b1 b2,b1)<br>(b1 b2,b2)<br></code></pre></td></tr></table></figure><h3 id="分区转换"><a href="#分区转换" class="headerlink" title="分区转换"></a>分区转换</h3><h4 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h4><p>将 DStream 每个批次中的 RDD 的 partition 到目标数量，对数据进行随机重新分布，会产生 Shuffle</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.foreachRDD(rdd =&gt; println(<span class="hljs-string">&quot;number of partitions before repartition: &quot;</span> + rdd.partitions.length))<br>socketTextDStream.repartition(<span class="hljs-number">10</span>).foreachRDD(rdd =&gt; println(<span class="hljs-string">&quot;number of partitions after repartition: &quot;</span> + rdd.partitions.length))<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出 repartition 前后的分区数</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs applescript"><span class="hljs-built_in">number</span> <span class="hljs-keyword">of</span> partitions <span class="hljs-keyword">before</span> repartition: <span class="hljs-number">2</span><br><span class="hljs-built_in">number</span> <span class="hljs-keyword">of</span> partitions <span class="hljs-keyword">after</span> repartition: <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><h4 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h4><p>对 DStream 每个批次中的 RDD 的每一个分区做转换操作，每个分区中的元素被封装成一个迭代器，因此这个转换函数应是 iterator &#x3D;&gt; iterator 的映射</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.mapPartitions(iter =&gt; iter.map(_.toUpperCase)).print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及转换后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329090000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329090000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>AA<br>BB<br></code></pre></td></tr></table></figure><h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><h4 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h4><p>将 DStream 每个批次中的 RDD 的每个分区中的所有记录合并成一个 Array  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.glom().map(_.mkString(<span class="hljs-string">&quot;, &quot;</span>)).print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 glom 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329215000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br>cc<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329215000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa, bb<br>cc<br></code></pre></td></tr></table></figure><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p>对 DStream 每个批次中的 RDD 做聚合操作  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.reduce(_ + _).print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 reduce 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329430000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br>cc<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329430000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aabbcc<br></code></pre></td></tr></table></figure><h4 id="count"><a href="#count" class="headerlink" title="count"></a>count</h4><p>对 DStream 每个批次中的 RDD 求 count </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.count().print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 count 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329505000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br>cc<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329505000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>3<br></code></pre></td></tr></table></figure><h4 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue"></a>countByValue</h4><p>对 DStream 每个批次中的 RDD 的每一条记录求 count   </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.countByValue().print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>aa<br>bb<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及汇总后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329590000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>aa<br>bb<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724329590000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>(aa,2)<br>(bb,1)<br><br></code></pre></td></tr></table></figure><h4 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h4><p>对 K-V Pair 型 DStream 每个批次中的 RDD 按 key 进行分组, 具有相同 key 的记录会被 group 到一起<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream<br>  .map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br>  .groupByKey()<br>  .print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-built_in">a,</span>a1<br><span class="hljs-built_in">a,</span>a2<br><span class="hljs-built_in">b,</span>b1<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及汇总后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724401770000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a,a1<br>a,a2<br><span class="hljs-section">b,b1</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724401770000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,ArrayBuffer(a1, a2))<br>(b,ArrayBuffer(b1))<br></code></pre></td></tr></table></figure><h4 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h4><p>对 K-V Pair 型 DStream 每个批次中的 RDD 按 key 进行 reduce 操作, 具有相同 key 的记录将按照用户指定的 (left, right) &#x3D;&gt; result 函数从左到右进行合并<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream<br>  .map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br>  .groupByKey()<br>  .print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-built_in">a,</span>a1<br><span class="hljs-built_in">a,</span>a2<br><span class="hljs-built_in">b,</span>b1<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及汇总后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724401885000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a,a1<br>a,a2<br><span class="hljs-section">b,b1</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724401885000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,a1a2)<br>(b,b1)<br><br></code></pre></td></tr></table></figure><h4 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h4><p>对 DStream 每个批次中的 RDD 进行合并操作，与 <a href="spark-rdd.md#combinebykey">rdd-combineByKey</a> 行为一致，操作分两个阶段，先对单个分区内的数据聚合，再对所有分区聚合结果进行聚合，从而得到最终的聚合结果<br>它允许返回一个与 DStream 记录类型 V 不同的类型 U, 比如将元素(Int) group 成一个 List<br>同样需要两个聚合函数，以及一个用来创建初始值的函数, 这个函数的入参将是每个分区的第一个元素  </p><ul><li>createCombiner: V &#x3D;&gt; U, 用于创建每个分区的初始值  </li><li>mergeValue: (U, V) &#x3D;&gt; U, 作用在每个分区内数据的聚合函数  </li><li>mergeCombiners: (U, U) &#x3D;&gt; U, 作用在每个分区聚合结果上的聚合函数  </li><li>partitioner, 用来对数据进行重分区的 partitioner, 一般无特殊要求，给 HashPartitioner 即可<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream<br>  .map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br>  .combineByKey[<span class="hljs-type">List</span>[<span class="hljs-type">String</span>]](<br>    createCombiner = (s: <span class="hljs-type">String</span>) =&gt; <span class="hljs-type">List</span>[<span class="hljs-type">String</span>](s),<br>    mergeValue = (l: <span class="hljs-type">List</span>[<span class="hljs-type">String</span>], s: <span class="hljs-type">String</span>) =&gt; l :+ s,<br>    mergeCombiner = (l1: <span class="hljs-type">List</span>[<span class="hljs-type">String</span>], l2: <span class="hljs-type">List</span>[<span class="hljs-type">String</span>]) =&gt; l1 ++ l2,<br>    partitioner = <span class="hljs-keyword">new</span> <span class="hljs-type">HashPartitioner</span>(ssc.sparkContext.defaultMinPartitions))<br>  .print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-built_in">a,</span>a1<br><span class="hljs-built_in">a,</span>a2<br><span class="hljs-built_in">b,</span>b1<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及汇总后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724402650000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a,a1<br>a,a2<br><span class="hljs-section">b,b1</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724402650000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(b,List(b1))<br>(a,List(a1, a2))<br></code></pre></td></tr></table></figure><h3 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h3><h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>将当前 DStream 与另一个 DStream 的数据统一起来，返回一个新的 DStream，两个 DStream 的滑动窗口间隔必须相同<br><em><strong>作用于滑动窗口时，两个滑动窗口必须有相同的滑动时间</strong></em>  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream9999 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br><span class="hljs-keyword">val</span> socketTextDStream9998 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9998</span>)<br><br>socketTextDStream9999.print()<br>socketTextDStream9998.print()<br>socketTextDStream9999.union(socketTextDStream9998).print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 和 9998 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br><br><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9998</span><br><span class="hljs-number">11</span><br><span class="hljs-number">22</span><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 union 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724328350000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724328350000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>11<br>22<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724328350000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br>11<br>22<br></code></pre></td></tr></table></figure><h4 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h4><p>将两个 DStream 每个批次中的 RDD 按 Key 关联在一起，返回关联后的 DStream，相同 Key 的 value 会被 group 到一个集合中<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em><br><em><strong>作用于滑动窗口时，两个滑动窗口必须有相同的滑动时间</strong></em>  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream9999 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br><span class="hljs-keyword">val</span> socketTextDStream9998 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9998</span>)<br><br><span class="hljs-keyword">val</span> kvDStream9999 = socketTextDStream9999.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><span class="hljs-keyword">val</span> kvDStream9998 = socketTextDStream9998.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><br>socketTextDStream9999.print()<br>socketTextDStream9998.print()<br><br>kvDStream9999.cogroup(kvDStream9998).print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 和 9998 端口发送一些数据</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-built_in">a,</span>a1<br><span class="hljs-built_in">a,</span>a2<br><span class="hljs-built_in">b,</span>b1<br><br>$ nc -lk <span class="hljs-number">9998</span><br><span class="hljs-built_in">a,</span>A1<br><span class="hljs-built_in">a,</span>A1<br><span class="hljs-built_in">b,</span>B1<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 cogroup 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724403410000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a,a1<br>a,a2<br><span class="hljs-section">b,b1</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724403410000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>a,A1<br>a,A1<br><span class="hljs-section">b,B1</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724403410000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,(CompactBuffer(a1, a2),CompactBuffer(A1, A1)))<br>(b,(CompactBuffer(b1),CompactBuffer(B1)))<br></code></pre></td></tr></table></figure><h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>将当前 DStream 每个批次中的 RDD 与另外的 DStream 对应批次中的 RDD 进行关联,，返回关联后的 DStream，对于重复 value，会单独出现在不同记录中<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream9999 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br><span class="hljs-keyword">val</span> socketTextDStream9998 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9998</span>)<br><br><span class="hljs-keyword">val</span> kvDStream9999 = socketTextDStream9999.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><span class="hljs-keyword">val</span> kvDStream9998 = socketTextDStream9998.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><br>socketTextDStream9999.print()<br>socketTextDStream9998.print()<br><br>kvDStream9999.join(kvDStream9998).print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 和 9998 端口发送一些数据</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs armasm">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-number">1</span>,a<br><span class="hljs-number">2</span>,<span class="hljs-keyword">b</span><br><br>$ nc -lk <span class="hljs-number">9998</span><br><span class="hljs-number">1</span>,A<br><span class="hljs-number">2</span>,<span class="hljs-keyword">B</span><br><span class="hljs-number">2</span>,Bb<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 join 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724403920000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>1,a<br><span class="hljs-section">2,b</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724403920000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>1,A<br>2,B<br><span class="hljs-section">2,Bb</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724403920000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(2,(b,B))<br>(2,(b,Bb))<br>(1,(a,A))<br></code></pre></td></tr></table></figure><h4 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h4><p>将当前 DStream 每个批次中的 RDD 与另外的 DStream 对应批次中的 RDD 进行左关联, 结果集中仅包含左 DStream 中的全部记录，右 DStream 中匹配不到的数据置为空<br>即对于每一条记录 (key, value_left), 能匹配到时返回 (key, (value_left, value_right)), 匹配不到时返回(key, (value_left, None))<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em><br><em><strong>作用于滑动窗口时，两个滑动窗口必须有相同的滑动时间</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream9999 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br><span class="hljs-keyword">val</span> socketTextDStream9998 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9998</span>)<br><br><span class="hljs-keyword">val</span> kvDStream9999 = socketTextDStream9999.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><span class="hljs-keyword">val</span> kvDStream9998 = socketTextDStream9998.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><br>socketTextDStream9999.print()<br>socketTextDStream9998.print()<br><br>kvDStream9999.leftOuterJoin(kvDStream9998).print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 和 9998 端口发送一些数据</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-number">1</span>,a<br><span class="hljs-number">2</span>,<span class="hljs-keyword">b</span><br><span class="hljs-keyword"></span><span class="hljs-number">3</span>,c<br><br>$ nc -lk <span class="hljs-number">9998</span><br><span class="hljs-number">1</span>,A<br><span class="hljs-number">2</span>,<span class="hljs-keyword">B</span><br><span class="hljs-keyword"></span><span class="hljs-number">2</span>,<span class="hljs-keyword">Bb</span><br><span class="hljs-keyword"></span><span class="hljs-number">4</span>,D<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 leftOuterJoin 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724404360000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>1,a<br>2,b<br><span class="hljs-section">3,c</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724404360000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>1,A<br>2,B<br>2,Bb<br><span class="hljs-section">4,D</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724404360000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(2,(b,Some(B)))<br>(2,(b,Some(Bb)))<br>(3,(c,None))<br>(1,(a,Some(A)))<br></code></pre></td></tr></table></figure><h4 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h4><p>将当前 DStream 每个批次中的 RDD 与另外的 DStream 对应批次中的 RDD 进行右关联, 结果集中仅包含右 DStream 中的全部记录，左 DStream 中匹配不到的数据置为空<br>即对于每一条记录 (key, value_left), 能匹配到时返回 (key, (value_left, value_right)), 匹配不到时返回(key, (None, value_right))<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em><br><em><strong>作用于滑动窗口时，两个滑动窗口必须有相同的滑动时间</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream9999 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br><span class="hljs-keyword">val</span> socketTextDStream9998 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9998</span>)<br><br><span class="hljs-keyword">val</span> kvDStream9999 = socketTextDStream9999.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><span class="hljs-keyword">val</span> kvDStream9998 = socketTextDStream9998.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><br>socketTextDStream9999.print()<br>socketTextDStream9998.print()<br><br>kvDStream9999.rightOuterJoin(kvDStream9998).print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 和 9998 端口发送一些数据</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-number">1</span>,a<br><span class="hljs-number">2</span>,<span class="hljs-keyword">b</span><br><span class="hljs-keyword"></span><span class="hljs-number">3</span>,c<br><br>$ nc -lk <span class="hljs-number">9998</span><br><span class="hljs-number">1</span>,A<br><span class="hljs-number">2</span>,<span class="hljs-keyword">B</span><br><span class="hljs-keyword"></span><span class="hljs-number">2</span>,<span class="hljs-keyword">Bb</span><br><span class="hljs-keyword"></span><span class="hljs-number">4</span>,D<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 rightOuterJoin 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724404360000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>1,a<br>2,b<br><span class="hljs-section">3,c</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724404360000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>1,A<br>2,B<br>2,Bb<br><span class="hljs-section">4,D</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724404360000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(2,(Some(b),B))<br>(2,(Some(b),Bb))<br>(4,(None,D))<br>(1,(Some(a),A))<br></code></pre></td></tr></table></figure><h4 id="fullOuterJoin"><a href="#fullOuterJoin" class="headerlink" title="fullOuterJoin"></a>fullOuterJoin</h4><p>将当前 DStream 每个批次中的 RDD 与另外的 DStream 对应批次中的 RDD 进行全关联, 结果集中将包含左右 RDD 中的全部记录，匹配不到的数据置为空<br>即对于每一条记录 (key, value_left), 能匹配到时返回 (key, (value_left, value_right)), 匹配不到时返回(key, (value_left, value_right)) 或 (key, (None, value_right))<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em><br><em><strong>作用于滑动窗口时，两个滑动窗口必须有相同的滑动时间</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream9999 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br><span class="hljs-keyword">val</span> socketTextDStream9998 = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9998</span>)<br><br><span class="hljs-keyword">val</span> kvDStream9999 = socketTextDStream9999.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><span class="hljs-keyword">val</span> kvDStream9998 = socketTextDStream9998.map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br><br>socketTextDStream9999.print()<br>socketTextDStream9998.print()<br><br>kvDStream9999.fullOuterJoin(kvDStream9998).print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 和 9998 端口发送一些数据</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-number">1</span>,a<br><span class="hljs-number">2</span>,<span class="hljs-keyword">b</span><br><span class="hljs-keyword"></span><span class="hljs-number">3</span>,c<br><br>$ nc -lk <span class="hljs-number">9998</span><br><span class="hljs-number">1</span>,A<br><span class="hljs-number">2</span>,<span class="hljs-keyword">B</span><br><span class="hljs-keyword"></span><span class="hljs-number">2</span>,<span class="hljs-keyword">Bb</span><br><span class="hljs-keyword"></span><span class="hljs-number">4</span>,D<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及 fullOuterJoin 后的数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724404360000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>1,a<br>2,b<br><span class="hljs-section">3,c</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724404360000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>1,A<br>2,B<br>2,Bb<br><span class="hljs-section">4,D</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724404360000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(2,(Some(b),Some(B)))<br>(2,(Some(b),Some(Bb)))<br>(3,(Some(c),None))<br>(4,(None,Some(D)))<br>(1,(Some(a),Some(A)))<br></code></pre></td></tr></table></figure><h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><h4 id="window"><a href="#window" class="headerlink" title="window"></a>window</h4><p>生成一个新的 DStream，这个 DStream 以固定的时间宽度(windowDuration)以及滑动时间(slideDuration)的滑动窗口将数据聚集起来<br>滑动窗口中将包含该窗口时间宽度内的所有记录，其时间宽度和滑动时间必须是当前 DStream 的计算间隔的整数倍  </p><ul><li>windowDuration: 滑动窗口的时间宽度。假设其值计算间隔的两倍，那么每当滑动窗口触发时，窗口内的数据将包含当前微批以及上一个微批中的所有记录  </li><li>slideDuration：滑动窗口的滑动时间。即生成新的滑动窗口的时间间隔</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br><span class="hljs-comment">// 滑动窗口时间设置为 10 s (当前计算间隔的两倍)</span><br>socketTextDStream<br>  .window(windowDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">10</span>), slideDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">5</span>))<br>  .print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据，其中 <code>aa</code> 和 <code>bb</code> 发完等待5秒再发 <code>cc</code> 确保他们被两个批次处理</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的当前微批的数据，以及滑动窗口中数据。<br>由于滑动设置为计算间隔的两倍，因此滑动窗口中将同时包含当前的微批和上一个微批中的数据  </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724380055000 ms  // 第一个微批中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724380055000 ms  // 第一个滑动窗口中的数据。 由于上一个微批中无数据，因此当前滑动窗口中的数据等于当前微批中的数据 `aa` `bb`</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724380060000 ms  // 第二个微批中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>cc<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724380060000 ms  // 第二个滑动窗口中的数据。 当前滑动窗口中中包含当前微批中的数据 `cc` 与上一个微批中的数据 `aa` `bb`</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br>cc<br><br></code></pre></td></tr></table></figure><h4 id="reduceByWindow"><a href="#reduceByWindow" class="headerlink" title="reduceByWindow"></a>reduceByWindow</h4><p>对 DStream 每个滑动窗口中的 RDD 做聚合操作，相当于先对 DStream 开窗 <a href="#window">window</a>，再对每个滑动窗口进行 <a href="#reduce">reduce</a> 操作  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream<br>  .reduceByWindow(_ + _, windowDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">10</span>), slideDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">5</span>))<br>  .print()<br><span class="hljs-comment">//socketTextDStream</span><br><span class="hljs-comment">//  .window(windowDuration = Durations.seconds(10), slideDuration = Durations.seconds(5))</span><br><span class="hljs-comment">//  .reduce(_ + _)</span><br><span class="hljs-comment">//  .print()</span><br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据，其中 <code>aa</code> 和 <code>bb</code> 发完等待5秒再发 <code>cc</code> 确保他们被两个批次处理</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的当前微批的数据，以及滑动窗口中的聚合数据   </p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724395980000 ms  // 第一个微批中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724395980000 ms  // 第一个滑动窗口中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>aabb<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724395985000 ms  // 第二个微批中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>cc<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724395985000 ms  // 第二个滑动窗口中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>aabbcc<br><br></code></pre></td></tr></table></figure><h4 id="countByWindow"><a href="#countByWindow" class="headerlink" title="countByWindow"></a>countByWindow</h4><p>对 DStream 每个窗口中的 RDD 求 count，相当于先对 DStream 开窗 <a href="#window">window</a>，再对每个滑动窗口进行 <a href="#count">count</a> 操作</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream<br>  .countByWindow(windowDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">10</span>), slideDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">5</span>))<br>  .print()<br><span class="hljs-comment">//socketTextDStream</span><br><span class="hljs-comment">//  .window(windowDuration = Durations.seconds(10), slideDuration = Durations.seconds(5))</span><br><span class="hljs-comment">//  .count()</span><br><span class="hljs-comment">//  .print()</span><br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据，其中 <code>aa</code> 和 <code>bb</code> 发完等待5秒再发 <code>cc</code> 确保他们被两个批次处理</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的当前微批的数据，以及滑动窗口中的聚合数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724395980000 ms  // 第一个微批中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724395980000 ms  // 第一个滑动窗口中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>2<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724395985000 ms  // 第二个微批中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>cc<br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724395985000 ms  // 第二个滑动窗口中的数据</span><br><span class="hljs-code">-------------------------------------------</span><br>3<br><br></code></pre></td></tr></table></figure><h4 id="groupByKeyAndWindow"><a href="#groupByKeyAndWindow" class="headerlink" title="groupByKeyAndWindow"></a>groupByKeyAndWindow</h4><p>对 DStream 每个滑动窗口中的 RDD 按 Key 做聚合操作，相当于先对 DStream 开窗 <a href="#window">window</a>，再对每个滑动窗口进行 <a href="#groupByKey">groupByKey</a> 操作<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream<br>  .map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br>  .groupByKeyAndWindow(windowDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">10</span>), slideDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">5</span>))<br>  .print()<br><span class="hljs-comment">//socketTextDStream</span><br><span class="hljs-comment">//  .map(line =&gt; line.split(&quot;,&quot;)(0) -&gt; line.split(&quot;,&quot;)(1))</span><br><span class="hljs-comment">//  .window(windowDuration = Durations.seconds(10), slideDuration = Durations.seconds(5))</span><br><span class="hljs-comment">//  .reduceByKey(_ + _)</span><br><span class="hljs-comment">//  .print()</span><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据，其中 <code>a,a1</code> 和 <code>a,a2</code> 发完等待5秒再发 <code>b,b1</code> 确保他们被两个批次处理</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-built_in">a,</span>a1<br><span class="hljs-built_in">a,</span>a2<br><span class="hljs-built_in">b,</span>b1<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的当前微批的数据，以及滑动窗口中的聚合数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724405275000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a,a1<br><span class="hljs-section">a,a2</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724405275000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">(a,ArrayBuffer(a1, a2))</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724405280000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">b,b1</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724405280000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,ArrayBuffer(a1, a2))<br>(b,ArrayBuffer(b1))<br></code></pre></td></tr></table></figure><h4 id="reduceByKeyAndWindow"><a href="#reduceByKeyAndWindow" class="headerlink" title="reduceByKeyAndWindow"></a>reduceByKeyAndWindow</h4><p>对 DStream 每个滑动窗口中的 RDD 按 Key 做聚合操作，相当于先对 DStream 开窗 <a href="#window">window</a>，再对每个滑动窗口进行 <a href="#reduceByKey">reduceByKey</a> 操作</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream<br>  .map(line =&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">0</span>) -&gt; line.split(<span class="hljs-string">&quot;,&quot;</span>)(<span class="hljs-number">1</span>))<br>  .reduceByKeyAndWindow(_ + _, windowDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">10</span>), slideDuration = <span class="hljs-type">Durations</span>.seconds(<span class="hljs-number">5</span>))<br>  .print()<br><span class="hljs-comment">//socketTextDStream</span><br><span class="hljs-comment">//  .map(line =&gt; line.split(&quot;,&quot;)(0) -&gt; line.split(&quot;,&quot;)(1))</span><br><span class="hljs-comment">//  .window(windowDuration = Durations.seconds(10), slideDuration = Durations.seconds(5))</span><br><span class="hljs-comment">//  .reduceByKey(_ + _)</span><br><span class="hljs-comment">//  .print()</span><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据，其中 <code>a,a1</code> 和 <code>a,a2</code> 发完等待5秒再发 <code>b,b1</code> 确保他们被两个批次处理</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-built_in">a,</span>a1<br><span class="hljs-built_in">a,</span>a2<br><span class="hljs-built_in">b,</span>b1<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的当前微批的数据，以及滑动窗口中的聚合数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724405005000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a,a1<br><span class="hljs-section">a,a2</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724405005000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">(a,a1a2)</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724405010000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">b,b1</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724405010000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,a1a2)<br>(b,b1)<br></code></pre></td></tr></table></figure><h4 id="slice"><a href="#slice" class="headerlink" title="slice"></a>slice</h4><h3 id="状态算子"><a href="#状态算子" class="headerlink" title="状态算子"></a>状态算子</h3><h4 id="updateStateByKey"><a href="#updateStateByKey" class="headerlink" title="updateStateByKey"></a>updateStateByKey</h4><p>维护 DStream 的 Key 的全局状态，根据指定的 (values: scala.collection.Seq[U], state: Option[V]) &#x3D;&gt; Option(V) 函数 进行状态更新<br>每个批次中相同 Key 的 value 将被 group 成 values，当前 Key 的状态为 state<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br><span class="hljs-keyword">val</span> tatalCountByKey: (<span class="hljs-type">Seq</span>[<span class="hljs-type">Int</span>], <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>]) =&gt; <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>] = (values: <span class="hljs-type">Seq</span>[<span class="hljs-type">Int</span>], state: <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>]) =&gt; &#123;<br>  <span class="hljs-type">Some</span>(values.sum + state.getOrElse(<span class="hljs-number">0</span>))<br>&#125;<br>socketTextDStream.print()<br>socketTextDStream<br>  .flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>)).map((_, <span class="hljs-number">1</span>))<br>  .updateStateByKey(tatalCountByKey)<br>  .print()<br><br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据，其中 <code>a</code> 和 <code>b</code> 发完等待5秒再发 <code>a a</code> 和 <code>b b</code> 确保他们被两个批次处理</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-selector-tag">a</span><br><span class="hljs-selector-tag">b</span> <br><span class="hljs-selector-tag">a</span> <span class="hljs-selector-tag">a</span><br><span class="hljs-selector-tag">b</span> <span class="hljs-selector-tag">b</span> <br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及更新后的状态数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724409375000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a<br><span class="hljs-section">b </span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724409375000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,1)<br><span class="hljs-section">(b,1)</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724409380000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>a a<br><span class="hljs-section">b b </span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724409380000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,3)<br>(b,3)<br></code></pre></td></tr></table></figure><h4 id="mapWithState"><a href="#mapWithState" class="headerlink" title="mapWithState"></a>mapWithState</h4><p>维护 DStream 的 Key 的全局状态，根据指定的  (Time, String, Option[U], State[U]) &#x3D;&gt; Some[V] 函数 进行状态更新<br>每个批次中每个 Key 的每条记录，以及每个 Key 的当前状态都将作为参数传入函数进行计算，可根据 State 的 <code>exists</code>, <code>get</code>, <code>update</code> 等方法来维护状态<br><em><strong>只能作用于 K-V Pair 型 DStream</strong></em></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br><br><span class="hljs-keyword">val</span> totalCountByKey: (<span class="hljs-type">Time</span>, <span class="hljs-type">String</span>, <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>], <span class="hljs-type">State</span>[<span class="hljs-type">Int</span>]) =&gt; <span class="hljs-type">Some</span>[(<span class="hljs-type">String</span>, <span class="hljs-type">Int</span>)] = (time: <span class="hljs-type">Time</span>, key: <span class="hljs-type">String</span>, value: <span class="hljs-type">Option</span>[<span class="hljs-type">Int</span>], state: <span class="hljs-type">State</span>[<span class="hljs-type">Int</span>]) =&gt; &#123;<br>  <span class="hljs-keyword">if</span> (!state.exists())<br>    state.update(<span class="hljs-number">0</span>)<br>  <span class="hljs-keyword">else</span> &#123;<br>    state.update(state.get() + value.getOrElse(<span class="hljs-number">0</span>))<br>  &#125;<br>  <span class="hljs-type">Some</span>(&#123;key -&gt; state.get()&#125;)<br>&#125;<br><br>socketTextDStream.print()<br>socketTextDStream<br>  .flatMap(_.split(<span class="hljs-string">&quot; &quot;</span>)).map((_, <span class="hljs-number">1</span>))<br>  .mapWithState(<span class="hljs-type">StateSpec</span>.function(totalCountByKey))<br>  .print()<br><br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据，其中 <code>a</code> 和 <code>b</code> 发完等待5秒再发 <code>a a</code> 和 <code>b b</code> 确保他们被两个批次处理</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">$ nc -lk <span class="hljs-number">9999</span><br><span class="hljs-selector-tag">a</span><br><span class="hljs-selector-tag">b</span> <br><span class="hljs-selector-tag">a</span> <span class="hljs-selector-tag">a</span><br><span class="hljs-selector-tag">b</span> <span class="hljs-selector-tag">b</span> <br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出从 socket 接收到的数据，以及更新后的状态数据</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724410525000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>a<br>b <br><br><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724410525000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>(a,1)<br><span class="hljs-section">(b,1)</span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724410530000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>a a<br><span class="hljs-section">b b </span><br><span class="hljs-section">-------------------------------------------</span><br><span class="hljs-section">Time: 1724410530000 ms</span><br><span class="hljs-section">-------------------------------------------</span><br>(a,2)<br>(a,3)<br>(b,2)<br>(b,3)<br><br></code></pre></td></tr></table></figure><h2 id="Action-算子"><a href="#Action-算子" class="headerlink" title="Action 算子"></a>Action 算子</h2><h3 id="转换为-Java-集合"><a href="#转换为-Java-集合" class="headerlink" title="转换为 Java 集合"></a>转换为 Java 集合</h3><h4 id="foreachRDD"><a href="#foreachRDD" class="headerlink" title="foreachRDD"></a>foreachRDD</h4><p>对 DStream 每个批次中的 RDD 做遍历操作，可以将 RDD 输出到控制台或写入外部系统等操作，返回 Unit 类型即无返回值  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.foreachRDD(rdd =&gt; rdd.foreach(println(_)))<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出每个批次中的 RDD 的每一条记录</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724415010000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>(a,a1a2)<br>(b,b1)<br><br></code></pre></td></tr></table></figure><h4 id="print"><a href="#print" class="headerlink" title="print"></a>print</h4><p>将 DStream 每个批次中的 RDD 的前 10 条记录输出到控制台  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br><br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出每个批次中的 RDD 的每一条记录</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724415010000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br>cc<br></code></pre></td></tr></table></figure><h3 id="输出到外部系统"><a href="#输出到外部系统" class="headerlink" title="输出到外部系统"></a>输出到外部系统</h3><h4 id="saveAsTextFiles"><a href="#saveAsTextFiles" class="headerlink" title="saveAsTextFiles"></a>saveAsTextFiles</h4><p>将 DStream 每个批次中的 RDD 以 TEXT 文件格式写入 Hadoop 支持的外部文件系统<br>每个批次都会写一个新的文件目录以及相应的数据文件，因此长时间运行的流任务将产生很多的小文件  </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs scala"><span class="hljs-keyword">val</span> socketTextDStream = ssc.socketTextStream(<span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-number">9999</span>)<br>socketTextDStream.print()<br>socketTextDStream.saveAsTextFiles(<span class="hljs-string">&quot;/tmp/spark/output/socket_data&quot;</span>)<br></code></pre></td></tr></table></figure><p>启动程序后，使用 netcat 命令往本机的 9999 端口发送一些数据</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs crystal"><span class="hljs-variable">$ </span>nc -lk <span class="hljs-number">9999</span><br>aa<br>bb<br>cc<br></code></pre></td></tr></table></figure><p>Spark Streaming 任务的控制台将打印出每个批次中的 RDD 的每一条记录</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs asciidoc"><span class="hljs-code">-------------------------------------------</span><br><span class="hljs-code">Time: 1724653405000 ms</span><br><span class="hljs-code">-------------------------------------------</span><br>aa<br>bb<br>c<br></code></pre></td></tr></table></figure><p>并且会将每个批次中的数据写入指定的路径中  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cat</span> /tmp/spark/output/socket_data-1724653405000/*</span><br>aa<br>bb<br>c<br></code></pre></td></tr></table></figure><h4 id="saveAsObjectFiles"><a href="#saveAsObjectFiles" class="headerlink" title="saveAsObjectFiles"></a>saveAsObjectFiles</h4><p>将 DStream 每个批次中的 RDD 序列化之后，以 Hadoop SequenceFile 文件格式写入 Hadoop 支持的外部文件系统</p><h2 id="控制算子"><a href="#控制算子" class="headerlink" title="控制算子"></a>控制算子</h2><h3 id="persist"><a href="#persist" class="headerlink" title="persist"></a>persist</h3><p>将 DStream 每个批次中的 RDD 缓存到内存或者磁盘中，提升作业性能</p><h3 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h3><p>等同于缓存级别为 <code>MEMORY_ONLY_SER</code> 的 <a href="#persist">persist</a></p><h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h3><p>将当前的 DStream 按设置的间隔保存在设置的 checkpointDir 中，当程序重启时，将从最新 checkpoint 处继续运行 </p>]]></content>
    
    
    <categories>
      
      <category>Spark</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
